# Introduction to Other Languages

**One-time *quick* machine translation only, provided according to the version as of February 2, 2026**

**One-time *quick* machine translation only, provided according to the version as of February 2, 2026**

([Arabic](#Arabic) Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©, [Bengali](#Bengali) à¦¬à¦¾à¦‚à¦²à¦¾, [Russian](#Russian) Ñ€ÑƒÑÑĞºĞ¸Ğ¹, [Italian](#Italian) italiano, [Dutch](#Dutch) Nederlands, [Swedish](#Swedish) svenska)
<a name="arabic"></a>
<div dir="rtl">
'''
# Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©


## ğŸ“– Ø§Ù„Ø®Ù„ÙÙŠØ© ÙˆØ§Ù„Ø±Ø¤ÙŠØ©
ÙŠÙˆÙØ± Ù‡Ø°Ø§ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹ Ø¥Ø·Ø§Ø± ØªØ¯Ø±ÙŠØ¨ Ø¯Ù‚ÙŠÙ‚ (ØªØ¹Ø¯ÙŠÙ„ Ø¯Ù‚ÙŠÙ‚) Ù…Ø®ØµØµÙ‹Ø§ Ù„Ù€ **T5** Ø£Ùˆ **mT5**.

ÙŠÙ‡Ø¯Ù Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø¥Ù„Ù‰ Ù…Ù†Ø­ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ **"Ù…ØªØ§Ù†Ø© Ø¯Ù„Ø§Ù„ÙŠØ©"** Ø¯Ø§Ø®Ù„ÙŠØ© Ù…Ù† Ø®Ù„Ø§Ù„ **ØªØ­Ø³ÙŠÙ† Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨** Ø¨Ø¹Ù…Ù‚ØŒ Ù…Ù…Ø§ ÙŠØ¬Ø¹Ù„Ù‡ Ø£ÙƒØ«Ø± Ø§Ø³ØªÙ‚Ø±Ø§Ø±Ù‹Ø§ ÙÙŠ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù†ØµÙˆØµ Ø°Ø§Øª Ø§Ù„ÙƒØ«Ø§ÙØ© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§ØªÙŠØ© Ø§Ù„Ø¹Ø§Ù„ÙŠØ© Ù…Ø«Ù„ ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ù„Ø·Ø¨ ÙˆØ§Ù„Ø£Ø¯Ø¨ÙŠØ§Øª Ø§Ù„Ù…ØªØ®ØµØµØ©ØŒ Ù…Ø¹ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø¹ÙŠÙˆØ¨ Ø§Ù„Ù†ØµÙŠØ© Ø¨Ø¯Ù‚Ø© ÙˆØ­Ù‚Ù† Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…ØªØ®ØµØµØ© ÙÙŠ Ø§Ù„Ù…Ø¬Ø§Ù„ Ø¨Ø¯Ù‚Ø©.

Ø¨Ø³Ø¨Ø¨ **Ø§Ù„ÙØ±ÙˆÙ‚ Ø§Ù„Ù…Ù„Ø­ÙˆØ¸Ø© ÙÙŠ Ø§Ù„ØªÙˆØ²ÙŠØ¹** Ø¨ÙŠÙ† Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…ØªØ®ØµØµØ© Ù…Ø«Ù„ **Ø§Ù„Ø·Ø¨** ÙˆØ§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù„ØºÙˆÙŠØ© Ø§Ù„Ø¹Ø§Ù…Ø©ØŒ ÙØ¥Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ **ÙÙŠ Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø¯Ù‚ÙŠÙ‚ Ø¹Ø±Ø¶Ø© Ù„Ù„ÙˆÙ‚ÙˆØ¹ ÙÙŠ Ø£Ù‚ØµÙ‰ Ù…Ø­Ù„ÙŠ Ø£Ùˆ ØªÙˆÙ‚Ù Ù…Ø¨ÙƒØ± Ø¨Ø³Ø¨Ø¨ ØªÙ‚Ù„Ø¨Ø§Øª Ø§Ù„Ø®Ø³Ø§Ø±Ø©**ØŒ Ù„Ø°Ø§ ÙŠÙ‚Ø¯Ù… Ù‡Ø°Ø§ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ ØªØ­Ø³ÙŠÙ†Ù‹Ø§ Ù„Ù„Ø¢Ù„ÙŠØ§Øª Ù„Ø­Ù„ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©.

**"Ù…Ù† Ø§Ù„Ø£ÙØ¶Ù„ Ø§Ù„Ø¥ÙØ±Ø§Ø· ÙÙŠ Ø§Ù„Ù…Ù„Ø§Ø¡Ù…Ø© Ù‚Ù„ÙŠÙ„Ø§Ù‹ØŒ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø¹Ø¯Ù… Ø§Ù„ØªÙ‚Ø§Ø±Ø¨ Ø§Ù„ÙƒØ§Ù…Ù„"**. Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù„Ù…Ø¬Ø§Ù„Ø§Øª Ø§Ù„Ù…ØªØ®ØµØµØ© Ø§Ù„ØªÙŠ Ù„Ø§ ØªØ³Ù…Ø­ Ø¨Ø§Ù„ØºÙ…ÙˆØ¶ØŒ ÙØ¥Ù† Ø²ÙŠØ§Ø¯Ø© Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù‡ÙŠ Ø§Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù„Ù€"Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© Ø§Ù„Ø¯Ù„Ø§Ù„Ø©" Ù„Ù„Ù†Ù…ÙˆØ°Ø¬.

>Ø¨Ø³Ø¨Ø¨ Ù‚ÙŠÙˆØ¯ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø£Ø¬Ù‡Ø²Ø© Ø§Ù„Ù…Ø­Ù„ÙŠØ©ØŒ Ù‡Ù†Ø§Ùƒ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙ†Ø§Ø²Ù„Ø§Øª ÙÙŠ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª. Ø§Ù†Ø¸Ø± Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ù„Ù„ØªÙƒÙˆÙŠÙ†Ø§Øª Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©.

---

## âœ… Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©

* **Ø¢Ù„ÙŠØ© Ø§Ù„Ø¥Ø­Ù…Ø§Ø¡**: Ù…Ù† Ø®Ù„Ø§Ù„ ØªØ¹ÙŠÙŠÙ† Ø¹ØªØ¨Ø© `start_step`ØŒ ÙŠØªÙ… ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙ‚Ù„Ø¨Ø§Øª Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ø§Ù„Ù…Ø­Ù„ÙŠØ© ØºÙŠØ± Ø§Ù„Ù…Ø³ØªÙ‚Ø±Ø© ÙÙŠ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¨Ø´ÙƒÙ„ Ø¥Ø¬Ø¨Ø§Ø±ÙŠ. (Ø¨Ø¯Ø¡ Ø¨Ø§Ø±Ø¯)
* **ØªÙ‚ÙŠÙŠÙ… Ø§ØªØ¬Ø§Ù‡ Ø®Ø³Ø§Ø±Ø© Ø§Ù„Ù†ÙˆØ§ÙØ°**: Ù…Ù† Ø®Ù„Ø§Ù„ Ø¥Ø¹Ø¯Ø§Ø¯ `patience`ØŒ ÙŠÙØ³Ù…Ø­ Ù„Ù„Ø®Ø³Ø§Ø±Ø© Ø¨Ø§Ù„ØªÙ‚Ù„Ø¨ Ø£Ùˆ Ø§Ù„ØªÙˆÙ‚Ù ÙÙŠ Ø¯ÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ ÙˆÙŠØªÙ… Ø§Ù„ØªÙˆÙ‚Ù ÙÙ‚Ø· Ø¹Ù†Ø¯Ù…Ø§ ØªÙØ´Ù„ Ø§Ù„Ø®Ø³Ø§Ø±Ø© ÙÙŠ ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ø£Ù…Ø«Ù„ Ù„Ø¹Ø¯Ø© Ù…Ø±Ø§Ø­Ù„ Ù…ØªØªØ§Ù„ÙŠØ©ØŒ Ù…Ù…Ø§ ÙŠÙ…Ù†Ø¹ Ø§Ù„ØªÙˆÙ‚Ù Ø§Ù„Ù…Ø¨ÙƒØ± Ø¨Ø³Ø¨Ø¨ "ÙØªØ±Ø© Ø§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø±" Ø§Ù„ÙƒØ§Ø°Ø¨Ø© Ø§Ù„Ù†Ø§ØªØ¬Ø© Ø¹Ù† Ø§Ù„ØªÙ‚Ù„Ø¨Ø§Øª Ø§Ù„Ù…Ø¤Ù‚ØªØ©.
* **ØªØªØ¨Ø¹ Ø§Ù„Ø­Ø§Ù„Ø©**: ÙŠÙˆÙØ± `SafeDetailedProgressCallback` ØªØ·ÙˆØ± Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù… ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ÙØ¹Ù„ÙŠ ÙˆØªÙ†Ø¨Ø¤ ETA Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ (ØªØ±Ø¯Ø¯ Ù‚Ø§Ø¨Ù„ Ù„Ù„ØªØ¹Ø¯ÙŠÙ„)ØŒ ÙˆÙŠØ¯Ø¹Ù… Ø§Ù„Ø±ØµØ¯ Ø§Ù„Ø´ÙØ§Ù Ù„Ù…Ù‡Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø·ÙˆÙŠÙ„Ø© Ø§Ù„Ù…Ø¯Ù‰.
* **Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ÙØ¹Ù„ÙŠ ÙˆØ§Ø³ØªØ¦Ù†Ø§Ù Ù†Ù‚Ø·Ø© Ø§Ù„ØªÙˆÙ‚Ù**: Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ø§Ù„ÙŠØ© Ø§Ù„ØªÙƒÙ„ÙØ© ÙÙŠ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø·Ø¨ÙŠØŒ ÙŠØªØ¶Ù…Ù† Ù…Ù‚Ø§Ø·Ø¹Ø© ÙŠØ¯ÙˆÙŠØ© Ù…Ø¯Ù…Ø¬Ø© (Ù…Ø¹Ø§Ù„Ø¬Ø© KeyboardInterrupt) *Ctrl+C* ÙˆØ§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ÙØ¹Ù„ÙŠØŒ Ù…Ù…Ø§ ÙŠØ¶Ù…Ù† Ø­ÙØ¸ Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ù…Ø«Ù„ (Best Weights) ÙˆØ£ÙˆØ²Ø§Ù† Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© Ø¨Ø´ÙƒÙ„ ÙƒØ§Ù…Ù„ Ù‚Ø¯Ø± Ø§Ù„Ø¥Ù…ÙƒØ§Ù† ÙÙŠ Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø·ÙˆØ§Ø±Ø¦.

---

## ğŸ› ï¸ ØªÙØ§ØµÙŠÙ„ Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„ØªÙ‚Ù†ÙŠ (Technical Deep-Dive)

### 1. Ø¢Ù„ÙŠØ© ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ØªÙ‚Ø§Ø±Ø¨ Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ù…Ø±Ø§Ø­Ù„ (Multi-stage Convergence Analysis)
Ø¹Ù„Ù‰ Ø¹ÙƒØ³ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ø¹Ø§Ù…Ø©ØŒ ØªØ¸Ù‡Ø± Ù…Ù†Ø­Ù†Ù‰ Ø§Ù„Ø®Ø³Ø§Ø±Ø© ÙÙŠ Ù…Ù‡Ø§Ù… Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø¯Ù‚ÙŠÙ‚ Ø§Ù„Ø·Ø¨ÙŠ ØºØ§Ù„Ø¨Ù‹Ø§ Ø³Ù…Ø© "Ù‡Ø¨ÙˆØ· Ù…ØªØ¯Ø±Ø¬". ÙŠØ³ØªØ¨Ø¯Ù„ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ **ØªÙ‚ÙŠÙŠÙ… Ø§ØªØ¬Ø§Ù‡ Ø®Ø³Ø§Ø±Ø© Ø§Ù„Ù†ÙˆØ§ÙØ°** Ø¨Ø§Ù„Ø­ÙƒÙ… Ø§Ù„Ù„Ø­Ø¸ÙŠ:
* **ØªØ¬Ù†Ø¨ ÙØªØ±Ø§Øª "Ø§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø§Ù„ÙƒØ§Ø°Ø¨"**: ÙŠØ¸Ù‡Ø± T5 ÙÙŠ Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ù†Ù‚Ù„ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø¬Ø§Ù„ ÙØªØ±Ø© Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø­ÙŠØ« ÙŠÙƒÙˆÙ† Ù‡Ø¨ÙˆØ· Ø§Ù„Ø®Ø³Ø§Ø±Ø© Ø¶Ø¹ÙŠÙÙ‹Ø§. Ø¥Ø°Ø§ ØªÙ… ØªØ´ØºÙŠÙ„ Ø§Ù„ØªÙˆÙ‚Ù Ø§Ù„Ù…Ø¨ÙƒØ± ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø±Ø­Ù„Ø©ØŒ ÙØ¥Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙŠÙ…ØªÙ„Ùƒ ÙÙ‚Ø· Ø§Ù„Ø¥Ø­Ø³Ø§Ø³ Ø§Ù„Ù„ØºÙˆÙŠ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØŒ Ù…Ø¹ Ù†Ù‚Øµ ÙÙŠ Ø§Ù„Ù…Ù„Ø§Ø¡Ù…Ø© Ø§Ù„Ø¹Ù…ÙŠÙ‚Ø© Ù„Ù„Ù…Ù†Ø·Ù‚ Ø§Ù„Ø·Ø¨ÙŠ.
* **Ù…Ù†Ø·Ù‚ Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…ØªØ£Ø®Ø±**: Ù…Ù† Ø®Ù„Ø§Ù„ `DelayedEarlyStopping`ØŒ ÙŠØªÙ… ØªØ£Ø¬ÙŠÙ„ Ø§Ù„Ø­ÙƒÙ… Ø¥Ø¬Ø¨Ø§Ø±ÙŠÙ‹Ø§ Ù„Ø§Ù„ØªÙ‚Ø§Ø· **Ø§Ù„ØªÙ‚Ø§Ø±Ø¨ Ø§Ù„Ø«Ø§Ù†ÙˆÙŠ (Secondary Convergence)** Ø¨Ø¹Ø¯ Ø§Ù„ÙØªØ±Ø© Ø§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø±ÙŠØ© Ø§Ù„Ø£ÙˆÙ„Ù‰.

**ÙŠØªÙ… Ø¥ØµØ¯Ø§Ø± Ø¥Ø´Ø§Ø±Ø© Ø§Ù„ØªÙˆÙ‚Ù Ù…Ù† Ø§Ù„Ù†Ø¸Ø§Ù… ÙÙ‚Ø· Ø¨Ø¹Ø¯ ØªØ­Ù„ÙŠÙ„ Ù†ÙˆØ§ÙØ° Ø§Ù„Ø®Ø³Ø§Ø±Ø© Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© Ù…Ø±Ø§Øª Ø¹Ø¯ÙŠØ¯Ø©ØŒ ÙˆØªØ£ÙƒÙŠØ¯ Ø£Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù‚Ø¯ Ø¯Ø®Ù„ Ø­Ø§Ù„Ø© "Ø§Ù„ØªØ´Ø¨Ø¹ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ".**


### 2. Ø§Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø§Ù„ØªØ¯Ø±Ø¬ Ø§Ù„Ø¹Ø§Ù„ÙŠ (Gradient Dynamics Control)
Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù…Ø´ÙƒÙ„Ø© Ø¹Ø¯Ù… Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø§Ù„ØªØ¯Ø±Ø¬ Ø§Ù„Ù†Ø§ØªØ¬ Ø¹Ù† ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª Ø§Ù„Ø·Ø¨ÙŠØ© Ø§Ù„Ù…ØªØ®ØµØµØ© Ø§Ù„Ù†Ø§Ø¯Ø±ØŒ Ù‚Ø§Ù… Ø§Ù„Ø¥Ø·Ø§Ø± Ø¨ØªØ­Ø³ÙŠÙ†Ø§Øª ÙÙŠ Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ø³ÙÙ„ÙŠØ©:
* **ØªØ±Ø§ÙƒÙ… Ø§Ù„ØªØ¯Ø±Ø¬ (Gradient Accumulation)**: Ù…Ù† Ø®Ù„Ø§Ù„ `gradient_accumulation_steps=8` **ØªÙˆÙÙŠØ± Ø§Ù„Ø°Ø§ÙƒØ±Ø©ØŒ ÙˆØªÙ†Ø¹ÙŠÙ… Ø§Ù„ØµØ¯Ù…Ø§Øª Ø§Ù„ØªØ¯Ø±Ø¬ÙŠØ© Ø§Ù„ÙÙˆØ±ÙŠØ©** Ø§Ù„Ù†Ø§ØªØ¬Ø© Ø¹Ù† Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ø·ÙˆÙŠÙ„Ø© Ø§Ù„ØµØ¹Ø¨Ø©ØŒ Ù…Ø­Ø§ÙƒØ§Ø© Ø¨ÙŠØ¦Ø© ØªØ­Ø¯ÙŠØ« Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø© Ø§Ù„ÙƒØ¨ÙŠØ± Ø§Ù„Ù…Ø³ØªÙ‚Ø±.
* **ØªØ±Ø¯Ø¯ ØªÙ‚ÙŠÙŠÙ… ØºÙŠØ± Ù…ØªÙ…Ø§Ø«Ù„**: Ø¨Ø§Ù„ØªØ¹Ø§ÙˆÙ† Ù…Ø¹ `eval_steps=1000`ØŒ ÙÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø·ÙˆÙŠÙ„ Ø§Ù„Ù…Ø¯Ù‰ØŒ ÙŠØªÙ… Ø­ÙØ¸ Ø§Ù„Ø£ÙØ¶Ù„ Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© Ø¨ØªØ±Ø¯Ø¯ Ù…Ù†Ø®ÙØ¶ØŒ Ù…Ù…Ø§ ÙŠØ¶Ù…Ù† Ø£Ù† Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…Ù‚ÙÙ„Ø© Ø¨ÙˆØ§Ø³Ø·Ø© `load_best_model_at_end` ØªØªÙ…ØªØ¹ ÙØ¹Ù„ÙŠÙ‹Ø§ Ø¨Ø§Ù„Ù…ØªØ§Ù†Ø© Ø¹Ø¨Ø± Ø§Ù„Ø¹ÙŠÙ†Ø§Øª.
* **ØªØ±Ø¯Ø¯ Ù…Ø±Ø§Ù‚Ø¨Ø© ØºÙŠØ± Ù…ØªÙ…Ø§Ø«Ù„**: ØªÙƒÙˆÙŠÙ† `logging_steps=100` Ù…Ø¹ `eval_steps=1000`. Ù…Ø¹ Ø¶Ù…Ø§Ù† Ø§Ù„Ø±ØµØ¯ Ø¹Ø§Ù„ÙŠ Ø§Ù„ØªØ±Ø¯Ø¯ (Ù…Ø±Ø§Ù‚Ø¨Ø© Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„ØªØ¯Ø±Ø¬ Ø·Ø¨ÙŠØ¹ÙŠÙ‹Ø§)ØŒ ÙŠÙ‚Ù„Ù„ Ù…Ù† ØªØ±Ø¯Ø¯ ØªÙ‚ÙŠÙŠÙ… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ØªØ­Ù‚Ù‚ Ø¹Ø§Ù„ÙŠ Ø§Ù„ØªÙƒÙ„ÙØ©ØŒ Ù…Ù…Ø§ ÙŠØ¶Ù…Ù† ØªØ±ÙƒÙŠØ² Ø§Ù„Ù‚ÙˆØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© Ø¹Ù„Ù‰ ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª.

---
## ğŸ”¬ Ø±Ø¤Ù‰ Ø§Ù„ØªØ¯Ø±ÙŠØ¨: Ù„Ù…Ø§Ø°Ø§ Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ "ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø®Ø³Ø§Ø±Ø© Ø§Ù„Ù…ØªØ¹Ø¯Ø¯"ØŸ

ÙÙŠ Ù…Ù‡Ù…Ø© Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø·Ø¨ÙŠ Ø§Ù„Ø¯Ù‚ÙŠÙ‚ Ù‡Ø°Ù‡ØŒ ÙŠØ£ØªÙŠ Ù…ØµÙÙˆÙØ© ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ØªÙ‚Ø§Ø±Ø¨ ÙƒØ§Ù„ØªØ§Ù„ÙŠ:

| Ù…Ø±Ø­Ù„Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ | Ø£Ø¯Ø§Ø¡ Ø®ØµØ§Ø¦Øµ Ø§Ù„Ø®Ø³Ø§Ø±Ø© | Ø­Ø§Ù„Ø© Ø§Ù„Ø¯Ù„Ø§Ù„Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© | Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© |
| :--- | :--- | :--- | :--- |
| **Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø£ÙˆÙ„ÙŠØ© (0-6000 Ø®Ø·ÙˆØ©)** | Ø§Ù‡ØªØ²Ø§Ø² Ø¹Ù†ÙŠÙ Ø£Ùˆ Ø§Ù†Ø®ÙØ§Ø¶ Ø¨Ø·ÙŠØ¡ ØªØ¯Ø±ÙŠØ¬ÙŠ | Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø­Ø³ Ø§Ù„Ù„ØºÙˆÙŠ Ù„Ù„Ù…Ø¬Ø§Ù„ØŒ Ù…Ø­Ø§Ø°Ø§Ø© Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„Ø£ÙˆÙ„ÙŠØ© | **Ø§Ù„Ø§Ø³ØªÙ…Ø±Ø§Ø± Ø§Ù„Ø¥Ø¬Ø¨Ø§Ø±ÙŠ** (Ø­Ø¸Ø± Ø§Ù„ØªÙˆÙ‚Ù Ø§Ù„Ù…Ø¨ÙƒØ± Ø§Ù„Ø°ÙŠ Ø­Ø¯Ø« Ø³Ø§Ø¨Ù‚Ù‹Ø§) |
| **Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„ÙˆØ³Ø·Ù‰ (6000-12000 Ø®Ø·ÙˆØ©)** | Ø¸Ù‡ÙˆØ± ÙØªØ±Ø© Ù…Ù†ØµØ© Ø·ÙˆÙŠÙ„Ø© (ØªÙ‚Ø§Ø±Ø¨ Ø²Ø§Ø¦Ù) | Ø­Ù‚Ù† Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…ØªØ®ØµØµØ©ØŒ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù†Ù‚Øµ Ø§Ù„Ù†ØµÙˆØµ | **Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ù…Ø³ØªÙ…Ø±Ø©** (ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª ÙÙŠ Ù†Ø§ÙØ°Ø©) |
| **Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© (12000+ Ø®Ø·ÙˆØ©)** | Ø§Ù†Ø®ÙØ§Ø¶ Ø«Ø§Ù†ÙˆÙŠ Ø¹Ù„Ù‰ Ø´ÙƒÙ„ Ø³Ù„Ø§Ù„Ù… Ø«Ù… Ø§Ø³ØªÙ‚Ø±Ø§Ø± | ØªØ´Ø¨Ø¹ Ø§Ù„Ø¹Ù…Ù‚ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠØŒ Ø§Ù…ØªÙ„Ø§Ùƒ Ø§Ù„Ù‚Ø¯Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø§Ù„Ù…ØªÙŠÙ†Ø© | **Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ** (Ø§Ù„ØªÙˆÙ‚Ù Ø¹Ù†Ø¯ ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø¹ØªØ¨Ø©) |

---

## ğŸ“Š Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØªÙ‚Ø¯ÙŠØ± Ø­Ø¬Ù… Ø§Ù„Ø±Ù…ÙˆØ² (Token) 

ÙÙŠ Ù…Ù‡Ø§Ù… Ø§Ù„Ù…Ø¬Ø§Ù„ Ø§Ù„Ø·Ø¨ÙŠØŒ ÙŠØ­Ø¯Ø¯ Ø­Ø¬Ù… Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù†ØµÙˆØµ Ù…Ø¨Ø§Ø´Ø±Ø© Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¹Ù„Ù‰ Ù„Ù€"Ø§Ù„Ù…ØªØ§Ù†Ø© Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠØ©". ÙˆÙÙ‚Ù‹Ø§ Ù„Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¹Ù…Ù„ÙŠ:
* **Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø­Ø¬Ù…**:
    * **25 Ù…ÙŠØºØ§Ø¨Ø§ÙŠØª Ù†Øµ ØµÙŠÙ†ÙŠ**: Ø¨ÙŠØ§Ù†Ø§Øª Ø£ÙˆÙ„ÙŠØ©ØŒ ØªØ¯Ø¹Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙ‚Ø· ÙÙŠ Ù…Ø­Ø§Ø°Ø§Ø© Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©ØŒ ÙˆØªØ¸Ù‡Ø± "Ø¶Ø¹Ù Ø§Ù„Ø­Ø³ Ø§Ù„Ù„ØºÙˆÙŠ" Ø§Ù„ÙˆØ§Ø¶Ø­ Ø¹Ù†Ø¯ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù†Ù‚Øµ Ø§Ù„Ù†ØµÙˆØµ.
    * **256 Ù…ÙŠØºØ§Ø¨Ø§ÙŠØª Ù†Øµ ØµÙŠÙ†ÙŠ**: ÙŠØ¸Ù‡Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù‚Ø¯Ø±Ø© Ù…Ø³ØªÙ‚Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¯Ù‚ÙŠÙ‚ Ù„Ù„Ù…Ø¬Ø§Ù„ØŒ ÙˆÙŠØµÙ„ Ø¥Ù„Ù‰ Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª Ø§Ù„ØªÙ‚ÙŠÙŠÙ…ÙŠØ© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©ã€‚ï¼ˆØ§Ù†Ø¸Ø± Ø§Ù„Ø¹Ø±Ø¶ Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠï¼‰

* **Ù…Ø±Ø¬Ø¹ ØªØ­ÙˆÙŠÙ„ Ø±Ù…ÙˆØ² Ø§Ù„ØµÙŠÙ†ÙŠØ©**ï¼ˆØ¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ØªØ±Ù…ÙŠØ² UTF-8 ÙˆÙ…Ù‚Ø³Ù… mT5ï¼‰:

| Ø­Ø¬Ù… Ø§Ù„Ù†Øµ | ØªÙ‚Ø¯ÙŠØ± Ø¹Ø¯Ø¯ Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„ØµÙŠÙ†ÙŠØ© | ØªÙ‚Ø¯ÙŠØ± Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø±Ù…ÙˆØ² | 
| :--- | :--- | :--- | 
| **25 Ù…ÙŠØºØ§Ø¨Ø§ÙŠØª** | Ø­ÙˆØ§Ù„ÙŠ 8 Ù…Ù„Ø§ÙŠÙŠÙ† Ø­Ø±Ù | Ø­ÙˆØ§Ù„ÙŠ 10 Ù…Ù„Ø§ÙŠÙŠÙ† | 
| **256 Ù…ÙŠØºØ§Ø¨Ø§ÙŠØª** | Ø­ÙˆØ§Ù„ÙŠ 85 Ù…Ù„ÙŠÙˆÙ† Ø­Ø±Ù | Ø­ÙˆØ§Ù„ÙŠ 100 Ù…Ù„ÙŠÙˆÙ† | 

> **Ù†ØµØ§Ø¦Ø­ Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª**: ÙŠÙÙ†ØµØ­ Ø¨Ø­Ù‚Ù† Ø¶ÙˆØ¶Ø§Ø¡ Ù…Ø¹ØªØ¯Ù„Ø©ØŒ Ù„Ù…Ø­Ø§ÙƒØ§Ø© Ø¨ÙŠØ¦Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø·Ø¨ÙŠØ© Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©ØŒ Ù…Ù…Ø§ ÙŠØ¬Ø¨Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ ØªØ¹Ù„Ù… ÙƒÙŠÙÙŠØ© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ Ù„Ù€"Ø§Ù„ØªØµØ­ÙŠØ­".

[Ø§Ù„Ø¹Ø±Ø¶ Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠ]

'''

---

<a name="bengali"></a>
# à¦¬à¦¾à¦‚à¦²à¦¾


## ğŸ“– à¦ªà¦Ÿà¦­à§‚à¦®à¦¿ à¦à¦¬à¦‚ à¦¦à§ƒà¦·à§à¦Ÿà¦¿à¦­à¦™à§à¦—à¦¿
à¦à¦‡ à¦°à¦¿à¦ªà§‹à¦œà¦¿à¦Ÿà¦°à¦¿ **T5** à¦¬à¦¾ **mT5** à¦†à¦°à§à¦•à¦¿à¦Ÿà§‡à¦•à¦šà¦¾à¦°à§‡à¦° à¦œà¦¨à§à¦¯ à¦•à¦¾à¦¸à§à¦Ÿà¦®à¦¾à¦‡à¦œà¦¡ à¦à¦•à¦Ÿà¦¿ à¦«à¦¾à¦‡à¦¨-à¦Ÿà¦¿à¦‰à¦¨à¦¿à¦‚ à¦Ÿà§à¦°à§‡à¦¨à¦¿à¦‚ à¦«à§à¦°à§‡à¦®à¦“à¦¯à¦¼à¦¾à¦°à§à¦• à¦ªà§à¦°à¦¦à¦¾à¦¨ à¦•à¦°à§‡à¥¤

à¦ªà§à¦°à¦•à¦²à§à¦ªà¦Ÿà¦¿ **à¦Ÿà§à¦°à§‡à¦¨à¦¿à¦‚ à¦•à§Œà¦¶à¦²**-à¦à¦° à¦—à¦­à§€à¦° à¦…à¦ªà§à¦Ÿà¦¿à¦®à¦¾à¦‡à¦œà§‡à¦¶à¦¨à§‡à¦° à¦®à¦¾à¦§à§à¦¯à¦®à§‡ à¦®à¦¡à§‡à¦²à¦•à§‡ à¦à¦•à¦Ÿà¦¿ à¦…à¦¨à§à¦¤à¦°à§à¦¨à¦¿à¦¹à¦¿à¦¤ **â€œà¦¸à§‡à¦®à¦¾à¦¨à§à¦Ÿà¦¿à¦• à¦¸à§à¦¥à¦¿à¦¤à¦¿à¦¸à§à¦¥à¦¾à¦ªà¦•à¦¤à¦¾â€** à¦ªà§à¦°à¦¦à¦¾à¦¨ à¦•à¦°à¦¾à¦° à¦²à¦•à§à¦·à§à¦¯ à¦°à¦¾à¦–à§‡, à¦¯à¦¾à¦¤à§‡ à¦šà¦¿à¦•à¦¿à¦¤à§à¦¸à¦¾ à¦°à¦¿à¦ªà§‹à¦°à§à¦Ÿ, à¦ªà§‡à¦¶à¦¾à¦¦à¦¾à¦° à¦¸à¦¾à¦¹à¦¿à¦¤à§à¦¯ à¦‡à¦¤à§à¦¯à¦¾à¦¦à¦¿ à¦‰à¦šà§à¦š à¦¤à¦¥à§à¦¯ à¦˜à¦¨à¦¤à§à¦¬à§‡à¦° à¦Ÿà§‡à¦•à§à¦¸à¦Ÿà§‡à¦° à¦®à§à¦–à§‹à¦®à§à¦–à¦¿ à¦¹à¦²à§‡ à¦à¦Ÿà¦¿ à¦Ÿà§‡à¦•à§à¦¸à¦Ÿà§‡à¦° à¦•à§à¦·à¦¤à¦¿ à¦†à¦°à¦“ à¦¸à§à¦¥à¦¿à¦¤à¦¿à¦¶à§€à¦²à¦­à¦¾à¦¬à§‡ à¦¹à§à¦¯à¦¾à¦¨à§à¦¡à§‡à¦² à¦•à¦°à¦¤à§‡ à¦ªà¦¾à¦°à§‡ à¦à¦¬à¦‚ à¦¡à§‹à¦®à§‡à¦‡à¦¨-à¦¨à¦¿à¦°à§à¦¦à¦¿à¦·à§à¦Ÿ à¦œà§à¦à¦¾à¦¨ à¦¨à¦¿à¦°à§à¦­à§à¦²à¦­à¦¾à¦¬à§‡ à¦‡à¦¨à¦œà§‡à¦•à§à¦Ÿ à¦•à¦°à¦¤à§‡ à¦ªà¦¾à¦°à§‡à¥¤

à¦¯à§‡à¦¹à§‡à¦¤à§ **à¦šà¦¿à¦•à¦¿à¦¤à§à¦¸à¦¾** à¦‡à¦¤à§à¦¯à¦¾à¦¦à¦¿ à¦ªà§‡à¦¶à¦¾à¦¦à¦¾à¦° à¦Ÿà§‡à¦•à§à¦¸à¦Ÿ à¦¸à¦¾à¦§à¦¾à¦°à¦£ à¦•à¦°à§à¦ªà¦¾à¦¸à§‡à¦° à¦¸à¦¾à¦¥à§‡ à¦‰à¦²à§à¦²à§‡à¦–à¦¯à§‹à¦—à§à¦¯ à¦¬à¦¿à¦¤à¦°à¦£ à¦ªà¦¾à¦°à§à¦¥à¦•à§à¦¯ à¦°à¦¯à¦¼à§‡à¦›à§‡, à¦®à¦¡à§‡à¦² **à¦«à¦¾à¦‡à¦¨-à¦Ÿà¦¿à¦‰à¦¨à¦¿à¦‚-à¦à¦° à¦ªà§à¦°à¦¾à¦¥à¦®à¦¿à¦• à¦ªà¦°à§à¦¯à¦¾à¦¯à¦¼à§‡ à¦¸à¦¹à¦œà§‡à¦‡ à¦²à§‹à¦•à¦¾à¦² à¦…à¦ªà¦Ÿà¦¿à¦®à¦¾à¦¯à¦¼ à¦†à¦Ÿà¦•à§‡ à¦¯à¦¾à¦¯à¦¼ à¦¬à¦¾ Loss à¦“à¦ à¦¾à¦¨à¦¾à¦®à¦¾à¦° à¦•à¦¾à¦°à¦£à§‡ à¦†à¦°à§à¦²à¦¿ à¦¸à§à¦Ÿà¦ªà¦¿à¦‚ à¦˜à¦Ÿà§‡**, à¦à¦‡ à¦ªà§à¦°à¦•à¦²à§à¦ªà¦Ÿà¦¿ à¦à¦‡ à¦¸à¦®à¦¸à§à¦¯à¦¾à¦° à¦¸à¦®à¦¾à¦§à¦¾à¦¨à§‡ à¦®à§‡à¦•à¦¾à¦¨à¦¿à¦œà¦® à¦…à¦ªà§à¦Ÿà¦¿à¦®à¦¾à¦‡à¦œà§‡à¦¶à¦¨ à¦ªà§à¦°à¦¬à¦°à§à¦¤à¦¨ à¦•à¦°à§‡à¥¤

**â€œà¦‰à¦ªà¦¯à§à¦•à§à¦¤ à¦…à¦¤à¦¿-à¦…à¦­à¦¿à¦¯à§‹à¦œà¦¨ à¦¹à§‹à¦•, à¦•à¦¿à¦¨à§à¦¤à§ à¦•à¦¨à¦­à¦¾à¦°à§à¦œà§‡à¦¨à§à¦¸ à¦…à¦¸à¦®à§à¦ªà§‚à¦°à§à¦£ à¦¨à¦¯à¦¼â€**à¥¤ à¦…à¦¸à§à¦ªà¦·à§à¦Ÿà¦¤à¦¾ à¦…à¦¸à§à¦¬à§€à¦•à¦¾à¦°à§à¦¯ à¦ªà§‡à¦¶à¦¾à¦¦à¦¾à¦° à¦•à§à¦·à§‡à¦¤à§à¦°à§‡à¦° à¦œà¦¨à§à¦¯, à¦…à¦¤à¦¿à¦°à¦¿à¦•à§à¦¤ à¦Ÿà§à¦°à§‡à¦¨à¦¿à¦‚ à¦§à¦¾à¦ªà¦—à§à¦²à¦¿ à¦®à¦¡à§‡à¦²à§‡à¦° â€œà¦¸à§‡à¦®à¦¾à¦¨à§à¦Ÿà¦¿à¦• à¦¨à¦¿à¦°à§à¦­à¦°à¦¯à§‹à¦—à§à¦¯à¦¤à¦¾â€à¦° à¦®à§Œà¦²à¦¿à¦• à¦¨à¦¿à¦¶à§à¦šà¦¯à¦¼à¦¤à¦¾à¥¤

>à¦²à§‹à¦•à¦¾à¦² à¦¡à¦¿à¦ªà§à¦²à¦¯à¦¼à¦®à§‡à¦¨à§à¦Ÿà§‡à¦° à¦¡à¦¿à¦­à¦¾à¦‡à¦¸ à¦ªà¦¾à¦°à¦«à¦°à¦®à§à¦¯à¦¾à¦¨à§à¦¸ à¦¸à§€à¦®à¦¿à¦¤ à¦¹à¦“à¦¯à¦¼à¦¾à¦¯à¦¼ à¦¸à§‡à¦Ÿà¦¿à¦‚à¦¸à§‡ à¦…à¦¨à§‡à¦• à¦•à¦®à§à¦ªà§à¦°à§‹à¦®à¦¾à¦‡à¦œ à¦°à¦¯à¦¼à§‡à¦›à§‡à¥¤ à¦¨à¦¿à¦°à§à¦¦à¦¿à¦·à§à¦Ÿ à¦•à¦¨à¦«à¦¿à¦—à¦¾à¦°à§‡à¦¶à¦¨ à¦¤à¦¾à¦²à¦¿à¦•à¦¾ à¦¦à§‡à¦–à§à¦¨ Requirementsà¥¤

---

## âœ… à¦•à§‹à¦° à¦«à¦¿à¦šà¦¾à¦°

* **à¦ªà§à¦°à¦¿-à¦¹à¦¿à¦Ÿ à¦®à§‡à¦•à¦¾à¦¨à¦¿à¦œà¦®**ï¼š`start_step` à¦¥à§à¦°à§‡à¦¶à¦¹à§‹à¦²à§à¦¡ à¦¸à§‡à¦Ÿ à¦•à¦°à§‡ à¦ªà§à¦°à¦¾à¦¥à¦®à¦¿à¦• à¦…à¦¸à§à¦¥à¦¿à¦° à¦²à§‹à¦•à¦¾à¦² à¦°à§à¦¯à¦¾à¦¨à§à¦¡à¦® à¦“à¦ à¦¾à¦¨à¦¾à¦®à¦¾ à¦à¦¡à¦¼à¦¾à¦¨à§‹à¦° à¦œà¦¨à§à¦¯ à¦œà§‹à¦° à¦•à¦°à§‡à¥¤ï¼ˆà¦•à§‹à¦²à§à¦¡ à¦¸à§à¦Ÿà¦¾à¦°à§à¦Ÿï¼‰
* **à¦‰à¦‡à¦¨à§à¦¡à§‹à¦•à§ƒà¦¤ Loss à¦Ÿà§à¦°à§‡à¦¨à§à¦¡ à¦®à§‚à¦²à§à¦¯à¦¾à¦¯à¦¼à¦¨**ï¼š`patience` à¦¸à§‡à¦Ÿà¦¿à¦‚à¦¸à§‡à¦° à¦®à¦¾à¦§à§à¦¯à¦®à§‡ Loss-à¦•à§‡ à¦¨à¦¿à¦°à§à¦¦à¦¿à¦·à§à¦Ÿ à¦šà¦•à§à¦°à§‡ à¦“à¦ à¦¾à¦¨à¦¾à¦®à¦¾ à¦¬à¦¾ à¦¸à§à¦¥à¦¬à¦¿à¦°à¦¤à¦¾ à¦…à¦¨à§à¦®à§‹à¦¦à¦¨ à¦•à¦°à§‡, à¦¶à§à¦§à§à¦®à¦¾à¦¤à§à¦° Loss à¦•à§à¦°à¦®à¦¾à¦—à¦¤ à¦à¦•à¦¾à¦§à¦¿à¦• à¦ªà¦°à§à¦¯à¦¾à¦¯à¦¼à§‡ à¦¸à¦°à§à¦¬à§‹à¦¤à§à¦¤à¦® à¦°à§‡à¦•à¦°à§à¦¡ à¦°à¦¿à¦«à§à¦°à§‡à¦¶ à¦•à¦°à¦¤à§‡ à¦¬à§à¦¯à¦°à§à¦¥ à¦¹à¦²à§‡ à¦¸à§à¦Ÿà¦ª à¦•à¦°à§‡, à¦…à¦¸à§à¦¥à¦¾à¦¯à¦¼à§€ à¦“à¦ à¦¾à¦¨à¦¾à¦®à¦¾à¦° à¦•à¦¾à¦°à¦£à§‡ à¦®à¦¡à§‡à¦²à§‡à¦° à¦®à¦¿à¦¥à§à¦¯à¦¾ â€œà¦ªà§à¦²à§à¦¯à¦¾à¦Ÿà§‹ à¦ªà¦¿à¦°à¦¿à¦¯à¦¼à¦¡â€ à¦à¦° à¦…à¦•à¦¾à¦² à¦¸à§à¦Ÿà¦ª à¦ªà§à¦°à¦¤à¦¿à¦°à§‹à¦§ à¦•à¦°à§‡à¥¤
* **à¦¸à§à¦Ÿà§à¦¯à¦¾à¦Ÿà¦¾à¦¸ à¦Ÿà§à¦°à§à¦¯à¦¾à¦•à¦¿à¦‚**ï¼š`SafeDetailedProgressCallback` à¦°à¦¿à¦¯à¦¼à§‡à¦²-à¦Ÿà¦¾à¦‡à¦® à¦²à¦¾à¦°à§à¦¨à¦¿à¦‚ à¦°à§‡à¦Ÿ à¦¬à¦¿à¦¬à¦°à§à¦¤à¦¨ à¦à¦¬à¦‚ à¦¡à¦¾à¦¯à¦¼à¦¨à¦¾à¦®à¦¿à¦• ETA à¦ªà§à¦°à§‡à¦¡à¦¿à¦•à¦¶à¦¨ à¦ªà§à¦°à¦¦à¦¾à¦¨ à¦•à¦°à§‡ï¼ˆà¦«à§à¦°à¦¿à¦•à§‹à¦¯à¦¼à§‡à¦¨à§à¦¸à¦¿ à¦¸à¦¾à¦®à¦à§à¦œà¦¸à§à¦¯à¦¯à§‹à¦—à§à¦¯ï¼‰, à¦¦à§€à¦°à§à¦˜à¦®à§‡à¦¯à¦¼à¦¾à¦¦à§€ à¦Ÿà§à¦°à§‡à¦¨à¦¿à¦‚ à¦œà¦¬à§‡à¦° à¦¸à§à¦¬à¦šà§à¦› à¦®à¦¨à¦¿à¦Ÿà¦°à¦¿à¦‚ à¦¸à¦®à¦°à§à¦¥à¦¨ à¦•à¦°à§‡à¥¤
* **à¦°à¦¿à¦¯à¦¼à§‡à¦²-à¦Ÿà¦¾à¦‡à¦® à¦¬à§à¦¯à¦¾à¦•à¦†à¦ª à¦à¦¬à¦‚ à¦¬à§à¦°à§‡à¦•à¦ªà¦¯à¦¼à§‡à¦¨à§à¦Ÿ à¦°à¦¿à¦¸à¦¾à¦®à§à¦ªà¦¶à¦¨**ï¼šà¦šà¦¿à¦•à¦¿à¦¤à§à¦¸à¦¾ à¦—à¦¬à§‡à¦·à¦£à¦¾à¦° à¦‰à¦šà§à¦š à¦¸à¦®à¦¯à¦¼à¦¸à¦¾à¦ªà§‡à¦•à§à¦· à¦Ÿà§à¦°à§‡à¦¨à¦¿à¦‚ à¦¸à¦¿à¦¨à¦¾à¦°à¦¿à¦“à¦° à¦œà¦¨à§à¦¯, à¦®à§à¦¯à¦¾à¦¨à§à¦¯à¦¼à¦¾à¦² à¦‡à¦¨à§à¦Ÿà¦¾à¦°à¦¾à¦ªà§à¦Ÿï¼ˆKeyboardInterrupt Handlingï¼‰*Ctrl+C* à¦à¦¬à¦‚ à¦°à¦¿à¦¯à¦¼à§‡à¦²-à¦Ÿà¦¾à¦‡à¦® à¦¬à§à¦¯à¦¾à¦•à¦†à¦ª à¦…à¦¨à§à¦¤à¦°à§à¦­à§à¦•à§à¦¤ à¦•à¦°à§‡, à¦†à¦•à¦¸à§à¦®à¦¿à¦• à¦ªà¦°à¦¿à¦¸à§à¦¥à¦¿à¦¤à¦¿à¦¤à§‡ à¦®à¦¡à§‡à¦²à§‡à¦° à¦¸à¦°à§à¦¬à§‹à¦¤à§à¦¤à¦® à¦“à¦¯à¦¼à§‡à¦Ÿï¼ˆBest Weightsï¼‰ à¦à¦¬à¦‚ à¦à¦•à¦¾à¦§à¦¿à¦• à¦ªà§à¦°à¦•à§à¦°à¦¿à¦¯à¦¼à¦¾ à¦“à¦¯à¦¼à§‡à¦Ÿ à¦¯à¦¥à¦¾à¦¸à¦®à§à¦­à¦¬ à¦¸à¦®à§à¦ªà§‚à¦°à§à¦£à¦­à¦¾à¦¬à§‡ à¦¸à¦‚à¦°à¦•à§à¦·à¦£ à¦¨à¦¿à¦¶à§à¦šà¦¿à¦¤ à¦•à¦°à§‡à¥¤

---

## ğŸ› ï¸ à¦ªà§à¦°à¦¯à§à¦•à§à¦¤à¦¿à¦—à¦¤ à¦¬à¦¾à¦¸à§à¦¤à¦¬à¦¾à¦¯à¦¼à¦¨à§‡à¦° à¦¬à¦¿à¦¸à§à¦¤à¦¾à¦°à¦¿à¦¤ï¼ˆTechnical Deep-Diveï¼‰

### à§§. à¦¬à¦¹à§-à¦ªà¦°à§à¦¯à¦¾à¦¯à¦¼ à¦•à¦¨à¦­à¦¾à¦°à§à¦œà§‡à¦¨à§à¦¸ à¦¬à¦¿à¦šà¦¾à¦° à¦®à§‡à¦•à¦¾à¦¨à¦¿à¦œà¦®ï¼ˆMulti-stage Convergence Analysisï¼‰
à¦¸à¦¾à¦§à¦¾à¦°à¦£ à¦Ÿà¦¾à¦¸à§à¦• à¦¥à§‡à¦•à§‡ à¦­à¦¿à¦¨à§à¦¨, à¦šà¦¿à¦•à¦¿à¦¤à§à¦¸à¦¾ à¦«à¦¾à¦‡à¦¨-à¦Ÿà¦¿à¦‰à¦¨à¦¿à¦‚ à¦Ÿà¦¾à¦¸à§à¦•à§‡à¦° Loss à¦•à¦¾à¦°à§à¦­ à¦ªà§à¦°à¦¾à¦¯à¦¼à¦‡ â€œà¦¸à¦¿à¦à¦¡à¦¼à¦¿-à¦†à¦•à§ƒà¦¤à¦¿à¦° à¦ªà¦¤à¦¨â€ à¦¬à§ˆà¦¶à¦¿à¦·à§à¦Ÿà§à¦¯ à¦¦à§‡à¦–à¦¾à¦¯à¦¼à¥¤ à¦à¦‡ à¦ªà§à¦°à¦•à¦²à§à¦ªà¦Ÿà¦¿ à¦‡à¦¨à¦¸à§à¦Ÿà§à¦¯à¦¾à¦¨à§à¦Ÿ à¦œà¦¾à¦œà¦®à§‡à¦¨à§à¦Ÿà§‡à¦° à¦ªà¦°à¦¿à¦¬à¦°à§à¦¤à§‡ **à¦‰à¦‡à¦¨à§à¦¡à§‹à¦•à§ƒà¦¤ Loss à¦Ÿà§à¦°à§‡à¦¨à§à¦¡ à¦®à§‚à¦²à§à¦¯à¦¾à¦¯à¦¼à¦¨** à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à§‡:
* **â€œà¦®à¦¿à¦¥à§à¦¯à¦¾ à¦¸à¦®à¦¤à¦²â€ à¦…à¦à§à¦šà¦² à¦à¦¡à¦¼à¦¾à¦¨à§‹**ï¼šT5 à¦¡à§‹à¦®à§‡à¦‡à¦¨ à¦Ÿà§à¦°à¦¾à¦¨à§à¦¸à¦«à¦¾à¦°à§‡à¦° à¦ªà§à¦°à¦¾à¦¥à¦®à¦¿à¦• à¦ªà¦°à§à¦¯à¦¾à¦¯à¦¼à§‡ à¦ªà§à¦°à¦¾à¦¯à¦¼à¦‡ Loss à¦ªà¦¤à¦¨ à¦¦à§à¦°à§à¦¬à¦² à¦ªà§à¦²à§à¦¯à¦¾à¦Ÿà§‹ à¦¦à§‡à¦–à¦¾à¦¯à¦¼à¥¤ à¦à¦‡ à¦¸à¦®à¦¯à¦¼ à¦†à¦°à§à¦²à¦¿ à¦¸à§à¦Ÿà¦ª à¦Ÿà§à¦°à¦¿à¦—à¦¾à¦° à¦¹à¦²à§‡, à¦®à¦¡à§‡à¦² à¦¶à§à¦§à§à¦®à¦¾à¦¤à§à¦° à¦®à§Œà¦²à¦¿à¦• à¦­à¦¾à¦·à¦¾-à¦¸à¦®à§à¦ªà¦°à§à¦•à¦¿à¦¤ à¦…à¦¨à§à¦­à§‚à¦¤à¦¿ à¦¥à¦¾à¦•à§‡, à¦•à¦¿à¦¨à§à¦¤à§ à¦šà¦¿à¦•à¦¿à¦¤à§à¦¸à¦¾ à¦²à¦œà¦¿à¦•à§‡à¦° à¦—à¦­à§€à¦° à¦«à¦¿à¦Ÿà¦¿à¦‚ à¦…à¦¨à§à¦ªà¦¸à§à¦¥à¦¿à¦¤à¥¤
* **à¦¬à¦¿à¦²à¦®à§à¦¬à¦¿à¦¤ à¦Ÿà§à¦°à¦¿à¦—à¦¾à¦° à¦²à¦œà¦¿à¦•**ï¼š`DelayedEarlyStopping` à¦à¦° à¦®à¦¾à¦§à§à¦¯à¦®à§‡ à¦œà¦¾à¦œà¦®à§‡à¦¨à§à¦Ÿ à¦¬à¦¿à¦²à¦®à§à¦¬à¦¿à¦¤ à¦•à¦°à§‡, à¦ªà§à¦°à¦¥à¦® à¦ªà§à¦²à§à¦¯à¦¾à¦Ÿà§‹à¦° à¦ªà¦°à¦¬à¦°à§à¦¤à§€ **à¦¦à§à¦¬à¦¿à¦¤à§€à¦¯à¦¼ à¦•à¦¨à¦­à¦¾à¦°à§à¦œà§‡à¦¨à§à¦¸ï¼ˆSecondary Convergenceï¼‰** à¦§à¦°à¦¤à§‡à¥¤

**à¦¶à§à¦§à§à¦®à¦¾à¦¤à§à¦° à¦à¦•à¦¾à¦§à¦¿à¦• Loss à¦‰à¦‡à¦¨à§à¦¡à§‹ à¦¬à¦¿à¦¶à§à¦²à§‡à¦·à¦£à§‡à¦° à¦ªà¦°, à¦®à¦¡à§‡à¦² â€œà¦¸à§‡à¦®à¦¾à¦¨à§à¦Ÿà¦¿à¦• à¦¸à§à¦¯à¦¾à¦šà§à¦°à§‡à¦¶à¦¨â€ à¦…à¦¬à¦¸à§à¦¥à¦¾à¦¯à¦¼ à¦ªà§à¦°à¦¬à§‡à¦¶ à¦•à¦°à§‡à¦›à§‡ à¦•à¦¨à¦«à¦¾à¦°à§à¦® à¦•à¦°à¦²à§‡ à¦¸à¦¿à¦¸à§à¦Ÿà§‡à¦® à¦¸à§à¦Ÿà¦ª à¦¸à¦¿à¦—à¦¨à§à¦¯à¦¾à¦² à¦¦à§‡à¦¯à¦¼à¥¤**


### à§¨. à¦‰à¦šà§à¦š-à¦•à¦•à§à¦·à¦ªà¦¥ à¦—à§à¦°à§‡à¦¡à¦¿à¦¯à¦¼à§‡à¦¨à§à¦Ÿ à¦¸à§à¦¥à¦¿à¦¤à¦¿à¦¶à§€à¦²à¦¤à¦¾ à¦¨à¦¿à¦¯à¦¼à¦¨à§à¦¤à§à¦°à¦£ï¼ˆGradient Dynamics Controlï¼‰
à¦šà¦¿à¦•à¦¿à¦¤à§à¦¸à¦¾ à¦ªà§‡à¦¶à¦¾à¦¦à¦¾à¦° à¦¶à¦¬à§à¦¦à¦•à§‹à¦·à§‡à¦° à¦¬à¦¿à¦°à¦² à¦¬à¦¿à¦¤à¦°à¦£à¦œà¦¨à¦¿à¦¤ à¦—à§à¦°à§‡à¦¡à¦¿à¦¯à¦¼à§‡à¦¨à§à¦Ÿ à¦…à¦¸à§à¦¥à¦¿à¦°à¦¤à¦¾à¦° à¦¸à¦®à¦¸à§à¦¯à¦¾à¦° à¦œà¦¨à§à¦¯, à¦«à§à¦°à§‡à¦®à¦“à¦¯à¦¼à¦¾à¦°à§à¦•à¦Ÿà¦¿ à¦¬à§‡à¦¸ à¦²à§‡à¦¯à¦¼à¦¾à¦°à§‡ à¦…à¦ªà§à¦Ÿà¦¿à¦®à¦¾à¦‡à¦œà§‡à¦¶à¦¨ à¦•à¦°à§‡à¦›à§‡:
* **à¦—à§à¦°à§‡à¦¡à¦¿à¦¯à¦¼à§‡à¦¨à§à¦Ÿ à¦…à§à¦¯à¦¾à¦•à¦¿à¦‰à¦®à§à¦²à§‡à¦¶à¦¨ï¼ˆGradient Accumulationï¼‰**ï¼š`gradient_accumulation_steps=8` à¦à¦° à¦®à¦¾à¦§à§à¦¯à¦®à§‡ **à¦®à§‡à¦®à¦°à¦¿ à¦¸à¦¾à¦¶à§à¦°à¦¯à¦¼ à¦•à¦°à¦¾ à¦¹à¦¯à¦¼, à¦à¦•à¦‡ à¦¸à¦¾à¦¥à§‡ à¦¦à§€à¦°à§à¦˜ à¦•à¦ à¦¿à¦¨ à¦¬à¦¾à¦•à§à¦¯**à¦œà¦¨à¦¿à¦¤ à¦‡à¦¨à¦¸à§à¦Ÿà§à¦¯à¦¾à¦¨à§à¦Ÿ à¦—à§à¦°à§‡à¦¡à¦¿à¦¯à¦¼à§‡à¦¨à§à¦Ÿ à¦¶à¦• à¦¸à§à¦®à§à¦¥ à¦•à¦°à¦¾ à¦¹à¦¯à¦¼, à¦¸à§à¦¥à¦¿à¦¤à¦¿à¦¶à§€à¦² à¦¬à¦¡à¦¼ Batch Size à¦†à¦ªà¦¡à§‡à¦Ÿ à¦ªà¦°à¦¿à¦¬à§‡à¦¶ à¦¸à¦¿à¦®à§à¦²à§‡à¦Ÿ à¦•à¦°à§‡à¥¤
* **à¦…à¦¸à¦®à¦®à¦¿à¦¤ à¦®à§‚à¦²à§à¦¯à¦¾à¦¯à¦¼à¦¨ à¦«à§à¦°à¦¿à¦•à§‹à¦¯à¦¼à§‡à¦¨à§à¦¸**ï¼š`eval_steps=1000` à¦à¦° à¦¸à¦¾à¦¥à§‡ à¦®à¦¿à¦²à¦¿à¦¯à¦¼à§‡, à¦¦à§€à¦°à§à¦˜à¦®à§‡à¦¯à¦¼à¦¾à¦¦à§€ à¦Ÿà§à¦°à§‡à¦¨à¦¿à¦‚à¦¯à¦¼à§‡ à¦•à¦® à¦«à§à¦°à¦¿à¦•à§‹à¦¯à¦¼à§‡à¦¨à§à¦¸à§‡ à¦‰à¦šà§à¦š-à¦¨à¦¿à¦°à§à¦­à§à¦²à¦¤à¦¾ à¦…à¦ªà¦Ÿà¦¿à¦®à¦¾à¦² à¦¸à¦‚à¦°à¦•à§à¦·à¦£ à¦•à¦°à§‡, `load_best_model_at_end` à¦²à¦•-à¦•à¦°à¦¾ à¦“à¦¯à¦¼à§‡à¦Ÿ à¦¸à¦¤à§à¦¯à¦¿à¦•à¦¾à¦°à§‡à¦° à¦•à§à¦°à¦¸-à¦¸à§à¦¯à¦¾à¦®à§à¦ªà¦² à¦°à§‹à¦¬à¦¾à¦¸à§à¦Ÿà¦¨à§‡à¦¸ à¦¸à¦®à§à¦ªà¦¨à§à¦¨ à¦¨à¦¿à¦¶à§à¦šà¦¿à¦¤ à¦•à¦°à§‡à¥¤
* **à¦…à¦¸à¦®à¦®à¦¿à¦¤ à¦®à¦¨à¦¿à¦Ÿà¦°à¦¿à¦‚ à¦«à§à¦°à¦¿à¦•à§‹à¦¯à¦¼à§‡à¦¨à§à¦¸**ï¼š`logging_steps=100` à¦à¦¬à¦‚ `eval_steps=1000` à¦•à¦¨à¦«à¦¿à¦—à¦¾à¦° à¦•à¦°à¦¾à¥¤ à¦‰à¦šà§à¦š-à¦«à§à¦°à¦¿à¦•à§‹à¦¯à¦¼à§‡à¦¨à§à¦¸à¦¿ à¦Ÿà§‡à¦²à¦¿à¦®à§‡à¦Ÿà§à¦°à¦¿ï¼ˆà¦—à§à¦°à§‡à¦¡à¦¿à¦¯à¦¼à§‡à¦¨à§à¦Ÿ à¦¸à§à¦¬à¦¾à¦­à¦¾à¦¬à¦¿à¦• à¦•à¦¿à¦¨à¦¾ à¦®à¦¨à¦¿à¦Ÿà¦° à¦•à¦°à¦¾ï¼‰à¦¨à¦¿à¦¶à§à¦šà¦¿à¦¤ à¦•à¦°à¦¾à¦° à¦¸à¦¾à¦¥à§‡ à¦¸à¦¾à¦¥à§‡, à¦‰à¦šà§à¦š-à¦¸à¦®à¦¯à¦¼à¦¸à¦¾à¦ªà§‡à¦•à§à¦· à¦­à§à¦¯à¦¾à¦²à¦¿à¦¡à§‡à¦¶à¦¨ à¦¸à§‡à¦Ÿ à¦®à§‚à¦²à§à¦¯à¦¾à¦¯à¦¼à¦¨ à¦«à§à¦°à¦¿à¦•à§‹à¦¯à¦¼à§‡à¦¨à§à¦¸ à¦•à¦®à¦¿à¦¯à¦¼à§‡, à¦•à¦®à§à¦ªà¦¿à¦‰à¦Ÿà¦¿à¦‚ à¦ªà¦¾à¦“à¦¯à¦¼à¦¾à¦°à¦•à§‡ à¦ªà§à¦¯à¦¾à¦°à¦¾à¦®à¦¿à¦Ÿà¦¾à¦° à¦†à¦ªà¦¡à§‡à¦Ÿà§‡ à¦•à§‡à¦¨à§à¦¦à§à¦°à§€à¦­à§‚à¦¤ à¦•à¦°à§‡à¥¤

---
## ğŸ”¬ à¦ªà§à¦°à¦¶à¦¿à¦•à§à¦·à¦£ à¦…à¦¨à§à¦¤à¦°à§à¦¦à§ƒà¦·à§à¦Ÿà¦¿: à¦•à§‡à¦¨ â€œà¦¬à¦¹à§à¦¬à¦¾à¦° Loss à¦¬à¦¿à¦¶à§à¦²à§‡à¦·à¦£â€ à¦ªà§à¦°à¦¯à¦¼à§‹à¦œà¦¨?

à¦à¦‡ à¦šà¦¿à¦•à¦¿à§à¦¸à¦¾ à¦ªà¦°à¦¿à¦®à¦¾à¦°à§à¦œà¦¨ à¦•à¦¾à¦œà§‡, à¦¸à¦‚à¦•à¦²à¦¨ à¦¨à¦¿à¦°à§à¦§à¦¾à¦°à¦£ à¦®à§à¦¯à¦¾à¦Ÿà§à¦°à¦¿à¦•à§à¦¸ à¦¨à¦¿à¦®à§à¦¨à¦°à§‚à¦ª:

| à¦ªà§à¦°à¦¶à¦¿à¦•à§à¦·à¦£ à¦ªà¦°à§à¦¯à¦¾à¦¯à¦¼ | Loss à¦¬à§ˆà¦¶à¦¿à¦·à§à¦Ÿà§à¦¯ à¦ªà§à¦°à¦•à¦¾à¦¶ | à¦•à§‹à¦° à¦¸à§‡à¦®à¦¾à¦¨à§à¦Ÿà¦¿à¦• à¦…à¦¬à¦¸à§à¦¥à¦¾ | à¦•à§Œà¦¶à¦² à¦ªà§à¦°à¦¤à¦¿à¦•à§à¦°à¦¿à¦¯à¦¼à¦¾ |
| :--- | :--- | :--- | :--- |
| **à¦ªà§à¦°à¦¾à¦¥à¦®à¦¿à¦• (à§¦-à§¬à§¦à§¦à§¦ à¦§à¦¾à¦ª)** | à¦¤à§€à¦¬à§à¦° à¦¨à¦¡à¦¼à¦¾à¦šà¦¡à¦¼à¦¾ à¦¬à¦¾ à¦§à§€à¦°à§‡ à¦§à§€à¦°à§‡ à¦¹à§à¦°à¦¾à¦¸ | à¦•à§à¦·à§‡à¦¤à§à¦°à§€à¦¯à¦¼ à¦­à¦¾à¦·à¦¾à¦—à¦¤ à¦…à¦¨à§à¦­à§‚à¦¤à¦¿ à¦¸à§à¦¥à¦¾à¦ªà¦¨, à¦ªà§à¦¯à¦¾à¦°à¦¾à¦®à¦¿à¦Ÿà¦¾à¦° à¦ªà§à¦°à¦¾à¦¥à¦®à¦¿à¦• à¦¸à¦¾à¦®à¦à§à¦œà¦¸à§à¦¯ | **à¦œà§‹à¦°à¦ªà§‚à¦°à§à¦¬à¦• à¦…à¦¬à§à¦¯à¦¾à¦¹à¦¤** (à¦†à¦—à§‡ à¦˜à¦Ÿà§‡ à¦¯à¦¾à¦“à¦¯à¦¼à¦¾ à¦ªà§à¦°à¦¾à¦°à¦®à§à¦­à¦¿à¦• à¦¬à¦¨à§à¦§ à¦¨à¦¿à¦·à¦¿à¦¦à§à¦§) |
| **à¦®à¦§à§à¦¯à¦¬à¦°à§à¦¤à§€ (à§¬à§¦à§¦à§¦-à§§à§¨à§¦à§¦à§¦ à¦§à¦¾à¦ª)** | à¦¦à§€à¦°à§à¦˜ à¦ªà§à¦²à§à¦¯à¦¾à¦Ÿà¦«à¦°à§à¦® à¦¸à¦®à¦¯à¦¼ (à¦®à¦¿à¦¥à§à¦¯à¦¾ à¦¸à¦‚à¦•à¦²à¦¨) | à¦ªà§‡à¦¶à¦¾à¦¦à¦¾à¦° à¦œà§à¦à¦¾à¦¨ à¦¸à¦‚à¦¯à§‹à¦œà¦¨, à¦Ÿà§‡à¦•à§à¦¸à¦Ÿ à¦¤à§à¦°à§à¦Ÿà¦¿ à¦ªà§à¦°à¦•à§à¦°à¦¿à¦¯à¦¼à¦¾à¦•à¦°à¦£ | **à¦…à¦¬à§à¦¯à¦¾à¦¹à¦¤ à¦ªà¦°à§à¦¯à¦¬à§‡à¦•à§à¦·à¦£** (à¦‰à¦‡à¦¨à§à¦¡à§‹à¦•à§ƒà¦¤ à¦ªà§à¦°à¦¬à¦£à¦¤à¦¾ à¦¬à¦¿à¦¶à§à¦²à§‡à¦·à¦£) |
| **à¦ªà¦°à¦¬à¦°à§à¦¤à§€ (à§§à§¨à§¦à§¦à§¦+ à¦§à¦¾à¦ª)** | à¦¸à§à¦¤à¦°à¦•à§à¦°à¦®à¦¿à¦• à¦¦à§à¦¬à¦¿à¦¤à§€à¦¯à¦¼ à¦¹à§à¦°à¦¾à¦¸à§‡à¦° à¦ªà¦° à¦¸à§à¦¥à¦¿à¦¤à¦¿à¦¶à§€à¦² | à¦¸à§‡à¦®à¦¾à¦¨à§à¦Ÿà¦¿à¦• à¦—à¦­à§€à¦°à¦¤à¦¾ à¦ªà¦°à¦¿à¦ªà§‚à¦°à§à¦£, à¦¸à§à¦¥à¦¿à¦¤à¦¿à¦¸à§à¦¥à¦¾à¦ªà¦• à¦ªà§à¦¨à¦°à§à¦¦à§à¦§à¦¾à¦° à¦•à§à¦·à¦®à¦¤à¦¾ | **à¦—à¦¤à¦¿à¦¶à§€à¦² à¦®à§‚à¦²à§à¦¯à¦¾à¦¯à¦¼à¦¨** (à¦¥à§à¦°à§‡à¦¶à¦¹à§‹à¦²à§à¦¡ à¦¸à¦¨à§à¦¤à§à¦·à§à¦Ÿà¦¿ à¦¬à¦¨à§à¦§) |

---

## ğŸ“Š à¦¡à§‡à¦Ÿà¦¾à¦¸à§‡à¦Ÿ à¦ªà§à¦°à¦¸à§à¦¤à§à¦¤à¦¿ à¦à¦¬à¦‚ Token à¦¸à§à¦•à§‡à¦² à¦…à¦¨à§à¦®à¦¾à¦¨

à¦šà¦¿à¦•à¦¿à§à¦¸à¦¾ à¦•à§à¦·à§‡à¦¤à§à¦°à§‡à¦° à¦•à¦¾à¦œà§‡, à¦•à¦°à§à¦ªà¦¾à¦¸à§‡à¦° à¦¸à§à¦•à§‡à¦² à¦¸à¦°à¦¾à¦¸à¦°à¦¿ â€œà¦¸à§‡à¦®à¦¾à¦¨à§à¦Ÿà¦¿à¦• à¦¸à§à¦¥à¦¿à¦¤à¦¿à¦¸à§à¦¥à¦¾à¦ªà¦•à¦¤à¦¾â€à¦° à¦‰à¦ªà¦°à§‡à¦° à¦¸à§€à¦®à¦¾ à¦¨à¦¿à¦°à§à¦§à¦¾à¦°à¦£ à¦•à¦°à§‡à¥¤ à¦¬à¦¾à¦¸à§à¦¤à¦¬ à¦¯à¦¾à¦šà¦¾à¦‡ à¦…à¦¨à§à¦¸à¦¾à¦°à§‡:
* **à¦¸à§à¦•à§‡à¦² à¦¤à§à¦²à¦¨à¦¾**:
    * **à§¨à§« MB à¦šà§€à¦¨à¦¾ à¦Ÿà§‡à¦•à§à¦¸à¦Ÿ**: à¦ªà§à¦°à¦¾à¦¥à¦®à¦¿à¦• à¦¡à§‡à¦Ÿà¦¾, à¦¶à§à¦§à§à¦®à¦¾à¦¤à§à¦° à¦®à¦¡à§‡à¦²à¦•à§‡ à¦®à§Œà¦²à¦¿à¦• à¦¶à¦¬à§à¦¦à¦¾à¦°à§à¦¥ à¦¸à¦¾à¦®à¦à§à¦œà¦¸à§à¦¯ à¦¸à¦®à§à¦ªà¦¨à§à¦¨ à¦•à¦°à¦¤à§‡ à¦¸à¦•à§à¦·à¦®, à¦Ÿà§‡à¦•à§à¦¸à¦Ÿ à¦¤à§à¦°à§à¦Ÿà¦¿ à¦ªà§à¦°à¦•à§à¦°à¦¿à¦¯à¦¼à¦¾à¦•à¦°à¦£à§‡ à¦¸à§à¦ªà¦·à§à¦Ÿ â€œà¦­à¦¾à¦·à¦¾à¦—à¦¤ à¦…à¦ªà¦°à§à¦¯à¦¾à¦ªà§à¦¤à¦¤à¦¾â€ à¦ªà§à¦°à¦•à¦¾à¦¶ à¦•à¦°à§‡à¥¤
    * **à§¨à§«à§¬ MB à¦šà§€à¦¨à¦¾ à¦Ÿà§‡à¦•à§à¦¸à¦Ÿ**: à¦®à¦¡à§‡à¦² à¦¸à§à¦¥à¦¿à¦¤à¦¿à¦¶à§€à¦² à¦•à§à¦·à§‡à¦¤à§à¦°à§€à¦¯à¦¼ à¦ªà¦°à¦¿à¦®à¦¾à¦°à§à¦œà¦¨ à¦•à§à¦·à¦®à¦¤à¦¾ à¦ªà§à¦°à¦¦à¦°à§à¦¶à¦¨ à¦•à¦°à§‡, à¦šà§‚à¦¡à¦¼à¦¾à¦¨à§à¦¤ à¦¯à¦¾à¦šà¦¾à¦‡ à¦ªà§à¦°à¦¤à§à¦¯à¦¾à¦¶à¦¾ à¦ªà§‚à¦°à¦£ à¦•à¦°à§‡à¥¤(à¦¦à§‡à¦–à§à¦¨ demo)

* **à¦šà§€à¦¨à¦¾ Token à¦°à§‚à¦ªà¦¾à¦¨à§à¦¤à¦° à¦°à§‡à¦«à¦¾à¦°à§‡à¦¨à§à¦¸**ï¼ˆUTF-8 à¦à¦¨à¦•à§‹à¦¡à¦¿à¦‚ à¦à¦¬à¦‚ mT5 à¦Ÿà§‹à¦•à§‡à¦¨à¦¾à¦‡à¦œà¦¾à¦° à¦­à¦¿à¦¤à§à¦¤à¦¿à¦•ï¼‰:

| à¦Ÿà§‡à¦•à§à¦¸à¦Ÿ à¦†à¦•à¦¾à¦° | à¦…à¦¨à§à¦®à¦¿à¦¤ à¦šà§€à¦¨à¦¾ à¦…à¦•à§à¦·à¦° à¦¸à¦‚à¦–à§à¦¯à¦¾ | à¦…à¦¨à§à¦®à¦¿à¦¤ Token à¦®à§‹à¦Ÿ à¦¸à¦‚à¦–à§à¦¯à¦¾ | 
| :--- | :--- | :--- | 
| **à§¨à§« MB** | à¦ªà§à¦°à¦¾à¦¯à¦¼ à§®à§¦à§¦ à¦®à¦¿à¦²à¦¿à¦¯à¦¼à¦¨ à¦…à¦•à§à¦·à¦° | à¦ªà§à¦°à¦¾à¦¯à¦¼ à§§à§¦à§¦à§¦ à¦®à¦¿à¦²à¦¿à¦¯à¦¼à¦¨ | 
| **à§¨à§«à§¬ MB** | à¦ªà§à¦°à¦¾à¦¯à¦¼ à§®à§«à§¦à§¦ à¦®à¦¿à¦²à¦¿à¦¯à¦¼à¦¨ à¦…à¦•à§à¦·à¦° | à¦ªà§à¦°à¦¾à¦¯à¦¼ à§§ à¦¬à¦¿à¦²à¦¿à¦¯à¦¼à¦¨ | 

> **à¦¡à§‡à¦Ÿà¦¾ à¦—à§à¦£à¦®à¦¾à¦¨ à¦Ÿà¦¿à¦ªà¦¸**: à¦‰à¦ªà¦¯à§à¦•à§à¦¤ à¦•à§‹à¦²à¦¾à¦¹à¦² à¦¸à¦‚à¦¯à§‹à¦œà¦¨à§‡à¦° à¦ªà¦°à¦¾à¦®à¦°à§à¦¶ à¦¦à§‡à¦“à¦¯à¦¼à¦¾ à¦¹à¦šà§à¦›à§‡, à¦¬à¦¾à¦¸à§à¦¤à¦¬ à¦šà¦¿à¦•à¦¿à§à¦¸à¦¾ à¦Ÿà§‡à¦•à§à¦¸à¦Ÿ à¦ªà¦°à¦¿à¦¬à§‡à¦¶ à¦…à¦¨à§à¦•à¦°à¦£ à¦•à¦°à¦¤à§‡, à¦®à¦¡à§‡à¦²à¦•à§‡ à¦ªà§à¦°à¦¸à¦™à§à¦— à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à§‡ â€œà¦¸à¦‚à¦¶à§‹à¦§à¦¨â€ à¦¶à§‡à¦–à¦¾à¦¨à§‹à¦° à¦œà¦¨à§à¦¯ à¦œà§‹à¦° à¦¦à§‡à¦“à¦¯à¦¼à¦¾à¥¤

[à¦¡à§‡à¦®à§‹](#Demo) 

---

<a name="russian"></a>
# Ğ ÑƒÑÑĞºĞ¸Ğ¹


## ğŸ“– Ğ¤Ğ¾Ğ½ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ğµ
Ğ­Ñ‚Ğ¾Ñ‚ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ (fine-tuning), ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ **T5** Ğ¸Ğ»Ğ¸ **mT5**.

ĞŸÑ€Ğ¾ĞµĞºÑ‚ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ° Ğ³Ğ»ÑƒĞ±Ğ¾ĞºÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ **ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ**, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğ°Ğ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞ¹ **Â«ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒÑÂ»**, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ĞµĞ¹ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´ĞµÑ„ĞµĞºÑ‚Ñ‹ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ²Ğ½ĞµĞ´Ñ€ÑÑ‚ÑŒ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğ¼Ğ¸ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ°Ğ¼Ğ¸, Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ¾Ğ¹ Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸.

Ğ˜Ğ·-Ğ·Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ğ¹ Ğ² Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ ĞºĞ°Ğº **Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ğ°**, Ğ¸ Ğ¾Ğ±Ñ‰Ğ¸Ğ¼Ğ¸ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ°Ğ¼Ğ¸, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° **Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ»ĞµĞ³ĞºĞ¾ Ğ¿Ğ¾Ğ¿Ğ°Ğ´Ğ°ÑÑ‚ Ğ² Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼ÑƒĞ¼Ñ‹ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ¶Ğ´ĞµĞ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¾ÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¸Ğ·-Ğ·Ğ° ĞºĞ¾Ğ»ĞµĞ±Ğ°Ğ½Ğ¸Ğ¹ Loss**. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹.

**Â«Ğ›ÑƒÑ‡ÑˆĞµ ÑƒĞ¼ĞµÑ€ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ñ‡ĞµĞ¼ Ğ½ĞµĞ¿Ğ¾Ğ»Ğ½Ğ°Ñ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒÂ»**. Ğ”Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹, Ğ½Ğµ Ğ´Ğ¾Ğ¿ÑƒÑĞºĞ°ÑÑ‰Ğ¸Ñ… Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸ĞµĞ¹ Â«ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸Â» Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.

>Ğ˜Ğ·-Ğ·Ğ° Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ°Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ¾Ğ². Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¹ ÑĞ¼. Requirements.

---

## âœ… ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸

* **ĞœĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ€Ğ°Ğ·Ğ¾Ğ³Ñ€ĞµĞ²Ğ°**: Ğ§ĞµÑ€ĞµĞ· ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºÑƒ Ğ¿Ğ¾Ñ€Ğ¾Ğ³Ğ° `start_step` Ğ¿Ñ€Ğ¸Ğ½ÑƒĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸Ğ·Ğ±ĞµĞ³Ğ°ĞµÑ‚ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ñ… ĞºĞ¾Ğ»ĞµĞ±Ğ°Ğ½Ğ¸Ğ¹. (Ğ¥Ğ¾Ğ»Ğ¾Ğ´Ğ½Ñ‹Ğ¹ Ğ·Ğ°Ğ¿ÑƒÑĞº)
* **ĞÑ†ĞµĞ½ĞºĞ° Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸Ğ¸ Loss Ğ² Ğ¾ĞºĞ½Ğµ**: Ğ§ĞµÑ€ĞµĞ· Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ `patience` Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Loss ĞºĞ¾Ğ»ĞµĞ±Ğ°Ñ‚ÑŒÑÑ Ğ¸Ğ»Ğ¸ stagnate Ğ² Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´, Ğ¾ÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ÑÑÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ¾Ğ³Ğ´Ğ° Loss Ğ½Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹ Ñ€ĞµĞºĞ¾Ñ€Ğ´ Ğ² Ñ‚ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ², Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°Ñ Ğ¿Ñ€ĞµĞ¶Ğ´ĞµĞ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºÑƒ Ğ¸Ğ·-Ğ·Ğ° Ğ»Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Â«Ğ¿Ğ»Ğ°Ñ‚Ğ¾Â» Ğ¾Ñ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ»ĞµĞ±Ğ°Ğ½Ğ¸Ğ¹.
* **ĞÑ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ**: `SafeDetailedProgressCallback` Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ learning rate Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ETA (Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµĞ¼Ğ°Ñ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ°), Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.
* **Ğ ĞµĞ·ĞµÑ€Ğ²Ğ½Ğ¾Ğµ ĞºĞ¾Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ğ²Ğ¾Ğ·Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ°**: Ğ”Ğ»Ñ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ… Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ (KeyboardInterrupt Handling) *Ctrl+C* Ğ¸ Ñ€ĞµĞ·ĞµÑ€Ğ²Ğ½Ğ¾Ğµ ĞºĞ¾Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²ĞµÑĞ¾Ğ² (Best Weights) Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ²ĞµÑĞ¾Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ² ÑĞ»ÑƒÑ‡Ğ°Ğµ Ğ²Ğ½ĞµĞ·Ğ°Ğ¿Ğ½Ñ‹Ñ… ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ğ¹.

---

## ğŸ› ï¸ Ğ¢ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ (Technical Deep-Dive)

### 1. ĞœĞ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ (Multi-stage Convergence Analysis)
Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, ĞºÑ€Ğ¸Ğ²Ğ°Ñ Loss Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ñ‹ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¸Ğ¼ĞµĞµÑ‚ Â«Ğ»ĞµÑÑ‚Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ğ¹Â» Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€ ÑĞ¿ÑƒÑĞºĞ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚ Ğ·Ğ°Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ½Ğ° **Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸Ğ¸ Loss Ğ² Ğ¾ĞºĞ½Ğµ**:
* **Ğ˜Ğ·Ğ±ĞµĞ¶Ğ°Ğ½Ğ¸Ğµ Â«Ğ¿ÑĞµĞ²Ğ´Ğ¾Ğ¿Ğ»Ğ°Ñ‚Ğ¾Â»**: ĞĞ° Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° Ğ´Ğ¾Ğ¼ĞµĞ½Ğ° T5 Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµÑ‚ÑÑ Ğ¿Ğ»Ğ°Ñ‚Ğ¾ Ñ ÑĞ»Ğ°Ğ±Ñ‹Ğ¼ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Loss. Ğ•ÑĞ»Ğ¸ Ğ² ÑÑ‚Ğ¾Ñ‚ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚ ÑÑ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ€Ğ°Ğ½Ğ½ÑÑ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ°, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ±ÑƒĞ´ĞµÑ‚ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ñ‡ÑƒÑ‚ÑŒÑ‘Ğ¼, Ğ±ĞµĞ· Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ğ½ĞºĞ¸ Ğ¿Ğ¾Ğ´ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºÑƒÑ Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ.
* **Ğ›Ğ¾Ğ³Ğ¸ĞºĞ° Ğ¾Ñ‚Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑÑ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ**: Ğ§ĞµÑ€ĞµĞ· `DelayedEarlyStopping` Ğ¿Ñ€Ğ¸Ğ½ÑƒĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚ĞºĞ»Ğ°Ğ´Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ¸Ñ‚ÑŒ **Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ñ‡Ğ½ÑƒÑ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ (Secondary Convergence)** Ğ¿Ğ¾ÑĞ»Ğµ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ñ‚Ğ¾.

**Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾ÑĞ»Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ² Ğ¾ĞºĞ¾Ğ½ Loss, Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ´Ğ¸Ğ²ÑˆĞ¸Ñ… Ğ²Ñ…Ğ¾Ğ´ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Â«ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ½Ğ°ÑÑ‹Ñ‰ĞµĞ½Ğ¸ÑÂ», ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ñ‹Ğ´Ğ°ÑÑ‚ ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸.**


### 2. ĞšĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹ÑÑˆĞ¸Ñ… Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² (Gradient Dynamics Control)
Ğ”Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ², Ğ²Ñ‹Ğ·Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ»ĞµĞºÑĞ¸ĞºĞ¸, Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ:
* **ĞĞ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² (Gradient Accumulation)**: Ğ§ĞµÑ€ĞµĞ· `gradient_accumulation_steps=8` **ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‚ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ¸ ÑĞ³Ğ»Ğ°Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚** Ğ¼Ğ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ ÑƒĞ´Ğ°Ñ€Ñ‹ Ğ¾Ñ‚ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½ÑƒÑ ÑÑ€ĞµĞ´Ñƒ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ Batch Size.
* **ĞÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ğ°Ñ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸**: Ğ’ ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğ¸ Ñ `eval_steps=1000` Ğ² Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ¾Ğ¹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ²ĞµÑĞ°, Ğ·Ğ°Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ `load_best_model_at_end`, Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°Ğ»Ğ¸ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ½Ğ° ĞºÑ€Ğ¾ÑÑ-Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞµ.
* **ĞÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ğ°Ñ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ° Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ°**: ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° `logging_steps=100` Ğ¸ `eval_steps=1000`. ĞŸÑ€Ğ¸ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğµ Ñ‚ĞµĞ»ĞµĞ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ (Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²) ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñƒ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ° Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞµ, ĞºĞ¾Ğ½Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ².

---
## ğŸ”¬ Ğ˜Ğ½ÑĞ°Ğ¹Ñ‚Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: ĞŸĞ¾Ñ‡ĞµĞ¼Ñƒ Ğ½ÑƒĞ¶ĞµĞ½ Â«Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºÑ€Ğ°Ñ‚Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· LossÂ»?

Ğ’ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ´Ğ¾Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ½Ğ° ÑÑ‚Ğ¾Ñ‚ Ñ€Ğ°Ğ· Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ğ° Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ°Ñ:

| Ğ­Ñ‚Ğ°Ğ¿ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ | ĞŸÑ€Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº Loss | ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ | ĞÑ‚Ğ²ĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ |
| :--- | :--- | :--- | :--- |
| **ĞĞ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ (0-6000 ÑˆĞ°Ğ³Ğ¾Ğ²)** | Ğ¡Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ»ĞµĞ±Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ·Ğ°Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ¸Ğµ | Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² | **ĞŸÑ€Ğ¸Ğ½ÑƒĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½Ğ¸Ğµ** (Ğ·Ğ°Ğ¿Ñ€ĞµÑ‚ Ñ€Ğ°Ğ½ĞµĞµ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ğ²ÑˆĞµĞ¹ Ñ€Ğ°Ğ½Ğ½ĞµĞ¹ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸) |
| **Ğ¡Ñ€ĞµĞ´Ğ½Ğ¸Ğ¹ (6000-12000 ÑˆĞ°Ğ³Ğ¾Ğ²)** | ĞŸĞ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ñ‚Ğ¾ (Ğ»Ğ¾Ğ¶Ğ½Ğ°Ñ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ) | Ğ’Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ´ĞµÑ„ĞµĞºÑ‚Ğ¾Ğ² Ñ‚ĞµĞºÑÑ‚Ğ° | **ĞŸÑ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ** (Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸Ğ¹ Ğ² Ğ¾ĞºĞ½Ğµ) |
| **ĞŸĞ¾Ğ·Ğ´Ğ½Ğ¸Ğ¹ (12000+ ÑˆĞ°Ğ³Ğ¾Ğ²)** | Ğ¡Ñ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ğ¾Ğµ Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¹ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ | ĞĞ°ÑÑ‹Ñ‰ĞµĞ½Ğ¸Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹, Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ | **Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ°** (Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾Ñ€Ğ¾Ğ³Ğ°) |

---

## ğŸ“Š ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Token 

Ğ’ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ± ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ° Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ²ĞµÑ€Ñ…Ğ½Ğ¸Ğ¹ Ğ¿Ñ€ĞµĞ´ĞµĞ» Â«ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸Â». Ğ¡Ğ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ½Ğ° Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞµ:
* **Ğ¡Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¾Ğ²**:
    * **25 ĞœĞ‘ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°**: ĞŸÑ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¾Ğ², Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ´ĞµÑ„ĞµĞºÑ‚Ğ¾Ğ² Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¿Ñ€Ğ¾ÑĞ²Ğ»ÑÑÑ‚ ÑĞ²Ğ½ÑƒÑ Â«Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÂ».
    * **256 ĞœĞ‘ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°**: ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ğ¹ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. (ÑĞ¼. demo)

* **Ğ¡Ğ¿Ñ€Ğ°Ğ²Ğ¾Ñ‡Ğ½Ğ¸Ğº Ğ¿Ğ¾ ĞºĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ñ… Token** (Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ UTF-8 Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° mT5):

| Ğ Ğ°Ğ·Ğ¼ĞµÑ€ Ñ‚ĞµĞºÑÑ‚Ğ° | ĞÑ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ñ… ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² | ĞÑ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±Ñ‰ĞµĞµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Token | 
| :--- | :--- | :--- | 
| **25 ĞœĞ‘** | ĞĞºĞ¾Ğ»Ğ¾ 8 Ğ¼Ğ»Ğ½ Ğ¸ĞµÑ€Ğ¾Ğ³Ğ»Ğ¸Ñ„Ğ¾Ğ² | ĞĞºĞ¾Ğ»Ğ¾ 10 Ğ¼Ğ»Ğ½ | 
| **256 ĞœĞ‘** | ĞĞºĞ¾Ğ»Ğ¾ 85 Ğ¼Ğ»Ğ½ Ğ¸ĞµÑ€Ğ¾Ğ³Ğ»Ğ¸Ñ„Ğ¾Ğ² | ĞĞºĞ¾Ğ»Ğ¾ 100 Ğ¼Ğ»Ğ½ | 

> **Ğ¡Ğ¾Ğ²ĞµÑ‚Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…**: Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ÑƒĞ¼ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğ¹ ÑˆÑƒĞ¼, Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑ Ñ€ĞµĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºÑƒÑ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²ÑƒÑ ÑÑ€ĞµĞ´Ñƒ, Ğ·Ğ°ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ»Ñ Â«ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸Â».

[Ğ”ĞµĞ¼Ğ¾](#Demo) 

---

<a name="italian"></a>
# Italiano


## ğŸ“– Contesto e Visione
Questo repository fornisce un framework di addestramento fine-tuning personalizzato per l'architettura **T5** o **mT5**.

Il progetto mira a ottimizzare profondamente le **strategie di addestramento**, conferendo al modello una **â€œresilienza semanticaâ€** intrinseca, permettendogli di gestire in modo piÃ¹ robusto le mancanze di testo e di iniettare con precisione conoscenze specialistiche del dominio quando affronta testi ad alta densitÃ  informativa come report medici e letteratura professionale.

A causa delle significative differenze di distribuzione tra testi professionali come la **medicina** e i corpus generali, il modello Ã¨ prone a cadere in ottimi locali o a fermarsi precocemente a causa di fluttuazioni della Loss nelle fasi iniziali di **fine-tuning**. Questo progetto introduce meccanismi di ottimizzazione per risolvere questo problema.

**â€œMeglio un lieve sovradattamento che una convergenza incompletaâ€**. Per domini professionali che non tollerano ambiguitÃ , un numero maggiore di passi di addestramento Ã¨ la garanzia di base per l'**â€œaffidabilitÃ  semanticaâ€** del modello.

> A causa delle limitate prestazioni dei dispositivi per il deployment locale, ci sono molte compromissioni nelle impostazioni. Elenco specifico delle configurazioni vedi Requirements.

---

## âœ… FunzionalitÃ  Principali

* **Meccanismo di preriscaldamento**ï¼šAttraverso la soglia `start_step`, forza l'evitamento delle fluttuazioni casuali locali instabili iniziali. (Cold start)
* **Valutazione della tendenza Loss a finestra**ï¼šAttraverso l'impostazione `patience`, permette alla Loss di fluttuare o stagnare per un certo periodo, fermandosi solo quando la Loss non aggiorna il record ottimale per piÃ¹ fasi consecutive, prevenendo fermate premature dovute a â€œplateauâ€ falsi causati da fluttuazioni temporanee.
* **Tracciamento dello stato**ï¼š`SafeDetailedProgressCallback` fornisce l'evoluzione in tempo reale del learning rate e previsioni dinamiche ETA (frequenza regolabile), supportando il monitoraggio trasparente diä½œä¸š di addestramento a lungo termine.
* **Backup in tempo reale e ripresa da checkpoint**ï¼šPer scenari di addestramento ad alta intensitÃ  come la ricerca medica, integra interruzione manuale (KeyboardInterrupt Handling) *Ctrl+C* e backup in tempo reale, garantendo il salvataggio completo possibile dei pesi ottimali (Best Weights) e di piÃ¹ pesi di processo in caso di interruzioni improvvise.

---

## ğŸ› ï¸ Dettagli di Implementazione Tecnica (Technical Deep-Dive)

### 1. Meccanismo di Discriminazione della Convergenza Multi-fase (Multi-stage Convergence Analysis)
Diversamente dai task generali, la curva Loss nei task di fine-tuning medico presenta spesso una caratteristica di â€œdiscesa a scaliniâ€. Questo progetto sostituisce il giudizio istantaneo con la **valutazione della tendenza Loss a finestra**:
* **Evitare intervalli â€œpseudo-pianiâ€**ï¼šT5 nelle fasi iniziali di trasferimento di dominio spesso mostra un plateau con discesa debole della Loss. Se in quel momento si attiva l'early stopping, il modello possiede solo una sensibilitÃ  linguistica di base, mancando di un adattamento profondo alla logica medica.
* **Logica di attivazione ritardata**ï¼šAttraverso `DelayedEarlyStopping`, si forza un rinvio del giudizio per catturare la **convergenza secondaria (Secondary Convergence)** dopo il primo plateau.

**Solo dopo multiple analisi di finestre Loss, confermando che il modello Ã¨ entrato in stato di â€œsaturazione semanticaâ€, il sistema emette il segnale di stop.**


### 2. Controllo della StabilitÃ  dei Gradienti di Ordine Superiore (Gradient Dynamics Control)
Per il problema di instabilitÃ  dei gradienti causato dalla distribuzione sparsa del vocabolario medico professionale, il framework ha ottimizzazioni a livello basso:
* **Accumulo di gradienti (Gradient Accumulation)**ï¼šAttraverso `gradient_accumulation_steps=8`, **risparmia VRAM e smorza l'impatto istantaneo dei gradienti** portato da frasi lunghe e difficili, simulando un ambiente di aggiornamento con Batch Size grande e stabile.
* **Frequenza di valutazione asimmetrica**ï¼šIn combinazione con `eval_steps=1000`, durante l'addestramento a lungo termine, esegue salvataggi di selezione ottimale ad alta precisione con bassa frequenza, garantendo che i pesi bloccati da `load_best_model_at_end` abbiano vera robustezza cross-sample.
* **Frequenza di monitoraggio asimmetrica**ï¼šConfigurazione `logging_steps=100` con `eval_steps=1000`. Garantendo telemetria ad alta frequenza (monitoraggio se i gradienti sono normali), riduce la frequenza delle valutazioni costose sul validation set, concentrando la potenza di calcolo sugli aggiornamenti dei parametri.

---
## ğŸ”¬ Insight di addestramento: perchÃ© Ã¨ necessario l'"analisi multipla della Loss"?

In questo compito di rifinitura medica, la matrice di determinazione della convergenza Ã¨ la seguente:

| Fase di addestramento | Manifestazione delle caratteristiche Loss | Stato semantico principale | Risposta strategica |
| :--- | :--- | :--- | :--- |
| **Iniziale (0-6000 passi)** | Oscillazioni violente o discesa lenta | Stabilimento del senso del dominio, allineamento iniziale dei parametri | **Forzare la continuazione** (vietare l'arresto anticipato verificatosi in passato) |
| **MetÃ  (6000-12000 passi)** | Periodo di plateau lungo (pseudo-convergenza) | Iniezione di conoscenza professionale, gestione dei difetti testuali | **Osservazione continua** (analisi di tendenza a finestra) |
| **Tarda (12000+ passi)** | Discesa secondaria a scalini seguita da stabilizzazione | Saturazione della profonditÃ  semantica, capacitÃ  di ripristino resiliente | **Valutazione dinamica** (arresto al soddisfacimento della soglia) |

---

## ğŸ“Š Preparazione del dataset e stima della scala dei Token 

Nel compito del dominio medico, la scala del corpus determina direttamente il limite superiore della "resilienza semantica". Secondo la valutazione pratica:
* **Confronto scala**:
    * **25MB di testo cinese**: Dati preliminari, in grado di supportare solo l'allineamento base dei termini, con manifesta "carenza di senso linguistico" nella gestione dei difetti testuali.
    * **256MB di testo cinese**: Il modello mostra capacitÃ  stabili di rifinitura del dominio, raggiungendo le aspettative di valutazione finale. (vedi demo)

* **Riferimento di conversione Token cinese** (basato su codifica UTF-8 e tokenizer mT5):

| Dimensione testo | Numero stimato di caratteri cinesi | Totale Token stimato | 
| :--- | :--- | :--- | 
| **25 MB** | Circa 8 milioni di caratteri | Circa 10 milioni | 
| **256 MB** | Circa 85 milioni di caratteri | Circa 100 milioni | 

> **Suggerimenti qualitÃ  dati**: Si consiglia di iniettare rumore moderato, simulando l'ambiente testuale medico reale, forzando il modello a imparare come utilizzare il contesto per "correggere".

[Demo](#Demo) 

---

<a name="dutch"></a>
# Nederlands


## ğŸ“– Achtergrond en visie
Dit repository biedt een op maat gemaakt finetuning-trainingsframework voor de **T5** of **mT5** architectuur.

Het project streeft ernaar om door diepgaande optimalisatie van **trainingsstrategieÃ«n** het model een inherente **â€œsemantische veerkrachtâ€** te geven, zodat het bij het omgaan met medische rapporten, professionele literatuur en andere teksten met hoge informatiemeldichtheid robuuster tekstdefecten kan verwerken en domeinspecifieke kennis nauwkeurig kan injecteren.

Vanwege de significante distributieverschillen tussen professionele teksten zoals **medische** en algemene corpora, is het model in de **vroege finetuningfase zeer vatbaar voor lokale optima of vroegtijdige stopping door Loss-fluctuaties**; dit project introduceert mechanismen om dit probleem te optimaliseren.

**â€œLiever matige overfitting dan onvolledige convergentieâ€**. Voor domeinen waar ambiguÃ¯teit niet is toegestaan, vormt het verhogen van het aantal trainingsstappen de onderliggende garantie voor de â€œsemantische betrouwbaarheidâ€ van het model.

>Vanwege beperkte prestaties van lokaal gedeployde apparatuur zijn er veel compromissen in de instellingen. Zie Requirements voor de specifieke configuratielijst.

---

## âœ… Kernfuncties

* **Warm-up mechanisme**: Door het instellen van een `start_step` drempelwaarde, dwingt het het vermijden van vroege instabiele lokale willekeurige fluctuaties. (koude start)
* **Gewogen Loss-trendbeoordeling**: Via de `patience`-instelling staat het toe dat Loss binnen een bepaalde periode fluctueert of stagneert, en stopt alleen als Loss meerdere fasen achtereen geen nieuwe beste record vestigt, om te voorkomen dat het model te vroeg stopt door valse â€œplateauâ€-periodes veroorzaakt door tijdelijke fluctuaties.
* **Status tracking**: `SafeDetailedProgressCallback` biedt realtime evolutie van de leercurve en dynamische ETA-voorspelling (aanpasbare frequentie), ter ondersteuning van transparante monitoring van langdurige trainingsopdrachten.
* **Realtime back-up en hervatting vanaf breakpoint**: Voor medische onderzoeksscenarioâ€™s met hoge rekentijd, ingebouwd handmatig onderbreken (KeyboardInterrupt Handling) *Ctrl+C* en realtime back-up, om ervoor te zorgen dat bij onverwachte situaties de optimale gewichten (Best Weights) en meerdere procesgewichten zo volledig mogelijk worden opgeslagen.

---

## ğŸ› ï¸ Technische implementatiedetails (Technical Deep-Dive)

### 1. Multi-stage convergentiebeoordelingsmechanisme (Multi-stage Convergence Analysis)
In tegenstelling tot algemene taken vertoont de Loss-curve van medische finetuningtaken vaak een â€œtrappenvormige dalingâ€. Dit project vervangt momentane beoordeling door **gewogen Loss-trendbeoordeling**:
* **Vermijden van â€œpseudo-plateauâ€-gebieden**: T5 vertoont in de vroege domeinmigratiefase vaak een plateau met zwakke Loss-daling. Als vroegtijdige stopping hier wordt geactiveerd, heeft het model alleen basale taalbegrip, maar mist diepe aanpassing aan medische logica.
* **Vertraagde activeringslogica**: Door `DelayedEarlyStopping` wordt de beoordeling geforceerd uitgesteld om de **secundaire convergentie (Secondary Convergence)** na het eerste plateau te vangen.

**Alleen na meerdere Loss-vensteranalyses, waarbij wordt bevestigd dat het model in een â€œsemantische verzadigingstoestandâ€ is, geeft het systeem een stopsignaal.**


### 2. Geavanceerde gradiÃ«ntstabiliteitscontrole (Gradient Dynamics Control)
Voor het probleem van gradiÃ«ntonstabiliteit veroorzaakt door schaarse distributie van medische vakterminologie, heeft het framework optimalisaties op laag niveau doorgevoerd:
* **GradiÃ«ntaccumulatie (Gradient Accumulation)**: Via `gradient_accumulation_steps=8` **spaar videogeheugen en verzacht tegelijkertijd de plotselinge gradiÃ«ntschok van lange moeilijke zinnen**, om een stabiele grote Batch Size-updateomgeving te simuleren.
* **Asymmetrische evaluatiefrequentie**: In combinatie met `eval_steps=1000` voert het in langdurige training met lage frequentie hoogpreciestere selectieve opslag uit, om ervoor te zorgen dat de door `load_best_model_at_end` vastgelegde gewichten echte robuustheid over monsters hebben.
* **Asymmetrische monitoringfrequentie**: Configuratie van `logging_steps=100` en `eval_steps=1000`. Dit garandeert hoge-frequentie telemetrie (monitoring of gradiÃ«nten normaal zijn) terwijl de frequentie van dure validatieset-evaluaties wordt verlaagd, zodat de rekenkracht wordt geconcentreerd op parameterupdates.

---
## ğŸ”¬ Trainingsinzichten: Waarom is "meerdere Loss-analyses" nodig?

In deze medische verfijningstaak luidt de convergentiematrix als volgt:

| Trainingsfase | Loss-kenmerken | Kernsemantische status | Strategische respons |
| :--- | :--- | :--- | :--- |
| **InitiÃ«le fase (0-6000 stappen)** | Sterke oscillaties of langzame daling | Domeinspecifiek taangevoel opbouwen, initiÃ«le parameteruitlijning | **Gedwongen voortzetting** (vroegtijdige stopzetting verboden) |
| **Middenfase (6000-12000 stappen)** | Lange plateaufase (pseudo-convergentie) | Injectie van professionele kennis, omgaan met tekstdefecten | **Voortdurende observatie** (venstergebaseerde trendanalyse) |
| **Laatste fase (12000+ stappen)** | Trapsgewijze secundaire daling gevolgd door stabilisatie | Semantische diepgang verzadigd, veerkracht in restauratie | **Dynamische evaluatie** (stoppen bij voldoening aan drempel) |

---

## ğŸ“Š Datasetvoorbereiding en Token-schaalraming 

In medische domeintaken bepaalt de schaal van het corpus direct de bovengrens van de "semantische veerkracht". Op basis van praktijkbeoordelingen:
* **Schaalvergelijking**:
    * **25 MB Chineestalige tekst**: Voorlopige data, ondersteunt alleen basisuitlijning van terminologie, toont duidelijke "taalgevoelsdeficiÃ«ntie" bij tekstdefecten.
    * **256 MB Chineestalige tekst**: Model toont stabiele domeinspecifieke verfijningscapaciteit, bereikt de verwachte eindbeoordeling. (Zie demo)

* **Referentie voor conversie van Chinese Tokens** (gebaseerd op UTF-8-codering en mT5-tokenizer):

| Tekstgrootte | Geschat aantal Chinese karakters | Geschat totaal aantal Tokens | 
| :--- | :--- | :--- | 
| **25 MB** | Ca. 8 miljoen tekens | Ca. 10 miljoen | 
| **256 MB** | Ca. 85 miljoen tekens | Ca. 100 miljoen | 

> **Tips voor gegevenskwaliteit**: Voeg matige ruis toe om echte medische tekstomgevingen te simuleren, dwing het model om context te gebruiken voor "correctie".

[Demo](#Demo) 

---

<a name="svenska"></a>
# Svenska


## ğŸ“– Bakgrund och vision
Detta repository tillhandahÃ¥ller ett skrÃ¤ddarsytt finjusteringsramverk fÃ¶r **T5** eller **mT5** arkitektur.

Projektet syftar till att genom djup optimering av **trÃ¤ningsstrategier** ge modellen en inneboende **"semantisk motstÃ¥ndskraft"**, sÃ¥ att den kan hantera textbrister mer robust och injicera domÃ¤nspecifik kunskap precist vid hantering av medicinska rapporter, professionell litteratur och andra texter med hÃ¶g informationsdensitet.

PÃ¥ grund av den betydande distributionsskillnaden mellan **medicinska** och allmÃ¤nna korpusar tenderar modeller att i **tidiga finjusteringsskeden hamna i lokala optima eller tidig stoppning pÃ¥ grund av Loss-fluktuationer**. Detta projekt introducerar mekanismoptimeringar fÃ¶r att lÃ¶sa detta problem.

**"Hellre mÃ¥ttlig Ã¶veranpassning Ã¤n ofullstÃ¤ndig konvergens"**. FÃ¶r domÃ¤ner som inte tillÃ¥ter tvetydighet Ã¤r Ã¶kade trÃ¤ningssteg en grundlÃ¤ggande garanti fÃ¶r modellens "semantiska tillfÃ¶rlitlighet".

>Med tanke pÃ¥ begrÃ¤nsad prestanda i lokalt deployade enheter finns mÃ¥nga kompromisser i instÃ¤llningarna. Specifik konfigurationslista finns i Requirements.

---

## âœ… KÃ¤rnfunktioner

* **UppvÃ¤rmningsmekanism**: Genom att ange `start_step` trÃ¶skel tvingas undvikande av initiala instabila lokala slumpmÃ¤ssiga fluktuationer. (Kallstart)
* **FÃ¶nsterbaserad Loss-trendbedÃ¶mning**: Genom `patience`-instÃ¤llning tillÃ¥ts Loss-fluktuationer eller stagnation inom en viss period, och stopp sker endast nÃ¤r Loss kontinuerligt misslyckas med att uppdatera bÃ¤sta rekord Ã¶ver flera faser, vilket fÃ¶rhindrar fÃ¶r tidigt stopp pÃ¥ grund av tillfÃ¤lliga fluktuationer som skapar falska "platÃ¥perioder".
* **StatusspÃ¥rning**: `SafeDetailedProgressCallback` ger realtidsvisning av inlÃ¤rningshastighetsutveckling och dynamisk ETA-prediktion (justerbar frekvens), och stÃ¶djer transparent Ã¶vervakning av lÃ¥ngvariga trÃ¤ningsjobb.
* **Realtidsbackup och checkpointÃ¥terupptagning**: FÃ¶r medicinska forskningsintensiva trÃ¤ningscenarier med inbyggd hantering av manuell avbrott (KeyboardInterrupt Handling) *Ctrl+C* och realtidsbackup, sÃ¤kerstÃ¤lls sÃ¥ komplett som mÃ¶jligt bevarande av modellens bÃ¤sta vikter (Best Weights) och flera processvikter vid ovÃ¤ntade hÃ¤ndelser.

---

## ğŸ› ï¸ Tekniska implementeringsdetaljer (Technical Deep-Dive)

### 1. Multistegs konvergensbedÃ¶mningsmekanism (Multi-stage Convergence Analysis)
Till skillnad frÃ¥n allmÃ¤nna uppgifter uppvisar Loss-kurvan fÃ¶r medicinska finjusteringsuppgifter ofta "trappstegsnedgÃ¥ng". Detta projekt ersÃ¤tter momentan bedÃ¶mning med **fÃ¶nsterbaserad Loss-trendbedÃ¶mning**:
* **Undvikande av "pseudo-platta" intervaller**: T5 uppvisar ofta svag Loss-nedgÃ¥ng i plattformar under tidiga domÃ¤nÃ¶verfÃ¶ringar. Om tidig stoppning utlÃ¶ses dÃ¥ har modellen endast grundlÃ¤ggande sprÃ¥kfÃ¶rstÃ¥else, men saknar djup anpassning till medicinsk logik.
* **FÃ¶rdrÃ¶jd utlÃ¶sningslogik**: Genom `DelayedEarlyStopping` tvingas fÃ¶rdrÃ¶jd bedÃ¶mning fÃ¶r att fÃ¥nga **sekundÃ¤r konvergens (Secondary Convergence)** efter den fÃ¶rsta plattformen.

**Endast efter flera Loss-fÃ¶nsteranalyser, nÃ¤r modellen bekrÃ¤ftats ha nÃ¥tt "semantisk mÃ¤ttnad", skickas stoppsignalen.**


### 2. HÃ¶gordningens gradientstabilitetskontroll (Gradient Dynamics Control)
FÃ¶r att hantera gradientinstabilitet orsakad av sparsamma medicinska specialisttermer har ramverket optimerats pÃ¥ lÃ¥g nivÃ¥:
* **Gradientackumulering (Gradient Accumulation)**: Genom `gradient_accumulation_steps=8` **sparas minne samtidigt som lÃ¥nga svÃ¥ra meningar** slÃ¤ta ut momentana gradientstÃ¶tar, och simulerar stabila uppdateringar med stor Batch Size.
* **Asymmetrisk utvÃ¤rderingsfrekvens**: Tillsammans med `eval_steps=1000` utfÃ¶rs hÃ¶gprecisionsval av bÃ¤sta modell med lÃ¤gre frekvens under lÃ¥ngvarig trÃ¤ning, vilket sÃ¤kerstÃ¤ller att `load_best_model_at_end` lÃ¥ser vikter med robusthet Ã¶ver prover.
* **Asymmetrisk Ã¶vervakningsfrekvens**: Konfigurera `logging_steps=100` och `eval_steps=1000`. Samtidigt som hÃ¶gfrejent telemetri (Ã¶vervakning av gradientnormalitet) garanteras, minskas frekvensen fÃ¶r resurskrÃ¤vande valideringsuppsÃ¤ttning, sÃ¥ att berÃ¤kningskraften fokuseras pÃ¥ parameteruppdateringar.

---
## ğŸ”¬ TrÃ¤ningsinsikter: VarfÃ¶r behÃ¶vs â€flera Loss-analyserâ€œ?

I denna medicinska finjusteringsuppgift Ã¤r konvergensbedÃ¶mningsmatrisen fÃ¶ljande:

| TrÃ¤ningsfas | Loss-karaktÃ¤rsuttryck | KÃ¤rnsemantiskt tillstÃ¥nd | Strategisvar |
| :--- | :--- | :--- | :--- |
| **Tidig fas (0-6000 steg)** | Kraftiga oscillationer eller lÃ¥ngsam nedgÃ¥ng | Etablering av domÃ¤nkÃ¤nslan, initial parameterjustering | **Tvinga fortsÃ¤tta** (blockera tidigare tidig stopp) |
| **Mellanas (6000-12000 steg)** | UppstÃ¥r lÃ¥ng platÃ¥period (falsk konvergens) | Injektion av specialistkunskap, hantering av textbrister | **Fortsatt observation** (fÃ¶nsterbaserad trendanalys) |
| **Sen fas (12000+ steg)** | Trappvis sekundÃ¤r nedgÃ¥ng fÃ¶ljt av platt | Semantisk djupmÃ¤ttnad, med motstÃ¥ndskraftig Ã¥terstÃ¤llning | **Dynamisk utvÃ¤rdering** (stoppa vid trÃ¶skelvÃ¤rde) |

---

## ğŸ“Š DatasetfÃ¶rberedelse och Token-skalauppskattning 

I medicinska domÃ¤nuppgifter bestÃ¤mmer korpusens skala direkt Ã¶vre grÃ¤nsen fÃ¶r â€semantisk motstÃ¥ndskraftâ€œ. Enligt praktiska utvÃ¤rderingar:
* **SkaljÃ¤mfÃ¶relse**:
    * **25 MB kinesisk text**: PreliminÃ¤ra data, som endast stÃ¶djer modellen fÃ¶r grundlÃ¤ggande termjustering, visar tydlig â€brist pÃ¥ kÃ¤nslanâ€œ vid hantering av textbrister.
    * **256 MB kinesisk text**: Modellen visar stabil domÃ¤nfinjustering, nÃ¥r fÃ¶rvÃ¤ntade slututvÃ¤rderingar. (Se demo)

* **Kinesisk Token-omvandlingsreferens** (baserat pÃ¥ UTF-8-kodning och mT5-tokeniserare):

| Textstorlek | Uppskattat antal kinesiska tecken | Uppskattad total Token-mÃ¤ngd | 
| :--- | :--- | :--- | 
| **25 MB** | Ca 8 miljoner tecken | Ca 10 miljoner | 
| **256 MB** | Ca 85 miljoner tecken | Ca 100 miljoner | 

> **Datakvalitetstips**: FÃ¶reslÃ¥ att injicera mÃ¥ttligt brus fÃ¶r att simulera verkliga medicinska textmiljÃ¶er, tvinga modellen att lÃ¤ra sig hur man anvÃ¤nder kontext fÃ¶r â€korrigeringâ€œ.

[Demo](#Demo) 

---

