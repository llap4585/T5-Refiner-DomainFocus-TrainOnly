# T5-Refiner-DomainFocus-TrainOnly
# ÁªèËøáT5-Refiner-DomainFocusÈ¢ÑÂ§ÑÁêÜÂêéÁöÑÊï∞ÊçÆÂæÆË∞ÉËÆ≠ÁªÉ‰ª£Á†Å

![Views](https://komarev.com/ghpvc/?username=llap4585&repo=T5-Refiner-DomainFocus-TrainOnly&label=Project%20Views&color=blue&style=flat-square)

If you like this project, give it a ‚≠êÔ∏è on GitHub!  
Your support keeps the project going and motivates me to improve it. üòÑ

> It is recommended that the data be preprocessed using the following project:  
> [T5-Refiner-DomainFocus](https://github.com/llap4585/T5-Refiner-DomainFocus)
>
> Êï∞ÊçÆÂª∫ËÆÆÁªèËøá‰ª•‰∏ãÈ°πÁõÆÈ¢ÑÂ§ÑÁêÜÔºö
> [T5-Refiner-DomainFocus](https://github.com/llap4585/T5-Refiner-DomainFocus)
>
> „Éá„Éº„Çø„ÅØ„ÄÅ‰ª•‰∏ã„ÅÆ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„Çí‰ΩøÁî®„Åó„Å¶ÂâçÂá¶ÁêÜ„Åô„Çã„Åì„Å®„Çí„Åä„Åô„Åô„ÇÅ„Åó„Åæ„ÅôÔºàÊ©üÊ¢∞ÁøªË®≥ÔºâÔºö  
> [T5-Refiner-DomainFocus](https://github.com/llap4585/T5-Refiner-DomainFocus)

---
<a name="Introduction"></a>
## Introduction
[‚≠êÔ∏èEnglish](#english) | [‚≠êÔ∏è‰∏≠Êñá](#chinese)

*Machine translation (Grok) /Ê©üÊ¢∞ÁøªË®≥:*

[Êó•Êú¨Ë™û](#japanese) | [Deutsch](#deutsch) | [Fran√ßais](#francais) | [Espa√±ol](#espanol) | [‡§π‡§ø‡§®‡•ç‡§¶‡•Ä](#hindi) | [ÌïúÍµ≠Ïñ¥](#korean) | [Portugu√™s](#portuguese)

### Introduction to Other Languages 

‚Äî **one-time *quick* machine translation only**, provided according to the version as of February 2, 2026:

Arabic ÿßŸÑÿπÿ±ÿ®Ÿäÿ©, Bengali ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ, Russian —Ä—É—Å—Å–∫–∏–π, Italian italiano, Dutch Nederlands, Swedish svenska

[Introduction to Other Languages](./Introduction-to-Other-Languages.md)

---

[Demo](#Demo) 

[Prerequisite - without experience using T5 or mT5](#Prerequisites)

[Requirements](#Requirements)

[References](#References)

[Privacy](#Privacy)
<a name="english"></a>
# ‚≠êÔ∏èEnglish


## üìñ Background and Vision
This repository provides a fine-tuning training framework customized for the **T5** or **mT5** architecture.

The project aims to endow the model with an inherent **"semantic resilience"** through deep optimization of **training strategies**, enabling it to more robustly handle text defects and precisely inject domain-specific knowledge when facing high-information-density texts such as medical reports and professional literature.

Due to significant distribution differences between professional texts like **medicine** and general corpora, models are **extremely prone to falling into local optima or early stopping due to Loss fluctuations in the early stages of fine-tuning**. This project introduces mechanisms to optimize this issue.

**"Better to moderately overfit than to converge incompletely."** For professional domains that do not tolerate ambiguity, increased training steps are the underlying guarantee of the model's "semantic reliability."

>Due to limited performance of locally deployed devices, there are many compromises in the settings. See Requirements for the specific configuration list.

---

## ‚úÖ Core Features

* **Warm-up Mechanism**: By setting the `start_step` threshold, it forcibly avoids initial unstable local random fluctuations. (Cold Start)
* **Windowed Loss Trend Evaluation**: Through `patience` settings, it allows Loss to fluctuate or stagnate within a certain period, and only stops when Loss fails to refresh the best record for multiple consecutive stages, preventing the model from stopping prematurely due to false "plateau" caused by temporary fluctuations.
* **Status Tracking**: `SafeDetailedProgressCallback` provides real-time learning rate evolution and dynamic ETA prediction (adjustable frequency), supporting transparent monitoring of long-term training jobs.
* **Real-time Backup and Checkpoint Resumption**: For high-time-consuming training scenarios in medical research, it embeds manual interruption (KeyboardInterrupt Handling) *Ctrl+C* and real-time backup, ensuring that the model's best weights (Best Weights) and multiple process weights are saved as completely as possible in case of emergencies.

---

## üõ†Ô∏è Technical Implementation Details (Technical Deep-Dive)

### 1. Multi-stage Convergence Judgment Mechanism (Multi-stage Convergence Analysis)
Unlike general tasks, the Loss curve in medical fine-tuning tasks often exhibits a "stepwise decline" characteristic. This project replaces instantaneous judgment with **windowed Loss trend evaluation**:
* **Avoid "Pseudo-Plateau" Intervals**: T5 often experiences plateau periods with weak Loss decline in the early stages of domain transfer. If early stopping is triggered at this time, the model only has basic linguistic sense and lacks deep fitting to medical logic.
* **Delayed Trigger Logic**: Through `DelayedEarlyStopping`, it forcibly delays the judgment to capture **secondary convergence** after the first plateau period.

**Only after multiple Loss window analyses confirm that the model has entered a "semantic saturation" state will the system issue a stop signal.**


### 2. High-order Gradient Stability Control (Gradient Dynamics Control)
To address the gradient instability caused by sparse distribution of medical professional vocabulary, the framework has been optimized at the underlying level:
* **Gradient Accumulation**: Through `gradient_accumulation_steps=8`, it **saves memory while smoothing the instantaneous gradient impact** brought by long and difficult sentences, simulating a stable large Batch Size update environment.
* **Asymmetric Evaluation Frequency**: Combined with `eval_steps=1000`, it performs high-precision best-model saving at a lower frequency during long-term training, ensuring that the weights locked by `load_best_model_at_end` truly have cross-sample robustness.
* **Asymmetric Monitoring Frequency**: Configured with `logging_steps=100` and `eval_steps=1000`. While ensuring high-frequency telemetry (monitoring if gradients are normal), it reduces the frequency of high-cost validation set evaluations, ensuring computational power is focused on parameter updates.

---
## üî¨ Training Insights: Why "Multiple Loss Analyses" Are Needed?

In this medical fine-tuning task, the convergence judgment matrix is as follows:

| Training Phase | Loss Feature Performance | Core Semantic State | Strategy Response |
| :--- | :--- | :--- | :--- |
| **Early Stage (0-6000 steps)** | Severe oscillations or slow gradual decline | Domain sense establishment, initial parameter alignment | **Force Continuation** (Ban premature early stopping that has occurred before) |
| **Mid Stage (6000-12000 steps)** | Appearance of long plateau (pseudo-convergence) | Professional knowledge injection, handling text defects | **Continuous Observation** (Windowed trend analysis) |
| **Late Stage (12000+ steps)** | Stable after stepwise secondary decline | Semantic depth saturation, resilient restoration capability | **Dynamic Evaluation** (Stop when threshold is met) |

---

## üìä Dataset Preparation and Token Scale Estimation 

In medical domain tasks, the corpus scale directly determines the upper limit of "semantic resilience". Based on practical evaluations:
* **Scale Comparison**:
    * **25MB Chinese Text**: Preliminary data, only supports the model in completing basic terminology alignment, showing obvious "poor sense" when handling text defects.
    * **256MB Chinese Text**: The model demonstrates stable domain fine-tuning capabilities, meeting final evaluation expectations. (See demo)

* **Chinese Token Conversion Reference** (based on UTF-8 encoding and mT5 tokenizer):

| Text Size | Estimated Chinese Characters | Estimated Total Tokens | 
| :--- | :--- | :--- | 
| **25 MB** | Approx. 8 million characters | Approx. 10 million | 
| **256 MB** | Approx. 85 million characters | Approx. 100 million | 

> **Data Quality Tips**: Recommend injecting moderate noise to simulate real medical text environments, forcing the model to learn how to use context for "correction".

[Demo](#Demo) 

---

<a name="chinese"></a>
# ‚≠êÔ∏è‰∏≠Êñá


## üìñ ËÉåÊôØ‰∏éÊÑøÊôØ
Êú¨‰ªìÂ∫ìÊèê‰æõ‰∫Ü‰∏Ä‰∏™ÈíàÂØπ **T5** Êàñ **mT5** Êû∂ÊûÑÂÆöÂà∂ÁöÑÁ≤æ‰øÆËÆ≠ÁªÉÊ°ÜÊû∂ÔºàÂæÆË∞ÉÔºâ„ÄÇ

È°πÁõÆÊó®Âú®ÈÄöËøá **ËÆ≠ÁªÉÁ≠ñÁï•**ÁöÑÊ∑±Â∫¶‰ºòÂåñÔºåËµã‰∫àÊ®°Âûã‰∏ÄÁßçÂÜÖÂú®ÁöÑ **‚ÄúËØ≠‰πâÈüßÊÄß‚Äù**Ôºå‰ΩøÂÖ∂Âú®Èù¢ÂØπÂåªÂ≠¶Êä•Âëä„ÄÅ‰∏ì‰∏öÊñáÁåÆÁ≠âÈ´ò‰ø°ÊÅØÂØÜÂ∫¶ÊñáÊú¨Êó∂ÔºåËÉΩÊõ¥Á®≥ÂÅ•Âú∞Â§ÑÁêÜÊñáÊú¨Áº∫ÊçüÂπ∂Á≤æÂáÜÊ≥®ÂÖ•È¢ÜÂüü‰∏ì‰∏öÁü•ËØÜ„ÄÇ

Áî±‰∫é **ÂåªÂ≠¶**Á≠â‰∏ì‰∏öÊñáÊú¨‰∏éÈÄöÁî®ËØ≠ÊñôÂ∫ìÂ≠òÂú®ÊòæËëóÂàÜÂ∏ÉÂ∑ÆÂºÇÔºåÊ®°ÂûãÂú® **ÂæÆË∞ÉÂàùÊúüÊûÅÊòìÈô∑ÂÖ•Â±ÄÈÉ®ÊúÄ‰ºòÊàñÂõ† Loss Ê≥¢Âä®ÂØºËá¥Êó©ÂÅú**ÔºåÊú¨È°πÁõÆÂºïÂÖ•‰∫ÜÊú∫Âà∂‰ºòÂåñËøô‰∏ÄÈóÆÈ¢ò„ÄÇ

**‚ÄúÂÆÅÂèØÈÄÇÂ∫¶ËøáÊãüÂêàÔºå‰∏çÂèØÊî∂Êïõ‰∏çÂΩªÂ∫ï‚Äù**„ÄÇÂØπ‰∫é‰∏çÂÆπËÆ∏Ê≠ß‰πâÁöÑ‰∏ì‰∏öÈ¢ÜÂüüÔºåÂ¢ûÂä†ÁöÑËÆ≠ÁªÉÊ≠•Êï∞ÊòØÊ®°Âûã‚ÄúËØ≠‰πâÂèØÈù†ÊÄß‚ÄùÁöÑÂ∫ïÂ±Ç‰øùÈöú„ÄÇ

>Áî±‰∫éÊú¨Âú∞ÈÉ®ÁΩ≤ÁöÑËÆæÂ§áÊÄßËÉΩÊúâÈôêÔºåËÆæÁΩÆ‰∏≠ÊúâÂæàÂ§öÂ¶•Âçè„ÄÇÂÖ∑‰ΩìÁöÑÈÖçÁΩÆÊ∏ÖÂçïËßÅRequirements„ÄÇ

---

## ‚úÖ Ê†∏ÂøÉÂäüËÉΩ

* **È¢ÑÁÉ≠Êú∫Âà∂**ÔºöÈÄöËøáËÆæÂÆö `start_step` ÈòàÂÄºÔºåÂº∫Âà∂ÈÅøÂºÄÂàùÊúü‰∏çÁ®≥ÂÆöÁöÑÂ±ÄÈÉ®ÈöèÊú∫Ê≥¢Âä®„ÄÇÔºàÂÜ∑ÂêØÂä®Ôºâ
* **Á™óÂè£Âåñ Loss Ë∂ãÂäøËØÑ‰º∞**ÔºöÈÄöËøá`patience`ËÆæÁΩÆÔºåÂÖÅËÆ∏ Loss Âú®‰∏ÄÂÆöÂë®ÊúüÂÜÖÂ≠òÂú®Ê≥¢Âä®ÊàñÂÅúÊªûÔºåÂè™ÊúâÂΩì Loss ËøûÁª≠ Â§ö‰∏™Èò∂ÊÆµÊú™ËÉΩÂà∑Êñ∞ÊúÄ‰ºòËÆ∞ÂΩïÊó∂ÊâçÂÅúÊ≠¢ÔºåÈò≤Ê≠¢Ê®°ÂûãÂú®Áî±‰∫éÊöÇÊó∂Ê≥¢Âä®ÂØºËá¥ÁöÑËôöÂÅá‚ÄúÂπ≥Âè∞Êúü‚ÄùËøáÊó©ÂÅúÊ≠¢„ÄÇ
* **Áä∂ÊÄÅËøΩË∏™**Ôºö`SafeDetailedProgressCallback` Êèê‰æõÂÆûÊó∂Â≠¶‰π†ÁéáÊºîÂèò‰∏éÂä®ÊÄÅ ETA È¢ÑÊµãÔºàÂèØË∞ÉÈ¢ëÁéáÔºâÔºåÊîØÊåÅÂØπÈïøÁ®ãËÆ≠ÁªÉ‰Ωú‰∏öÁöÑÈÄèÊòéÂåñÁõëÊéß„ÄÇ
* **ÂÆûÊó∂Â§á‰ªΩ‰∏éÊñ≠ÁÇπÊé•Áª≠**ÔºöÈíàÂØπÂåªÂ≠¶ÁßëÁ†îÈ´òËÄóÊó∂ËÆ≠ÁªÉÂú∫ÊôØÔºåÂÜÖÂµåÊâãÂä®‰∏≠Êñ≠ÔºàKeyboardInterrupt HandlingÔºâ*Ctrl+C*ÂíåÂÆûÊó∂Â§á‰ªΩÔºåÁ°Æ‰øùÂú®Á™ÅÂèëÁä∂ÂÜµ‰∏ãÔºåÊ®°ÂûãÁöÑÊúÄ‰ºòÊùÉÈáçÔºàBest WeightsÔºâÂíåÂ§ö‰∏™ËøáÁ®ãÊùÉÈáçÂæó‰ª•Â∞ΩÂèØËÉΩÁöÑÂÆåÊï¥‰øùÂ≠ò„ÄÇ

---

## üõ†Ô∏è ÊäÄÊúØÂÆûÁé∞ÁªÜËäÇ (Technical Deep-Dive)

### 1. Â§öÈò∂ÊÆµÊî∂ÊïõÂà§Âà´Êú∫Âà∂ (Multi-stage Convergence Analysis)
‰∏çÂêå‰∫éÈÄöÁî®‰ªªÂä°ÔºåÂåªÂ≠¶Á≤æ‰øÆ‰ªªÂä°ÁöÑ Loss Êõ≤Á∫øÂ∏∏ÂëàÁé∞‚ÄúÈò∂Ê¢ØÂºè‰∏ãÈôç‚ÄùÁâπÂæÅ„ÄÇÊú¨È°πÁõÆÈÄöËøá **Á™óÂè£Âåñ Loss Ë∂ãÂäøËØÑ‰º∞** ‰ª£ÊõøÁû¨Êó∂Âà§ÂÆöÔºö
* **ËßÑÈÅø‚Äú‰º™Âπ≥Áºì‚ÄùÂå∫Èó¥**ÔºöT5 Âú®È¢ÜÂüüËøÅÁßªÂàùÊúüÂ∏∏Âá∫Áé∞ Loss ‰∏ãÈôçÂº±ÁöÑÂπ≥Âè∞Êúü„ÄÇËã•Ê≠§Êó∂Ëß¶ÂèëÊó©ÂÅúÔºåÊ®°Âûã‰ªÖÂÖ∑Â§áÂü∫Á°ÄËØ≠ÊÑüÔºåËÄåÁº∫Â§±ÂØπÂåªÂ≠¶ÈÄªËæëÁöÑÊ∑±Â∫¶ÊãüÂêà„ÄÇ
* **Âª∂ËøüËß¶ÂèëÈÄªËæë**ÔºöÈÄöËøá `DelayedEarlyStopping` Âº∫Âà∂Êé®ËøüÂà§ÂÆöÔºåÊòØ‰∏∫‰∫ÜÊçïÊçâÁ¨¨‰∏Ä‰∏™Âπ≥Âè∞Êúü‰πãÂêéÁöÑ **‰∫åÊ¨°Êî∂ÊïõÔºàSecondary ConvergenceÔºâ**„ÄÇ

**Âè™ÊúâÁªèËøáÂ§öÊ¨° Loss Á™óÂè£ÂàÜÊûêÔºåÁ°ÆËÆ§Ê®°ÂûãËøõÂÖ•‚ÄúËØ≠‰πâÈ•±Âíå‚ÄùÁä∂ÊÄÅÂêéÔºåÁ≥ªÁªüÊâç‰ºöÂèëÂá∫ÂÅúÊ≠¢‰ø°Âè∑„ÄÇ**


### 2. È´òÈò∂Ê¢ØÂ∫¶Á®≥ÂÆöÊÄßÊéßÂà∂ (Gradient Dynamics Control)
ÈíàÂØπÂåªÂ≠¶‰∏ì‰∏öËØçÊ±áÂàÜÂ∏ÉÁ®ÄÁñèÂØºËá¥ÁöÑÊ¢ØÂ∫¶‰∏çÁ®≥ÂÆöÈóÆÈ¢òÔºåÊ°ÜÊû∂Âú®Â∫ïÂ±ÇÂÅö‰∫Ü‰ºòÂåñÔºö
* **Ê¢ØÂ∫¶Á¥ØÂä† (Gradient Accumulation)**ÔºöÈÄöËøá `gradient_accumulation_steps=8` **ÁúÅÊòæÂ≠ò,ÂêåÊó∂Âπ≥ÊªëÈïøÈöæÂè•**Â∏¶Êù•ÁöÑÁû¨Êó∂Ê¢ØÂ∫¶ÂÜ≤ÂáªÔºåÊ®°ÊãüÁ®≥ÂÆöÁöÑÂ§ß Batch Size Êõ¥Êñ∞ÁéØÂ¢É„ÄÇ
* **ÈùûÂØπÁß∞ËØÑ‰º∞È¢ëÁéá**ÔºöÈÖçÂêà `eval_steps=1000`ÔºåÂú®ÈïøÁ®ãËÆ≠ÁªÉ‰∏≠‰ª•ËæÉ‰ΩéÈ¢ëÁéáËøõË°åÈ´òÁ≤æÂ∫¶ÁöÑÊã©‰ºò‰øùÂ≠òÔºåÁ°Æ‰øù `load_best_model_at_end` ÈîÅÂÆöÁöÑÊùÉÈáçÁúüÊ≠£ÂÖ∑Â§áË∑®Ê†∑Êú¨ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ
* **ÈùûÂØπÁß∞ÁõëÊéßÈ¢ëÁéá**ÔºöÈÖçÁΩÆ `logging_steps=100` ‰∏é `eval_steps=1000`„ÄÇÂú®‰øùËØÅÈ´òÈ¢ëÈÅ•ÊµãÔºàÁõëÊµãÊ¢ØÂ∫¶ÊòØÂê¶Ê≠£Â∏∏ÔºâÁöÑÂêåÊó∂ÔºåÈôç‰ΩéÈ´òËÄóÊó∂ÁöÑÈ™åËØÅÈõÜËØÑ‰º∞È¢ëÁéáÔºåÁ°Æ‰øùÁÆóÂäõÈõÜ‰∏≠‰∫éÂèÇÊï∞Êõ¥Êñ∞„ÄÇ

---
## üî¨ ËÆ≠ÁªÉÊ¥ûÂØüÔºö‰∏∫‰ªÄ‰πàÈúÄË¶Å‚ÄúÂ§öÊ¨° Loss ÂàÜÊûê‚ÄùÔºü

Âú®Êú¨Ê¨°ÂåªÂ≠¶Á≤æ‰øÆ‰ªªÂä°‰∏≠ÔºåÊî∂ÊïõÂà§ÂÆöÁü©ÈòµÂ¶Ç‰∏ãÔºö

| ËÆ≠ÁªÉÈò∂ÊÆµ | Loss ÁâπÂæÅË°®Áé∞ | Ê†∏ÂøÉËØ≠‰πâÁä∂ÊÄÅ | Á≠ñÁï•ÂìçÂ∫î |
| :--- | :--- | :--- | :--- |
| **ÂàùÊúü (0-6000 Ê≠•)** | ÂâßÁÉàÈúáËç°ÊàñÁºìÊÖ¢ÁºìÈôç | È¢ÜÂüüËØ≠ÊÑüÂª∫Á´ãÔºåÂèÇÊï∞ÂàùÊ≠•ÂØπÈΩê | **Âº∫Âà∂ÊåÅÁª≠** (Â∞ÅÁ¶ÅÊõæÁªèÂèëÁîüËøáÁöÑÊó©ÂÅú) |
| **‰∏≠Êúü (6000-12000 Ê≠•)** | Âá∫Áé∞ÈïøÂπ≥Âè∞Êúü (‰º™Êî∂Êïõ) | ‰∏ì‰∏öÁü•ËØÜÊ≥®ÂÖ•ÔºåÂ§ÑÁêÜÊñáÊú¨Áº∫Êçü | **ÊåÅÁª≠ËßÇÊµã** (Á™óÂè£ÂåñË∂ãÂäøÂàÜÊûê) |
| **ÂêéÊúü (12000+ Ê≠•)** | Èò∂Ê¢ØÂºè‰∫åÊ¨°‰∏ãÈôçÂêéÂπ≥Á®≥ | ËØ≠‰πâÊ∑±Â∫¶È•±ÂíåÔºåÂÖ∑Â§áÈüßÊÄßËøòÂéü | **Âä®ÊÄÅËØÑ‰º∞** (Êª°Ë∂≥ÈòàÂÄºÂÅúÊ≠¢) |

---

## üìä Êï∞ÊçÆÈõÜÂáÜÂ§á‰∏é Token ËßÑÊ®°‰º∞ÁÆó 

Âú®ÂåªÂ≠¶È¢ÜÂüü‰ªªÂä°‰∏≠ÔºåËØ≠ÊñôÂ∫ìÁöÑËßÑÊ®°Áõ¥Êé•ÂÜ≥ÂÆö‰∫Ü‚ÄúËØ≠‰πâÈüßÊÄß‚ÄùÁöÑ‰∏äÈôê„ÄÇÊ†πÊçÆÂÆûÊàòËØÑ‰º∞Ôºö
* **ËßÑÊ®°ÂØπÊØî**Ôºö
    * **25MB ‰∏≠ÊñáÊñáÊú¨**ÔºöÂàùÊ≠•Êï∞ÊçÆÔºå‰ªÖËÉΩÊîØÊíëÊ®°ÂûãÂÆåÊàêÂü∫Á°ÄÊúØËØ≠ÂØπÈΩêÔºåÂú®Â§ÑÁêÜÊñáÊú¨Áº∫ÊçüÊó∂Ë°®Áé∞Âá∫ÊòéÊòæÁöÑ‚ÄúËØ≠ÊÑüÊ¨†‰Ω≥‚Äù„ÄÇ
    * **256MB ‰∏≠ÊñáÊñáÊú¨**ÔºöÊ®°ÂûãÂ±ïÁé∞Âá∫Á®≥ÂÆöÁöÑÈ¢ÜÂüüÁ≤æ‰øÆËÉΩÂäõÔºåËææÂà∞ÊúÄÁªàËØÑ‰º∞È¢ÑÊúü„ÄÇÔºàËßÅdemoÔºâ

* **‰∏≠Êñá Token Êç¢ÁÆóÂèÇËÄÉ**ÔºàÂü∫‰∫é UTF-8 ÁºñÁ†Å‰∏é mT5 ÂàÜËØçÂô®ÔºâÔºö

| ÊñáÊú¨Â§ßÂ∞è | È¢Ñ‰º∞‰∏≠ÊñáÂ≠óÁ¨¶Êï∞ | È¢Ñ‰º∞ Token ÊÄªÈáè | 
| :--- | :--- | :--- | 
| **25 MB** | Á∫¶ 800 ‰∏áÂ≠ó | Á∫¶ 1000 ‰∏á | 
| **256 MB** | Á∫¶ 8500 ‰∏áÂ≠ó | Á∫¶ 1 ‰∫ø | 

> **Êï∞ÊçÆË¥®Èáè Tips**ÔºöÂª∫ËÆÆÊ≥®ÂÖ•ÈÄÇÂ∫¶Âô™Â£∞ÔºåÊ®°ÊãüÁúüÂÆûÁöÑÂåªÂ≠¶ÊñáÊú¨ÁéØÂ¢ÉÔºåÂº∫Ëø´Ê®°ÂûãÂ≠¶‰π†Â¶Ç‰ΩïÂà©Áî®‰∏ä‰∏ãÊñá‚ÄúÁ∫†ÂÅè‚Äù„ÄÇ

[Demo](#Demo) 

---

<a name="japanese"></a>
# Êó•Êú¨Ë™û


## üìñ ËÉåÊôØ„Å®„Éì„Ç∏„Éß„É≥
Êú¨„É™„Éù„Ç∏„Éà„É™„ÅØ„ÄÅ**T5** „Åæ„Åü„ÅØ **mT5** „Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£Âêë„Åë„Å´„Ç´„Çπ„Çø„Éû„Ç§„Ç∫„Åï„Çå„ÅüÁ≤æ‰øÆË®ìÁ∑¥„Éï„É¨„Éº„É†„ÉØ„Éº„ÇØÔºà„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞Ôºâ„ÇíÊèê‰æõ„Åó„Åæ„Åô„ÄÇ

„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅØ„ÄÅ**Ë®ìÁ∑¥Êà¶Áï•**„ÅÆÊ∑±„ÅÑÊúÄÈÅ©Âåñ„ÇíÈÄö„Åò„Å¶„ÄÅ„É¢„Éá„É´„Å´ÂÜÖÂú®ÁöÑ„Å™ **„ÄåÊÑèÂë≥ÁöÑÂõûÂæ©Âäõ„Äç** „Çí‰∏é„Åà„ÄÅÂåªÂ≠¶„É¨„Éù„Éº„Éà„ÇÑÂ∞ÇÈñÄÊñáÁåÆ„Å™„Å©„ÅÆÈ´òÊÉÖÂ†±ÂØÜÂ∫¶„ÉÜ„Ç≠„Çπ„Éà„Å´Áõ¥Èù¢„Åó„ÅüÈöõ„Å´„ÄÅ„ÉÜ„Ç≠„Çπ„ÉàÊ¨†Êêç„Çí„Çà„ÇäÂ†ÖÁâ¢„Å´Âá¶ÁêÜ„Åó„ÄÅÂàÜÈáéÁâπÂåñÁü•Ë≠ò„ÇíÊ≠£Á¢∫„Å´Ê≥®ÂÖ•„Åô„Çã„Åì„Å®„ÇíÁõÆÊåá„Åó„Åæ„Åô„ÄÇ

**ÂåªÂ≠¶**„Å™„Å©„ÅÆÂ∞ÇÈñÄ„ÉÜ„Ç≠„Çπ„Éà„ÅØÊ±éÁî®„Ç≥„Éº„Éë„Çπ„Å®ÊúâÊÑè„Å™ÂàÜÂ∏ÉÂ∑ÆÁï∞„Åå„ÅÇ„Çã„Åü„ÇÅ„ÄÅ„É¢„Éá„É´„ÅØ **„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞ÂàùÊúü„Å´Â±ÄÊâÄÊúÄÈÅ©„Å´Èô•„Çä„ÇÑ„Åô„Åè„ÄÅLossÂ§âÂãï„ÅßÊó©ÊúüÂÅúÊ≠¢„ÇíÂºï„ÅçËµ∑„Åì„Åô** ÂïèÈ°å„Çí„ÄÅÊú¨„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅØ„É°„Ç´„Éã„Ç∫„É†ÊúÄÈÅ©Âåñ„ÅßËß£Ê±∫„Åó„Åæ„Åô„ÄÇ

**„ÄåÈÅ©Â∫¶„Å™ÈÅéÂ≠¶Áøí„ÇíË®±ÂÆπ„Åó„ÄÅÂèéÊùü„ÇíÂæπÂ∫ï„Åï„Åõ„Çã„Äç**„ÄÇÊõñÊòß„Åï„ÇíË®±„Åï„Å™„ÅÑÂ∞ÇÈñÄÂàÜÈáé„Åß„ÅØ„ÄÅËøΩÂä†„ÅÆË®ìÁ∑¥„Çπ„ÉÜ„ÉÉ„Éó„Åå„É¢„Éá„É´„ÅÆ„ÄåÊÑèÂë≥ÁöÑ‰ø°È†ºÊÄß„Äç„ÅÆÂü∫Áõ§‰øùÈöú„Åß„Åô„ÄÇ

>„É≠„Éº„Ç´„É´„Éá„Éó„É≠„Ç§„ÅÆ„Éá„Éê„Ç§„ÇπÊÄßËÉΩ„ÅåÈôêÂÆöÁöÑ„Å™„Åü„ÇÅ„ÄÅË®≠ÂÆö„Å´Â§ö„Åè„ÅÆÂ¶•Âçî„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇÂÖ∑‰ΩìÁöÑ„Å™ÊßãÊàê„É™„Çπ„Éà„ÅØRequirements„ÇíÂèÇÁÖß„ÄÇ

---

## ‚úÖ „Ç≥„Ç¢Ê©üËÉΩ

* **„Ç¶„Ç©„Éº„É†„Ç¢„ÉÉ„Éó„É°„Ç´„Éã„Ç∫„É†**Ôºö`start_step` ÈñæÂÄ§„ÇíË®≠ÂÆö„Åó„ÄÅÂàùÊúü„ÅÆ‰∏çÂÆâÂÆö„Å™Â±ÄÊâÄ„É©„É≥„ÉÄ„É†Â§âÂãï„ÇíÂº∑Âà∂ÁöÑ„Å´ÂõûÈÅø„ÄÇÔºà„Ç≥„Éº„É´„Éâ„Çπ„Çø„Éº„ÉàÔºâ
* **„Ç¶„Ç£„É≥„Éâ„Ç¶Âåñ Loss „Éà„É¨„É≥„ÉâË©ï‰æ°**Ôºö`patience`Ë®≠ÂÆö„Å´„Çà„Çä„ÄÅLoss„Åå‰∏ÄÂÆöÊúüÈñìÂ§âÂãï„ÇÑÂÅúÊªû„ÇíË®±ÂÆπ„Åó„ÄÅLoss„ÅåÈÄ£Á∂öË§áÊï∞ÊÆµÈöé„ÅßÊúÄÈÅ©Ë®òÈå≤„ÇíÊõ¥Êñ∞„Åß„Åç„Å™„Åã„Å£„ÅüÂ†¥Âêà„ÅÆ„ÅøÂÅúÊ≠¢„ÄÇ‰∏ÄÊôÇÂ§âÂãï„Å´„Çà„ÇãÂÅΩ„ÅÆ„Äå„Éó„É©„Éà„ÉºÊúü„Äç„Åß„ÅÆÊó©ÊúüÂÅúÊ≠¢„ÇíÈò≤„Åé„Åæ„Åô„ÄÇ
* **Áä∂ÊÖãËøΩË∑°**Ôºö`SafeDetailedProgressCallback` „Åå„É™„Ç¢„É´„Çø„Ç§„É†Â≠¶ÁøíÁéáÈÄ≤Âåñ„Å®ÂãïÁöÑETA‰∫àÊ∏¨ÔºàË™øÊï¥ÂèØËÉΩÈ†ªÂ∫¶Ôºâ„ÇíÊèê‰æõ„Åó„ÄÅÈï∑ÊôÇÈñìË®ìÁ∑¥„Ç∏„Éß„Éñ„ÅÆÈÄèÊòéÂåñÁõ£Ë¶ñ„Çí„Çµ„Éù„Éº„Éà„ÄÇ
* **„É™„Ç¢„É´„Çø„Ç§„É†„Éê„ÉÉ„ÇØ„Ç¢„ÉÉ„Éó„Å®„ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„ÉàÂÜçÈñã**ÔºöÂåªÂ≠¶Á†îÁ©∂„ÅÆÈ´òÊôÇÈñìÊ∂àË≤ªË®ìÁ∑¥„Ç∑„Éº„É≥Âêë„Åë„Å´„ÄÅÊâãÂãï‰∏≠Êñ≠ÔºàKeyboardInterrupt HandlingÔºâ*Ctrl+C*„Å®„É™„Ç¢„É´„Çø„Ç§„É†„Éê„ÉÉ„ÇØ„Ç¢„ÉÉ„Éó„ÇíÂÜÖËîµ„Åó„ÄÅÁ™ÅÂèëÁä∂Ê≥Å‰∏ã„ÅßÊúÄÈÅ©Èáç„ÅøÔºàBest WeightsÔºâ„Å®Ë§áÊï∞„Éó„É≠„Çª„ÇπÈáç„Åø„ÇíÂèØËÉΩ„Å™Èôê„ÇäÂÆåÂÖ®‰øùÂ≠ò„ÄÇ

---

## üõ†Ô∏è ÊäÄË°ìÂÆüË£ÖË©≥Á¥∞ (Technical Deep-Dive)

### 1. Â§öÊÆµÈöéÂèéÊùüÂà§Âà•„É°„Ç´„Éã„Ç∫„É† (Multi-stage Convergence Analysis)
Ê±éÁî®„Çø„Çπ„ÇØ„Å®„ÅØÁï∞„Å™„Çä„ÄÅÂåªÂ≠¶Á≤æ‰øÆ„Çø„Çπ„ÇØ„ÅÆLossÊõ≤Á∑ö„ÅØ„Åó„Å∞„Åó„Å∞„ÄåÈöéÊÆµÁä∂‰∏ãÈôç„ÄçÁâπÂæ¥„ÇíÁ§∫„Åó„Åæ„Åô„ÄÇÊú¨„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅØ **„Ç¶„Ç£„É≥„Éâ„Ç¶Âåñ Loss „Éà„É¨„É≥„ÉâË©ï‰æ°** „ÅßÁû¨ÈñìÂà§ÂÆö„ÇíÁΩÆ„ÅçÊèõ„ÅàÔºö
* **„ÄåÂÅΩÂπ≥Á∑©„ÄçÂå∫ÈñìÂõûÈÅø**ÔºöT5„ÅØ„Éâ„É°„Ç§„É≥ÁßªË°åÂàùÊúü„Å´Loss‰∏ãÈôçÂº±„ÅÑ„Éó„É©„Éà„ÉºÊúü„ÇíÈ†ªÁô∫„ÄÇÊ≠§Êó∂Êó©ÂÅúÁô∫Âãï„Åß„É¢„Éá„É´„ÅØÂü∫Á§éË™ûÊÑü„ÅÆ„Åø„ÅßÂåªÂ≠¶Ë´ñÁêÜ„ÅÆÊ∑±Â±§„Éï„Ç£„ÉÉ„ÉÜ„Ç£„É≥„Ç∞„ÇíÊ¨†Â¶Ç„ÄÇ
* **ÈÅÖÂª∂„Éà„É™„Ç¨„Éº„É≠„Ç∏„ÉÉ„ÇØ**Ôºö`DelayedEarlyStopping` „ÅßÂà§ÂÆö„ÇíÂº∑Âà∂ÈÅÖÂª∂„Åó„ÄÅÊúÄÂàù„ÅÆ„Éó„É©„Éà„ÉºÊúüÂæå„ÅÆ **‰∫åÊ¨°ÂèéÊùüÔºàSecondary ConvergenceÔºâ** „ÇíÊçïÊçâ„ÄÇ

**Ë§áÊï∞Loss„Ç¶„Ç£„É≥„Éâ„Ç¶ÂàÜÊûê„ÇíÁµå„Å¶„ÄÅ„É¢„Éá„É´„Åå„ÄåÊÑèÂë≥È£ΩÂíå„ÄçÁä∂ÊÖã„Å´ÂÖ•„Å£„Åü„Åì„Å®„ÇíÁ¢∫Ë™çÂæå„ÄÅ„Ç∑„Çπ„ÉÜ„É†„ÅåÂÅúÊ≠¢‰ø°Âè∑„ÇíÁô∫Âá∫„ÄÇ**


### 2. È´òÊ¨°ÂãæÈÖçÂÆâÂÆöÊÄßÂà∂Âæ° (Gradient Dynamics Control)
ÂåªÂ≠¶Â∞ÇÈñÄË™ûÂΩôÂàÜÂ∏ÉÂ∏åËñÑ„Å´„Çà„ÇãÂãæÈÖç‰∏çÂÆâÂÆöÂïèÈ°å„Å´ÂØæ„Åó„ÄÅ„Éï„É¨„Éº„É†„ÉØ„Éº„ÇØ„ÅØÂ∫ïÂ±Ç„ÅßÊúÄÈÅ©ÂåñÔºö
* **ÂãæÈÖçËìÑÁ©ç (Gradient Accumulation)**Ôºö`gradient_accumulation_steps=8` „Åß **„É°„É¢„É™ÁØÄÁ¥Ñ„Åó„Å§„Å§„ÄÅÈï∑Èõ£Êñá** „Å´„Çà„ÇãÁû¨ÈñìÂãæÈÖçË°ùÊíÉ„ÇíÂπ≥ÊªëÂåñ„Åó„ÄÅÂÆâÂÆöÂ§ßBatch SizeÊõ¥Êñ∞Áí∞Â¢É„Çí„Ç∑„Éü„É•„É¨„Éº„Éà„ÄÇ
* **ÈùûÂØæÁß∞Ë©ï‰æ°È†ªÂ∫¶**Ôºö`eval_steps=1000` „Å®ÈÖçÂêà„Åó„ÄÅÈï∑ÊôÇÈñìË®ìÁ∑¥„Åß‰ΩéÈ†ªÂ∫¶È´òÁ≤æÂ∫¶ÊìáÂÑ™‰øùÂ≠ò„ÇíÁ¢∫‰øù„ÄÅ`load_best_model_at_end` „Åß„É≠„ÉÉ„ÇØ„Åó„ÅüÈáç„Åø„ÅåÁúüÊ≠£„ÇØ„É≠„Çπ„Çµ„É≥„Éó„É´È†ëÂÅ•ÊÄß„ÇíÊåÅ„Å§„ÄÇ
* **ÈùûÂØæÁß∞Áõ£Ë¶ñÈ†ªÂ∫¶**Ôºö`logging_steps=100` „Å® `eval_steps=1000` „ÇíË®≠ÂÆö„ÄÇÈ´òÈ†ªÈÅ•Ê∏¨ÔºàÂãæÈÖçÊ≠£Â∏∏Áõ£Ë¶ñÔºâ„Çí‰øùË®º„Åó„Å§„Å§„ÄÅÈ´òÊ∂àË≤ªÊ§úË®ºÈõÜË©ï‰æ°È†ªÂ∫¶„Çí‰Ωé‰∏ã„Åï„Åõ„ÄÅÁÆóÂäõ„Çí„Éë„É©„É°„Éº„ÇøÊõ¥Êñ∞„Å´ÈõÜ‰∏≠„ÄÇ

---
## üî¨ Ë®ìÁ∑¥„ÅÆÊ¥ûÂØüÔºö„Å™„Åú„ÄåË§áÊï∞Âõû„ÅÆ Loss ÂàÜÊûê„Äç„ÅåÂøÖË¶Å„ÅãÔºü

‰ªäÂõû„ÅÆÂåªÂ≠¶Á≤æ‰øÆ„Çø„Çπ„ÇØ„Å´„Åä„ÅÑ„Å¶„ÄÅÂèéÊùüÂà§ÂÆö„Éû„Éà„É™„ÇØ„Çπ„ÅØ‰ª•‰∏ã„ÅÆÈÄö„ÇäÔºö

| Ë®ìÁ∑¥ÊÆµÈöé | Loss ÁâπÂæ¥Ë°®Áèæ | Ê†∏ÂøÉÊÑèÂë≥Áä∂ÊÖã | Êà¶Áï•ÂØæÂøú |
| :--- | :--- | :--- | :--- |
| **ÂàùÊúü (0-6000 „Çπ„ÉÜ„ÉÉ„Éó)** | ÊøÄ„Åó„ÅÑÊåØÂãï„Åæ„Åü„ÅØÁ∑©„ÇÑ„Åã„Å™Á∑©Èôç | È†òÂüüË™ûÊÑüÁ¢∫Á´ã„ÄÅ„Éë„É©„É°„Éº„ÇøÂàùÊúü„Ç¢„É©„Ç§„É°„É≥„Éà | **Âº∑Âà∂Á∂ôÁ∂ö** (ÈÅéÂéª„Å´Áô∫Áîü„Åó„ÅüÊó©ÊúüÂÅúÊ≠¢„ÇíÁ¶ÅÊ≠¢) |
| **‰∏≠Êúü (6000-12000 „Çπ„ÉÜ„ÉÉ„Éó)** | Èï∑Âπ≥Âè∞Êúü„ÅÆÂá∫Áèæ (Êì¨ÂèéÊùü) | Â∞ÇÈñÄÁü•Ë≠òÊ≥®ÂÖ•„ÄÅ„ÉÜ„Ç≠„Çπ„ÉàÊ¨†ÊêçÂá¶ÁêÜ | **Á∂ôÁ∂öË¶≥Ê∏¨** („Ç¶„Ç£„É≥„Éâ„Ç¶Âåñ„Éà„É¨„É≥„ÉâÂàÜÊûê) |
| **ÂæåÊúü (12000+ „Çπ„ÉÜ„ÉÉ„Éó)** | ÈöéÊÆµÁöÑ‰∫åÊ¨°‰∏ãÈôçÂæå„ÅÆÂÆâÂÆö | ÊÑèÂë≥Ê∑±„ÅïÈ£ΩÂíå„ÄÅËÄêÊÄßÂæ©ÂÖÉËÉΩÂäõ‰øùÊúâ | **ÂãïÁöÑË©ï‰æ°** (ÈñæÂÄ§Ê∫ÄË∂≥„ÅßÂÅúÊ≠¢) |

---

## üìä „Éá„Éº„Çø„Çª„ÉÉ„ÉàÊ∫ñÂÇô„Å® Token Ë¶èÊ®°Êé®ÂÆö 

ÂåªÂ≠¶È†òÂüü„Çø„Çπ„ÇØ„Å´„Åä„ÅÑ„Å¶„ÄÅ„Ç≥„Éº„Éë„Çπ„ÅÆË¶èÊ®°„Åå„ÄåÊÑèÂë≥ËÄêÊÄß„Äç„ÅÆ‰∏äÈôê„ÇíÁõ¥Êé•Ê±∫ÂÆö„Åó„Åæ„Åô„ÄÇÂÆüÊà¶Ë©ï‰æ°„Å´Âü∫„Å•„ÅçÔºö
* **Ë¶èÊ®°ÊØîËºÉ**Ôºö
    * **25MB ‰∏≠ÂõΩË™û„ÉÜ„Ç≠„Çπ„Éà**ÔºöÂàùÊúü„Éá„Éº„Çø„ÅÆ„Åø„ÄÅ„É¢„Éá„É´„ÅåÂü∫Êú¨Áî®Ë™û„Ç¢„É©„Ç§„É°„É≥„Éà„ÇíÂÆå‰∫Ü„Åô„Çã„ÅÆ„Å´ÂçÅÂàÜ„Å†„Åå„ÄÅ„ÉÜ„Ç≠„Çπ„ÉàÊ¨†ÊêçÂá¶ÁêÜÊôÇ„Å´Êòé„Çâ„Åã„Å™„ÄåË™ûÊÑü‰∏çË∂≥„Äç„ÇíÁ§∫„Åô„ÄÇ
    * **256MB ‰∏≠ÂõΩË™û„ÉÜ„Ç≠„Çπ„Éà**Ôºö„É¢„Éá„É´„ÅåÂÆâÂÆö„Åó„ÅüÈ†òÂüüÁ≤æ‰øÆËÉΩÂäõ„ÇíÁ§∫„Åó„ÄÅÊúÄÁµÇË©ï‰æ°ÊúüÂæÖ„Å´ÈÅî„Åô„Çã„ÄÇÔºà„Éá„É¢ÂèÇÁÖßÔºâ

* **‰∏≠ÂõΩË™û Token ÊèõÁÆóÂèÇËÄÉ**ÔºàUTF-8 „Ç®„É≥„Ç≥„Éº„Éá„Ç£„É≥„Ç∞„Å® mT5 „Éà„Éº„ÇØ„Éä„Ç§„Ç∂„Å´Âü∫„Å•„ÅèÔºâÔºö

| „ÉÜ„Ç≠„Çπ„Éà„Çµ„Ç§„Ç∫ | Êé®ÂÆö‰∏≠ÂõΩË™ûÊñáÂ≠óÊï∞ | Êé®ÂÆö Token Á∑èÈáè | 
| :--- | :--- | :--- | 
| **25 MB** | Á¥Ñ 800 ‰∏áÂ≠ó | Á¥Ñ 1000 ‰∏á | 
| **256 MB** | Á¥Ñ 8500 ‰∏áÂ≠ó | Á¥Ñ 1 ÂÑÑ | 

> **„Éá„Éº„ÇøÂìÅË≥™ Tips**ÔºöÈÅ©Â∫¶„Å™„Éé„Ç§„Ç∫„ÇíÊ≥®ÂÖ•„Åô„Çã„Åì„Å®„ÇíÊé®Â•®„Åó„ÄÅÊú¨Áâ©„ÅÆÂåªÂ≠¶„ÉÜ„Ç≠„Çπ„ÉàÁí∞Â¢É„Çí„Ç∑„Éü„É•„É¨„Éº„Éà„Åó„ÄÅ„É¢„Éá„É´„Å´„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà„ÇíÂà©Áî®„Åó„Åü„ÄåË®ÇÊ≠£„ÄçÂ≠¶Áøí„ÇíÂº∑Âà∂„ÄÇ

[„Éá„É¢](#Demo) 

---

<a name="deutsch"></a>
# Deutsch


## üìñ Hintergrund und Vision
Dieses Repository bietet einen ma√ügeschneiderten Feinabstimmungs-Trainingsrahmen (Fine-Tuning) f√ºr die **T5**- oder **mT5**-Architektur.

Das Projekt zielt darauf ab, durch tiefe Optimierung der **Trainingsstrategien** dem Modell eine inh√§rente **‚Äûsemantische Resilienz‚Äú** zu verleihen, damit es bei der Bearbeitung von hochinformationsdichten Texten wie medizinischen Berichten und Fachliteratur robuster Textl√ºcken handhabt und fachspezifisches Wissen pr√§zise injiziert.

Aufgrund signifikanter Verteilungsunterschiede zwischen fachspezifischen Texten wie **Medizin** und allgemeinen Korpusen neigt das Modell in der **Anfangsfeinabstimmung** dazu, in lokalen Optima steckenzubleiben oder durch Loss-Schwankungen zu fr√ºhes Stopping zu verursachen. Dieses Projekt l√∂st dieses Problem durch Mechanismusoptimierungen.

**‚ÄûLieber moderates Overfitting als unvollst√§ndige Konvergenz‚Äú**. F√ºr fachspezifische Bereiche, die keine Ambiguit√§ten erlauben, ist die Erh√∂hung der Trainingschritte die grundlegende Garantie f√ºr die ‚Äûsemantische Zuverl√§ssigkeit‚Äú des Modells.

>Da die Leistung lokal deployter Ger√§te begrenzt ist, gibt es viele Kompromisse in den Einstellungen. Die spezifische Konfigurationsliste finden Sie in Requirements.

---

## ‚úÖ Kernfunktionen

* **Aufw√§rmmechanismus**: Durch Festlegung eines `start_step`-Schwellenwerts werden anf√§ngliche instabile lokale Zufallsschwankungen erzwungenerma√üen vermieden. (Cold Start)
* **Fensterbasierte Loss-Trendbewertung**: Durch `patience`-Einstellung werden Loss-Schwankungen oder Stagnationen in einem bestimmten Zyklus erlaubt; das Training stoppt erst, wenn der Loss √ºber mehrere aufeinanderfolgende Phasen kein neues Bestwert-Update erreicht, um vorzeitiges Stopping aufgrund tempor√§rer Schwankungen und falscher ‚ÄûPlateaus‚Äú zu verhindern.
* **Statusverfolgung**: `SafeDetailedProgressCallback` bietet Echtzeit-Entwicklung des Lernrates und dynamische ETA-Vorhersagen (anpassbare Frequenz) und unterst√ºtzt transparente √úberwachung langer Trainingsjobs.
* **Echtzeit-Backup und Fortsetzung von Breakpoints**: F√ºr zeitintensive medizinische Forschungs-Trainingszenarien integriert es manuelle Unterbrechungen (KeyboardInterrupt-Behandlung) *Ctrl+C* und Echtzeit-Backups, um bei unvorhergesehenen Ereignissen die optimalen Gewichte (Best Weights) und mehrere Prozessgewichte so vollst√§ndig wie m√∂glich zu sichern.

---

## üõ†Ô∏è Technische Umsetzungsdetails (Technical Deep-Dive)

### 1. Mehrstufige Konvergenz-Erkennung (Multi-stage Convergence Analysis)
Im Gegensatz zu allgemeinen Aufgaben zeigen Loss-Kurven bei medizinischen Feinabstimmungsaufgaben oft ‚Äûtreppenf√∂rmige Abstiege‚Äú. Dieses Projekt ersetzt momentane Urteile durch **fensterbasierte Loss-Trendbewertung**:
* **Vermeidung von ‚ÄûPseudo-Plateaus‚Äú**: T5 zeigt in der fr√ºhen Dom√§nen√ºbertragung oft Plateaus mit schwachem Loss-Abstieg. Ein fr√ºhzeitiges Stopping w√ºrde das Modell nur mit grundlegender Sprachwahrnehmung zur√ºcklassen, ohne tiefe Anpassung an medizinische Logik.
* **Verz√∂gerte Trigger-Logik**: Durch `DelayedEarlyStopping` wird die Urteilsfindung erzwungenerma√üen verz√∂gert, um die **sekund√§re Konvergenz (Secondary Convergence)** nach dem ersten Plateau zu erfassen.

**Nur nach mehrfacher Loss-Fensteranalyse und Best√§tigung des Eintretens in einen ‚Äûsemantisch ges√§ttigten‚Äú Zustand sendet das System das Stoppsignal.**


### 2. H√∂herstufige Gradientenstabilit√§tskontrolle (Gradient Dynamics Control)
Zur L√∂sung der Gradienteninstabilit√§t durch sparse Verteilung medizinischer Fachvokabeln optimiert der Rahmen auf unterer Ebene:
* **Gradientenakkumulation (Gradient Accumulation)**: Durch `gradient_accumulation_steps=8` **Speicher sparen und gleichzeitigÁû¨Êó∂ Gradientenschl√§ge von langen schwierigen S√§tzen gl√§tten**, um stabile gro√üe Batch-Size-Update-Umgebungen zu simulieren.
* **Asymmetrische Bewertungsfrequenz**: In Kombination mit `eval_steps=1000` werden in langen Trainings hochwertige Best-Modelle in niedriger Frequenz gespeichert, um sicherzustellen, dass die durch `load_best_model_at_end` gesperrten Gewichte echte Cross-Sample-Robustheit besitzen.
* **Asymmetrische √úberwachungsfrequenz**: Konfiguration von `logging_steps=100` und `eval_steps=1000`. Hohe Frequenz f√ºr Telemetrie (√úberwachung, ob Gradienten normal sind) bei gleichzeitiger Reduzierung der hochrechenintensiven Validierungs-Frequenz, um Rechenleistung auf Parameter-Updates zu konzentrieren.

---
## üî¨ Trainings-Einblicke: Warum ‚Äûmehrfache Loss-Analyse‚Äú notwendig ist?

In dieser medizinischen Feinabstimmungsaufgabe lautet die Konvergenz-Entscheidungsmatrix wie folgt:

| Trainingsphase | Loss-Merkmalsauspr√§gung | Kernsemantischer Zustand | Strategische Reaktion |
| :--- | :--- | :--- | :--- |
| **Anfang (0-6000 Schritte)** | Starke Oszillationen oder langsamer Abfall | Aufbau des dom√§nenspezifischen Sprachgef√ºhls, erste Parameteranpassung | **Erzwungene Fortsetzung** (Fr√ºhstopp, der fr√ºher auftrat, ist verboten) |
| **Mitte (6000-12000 Schritte)** | Auftreten einer langen Plateau-Phase (Pseudo-Konvergenz) | Einspeisung fachlichen Wissens, Behandlung von Textdefekten | **Fortlaufende Beobachtung** (Fensterbasierte Trendanalyse) |
| **Sp√§t (12000+ Schritte)** | Treppenf√∂rmiger sekund√§rer Abfall mit anschlie√üender Stabilisierung | Semantische Tiefens√§ttigung, robuste Wiederherstellung | **Dynamische Bewertung** (Stopp bei Erreichen des Schwellenwerts) |

---

## üìä Datensatzvorbereitung und Token-Gr√∂√üenabsch√§tzung 

In medizinischen Fachaufgaben bestimmt die Gr√∂√üe des Korpus direkt die Obergrenze der ‚Äûsemantischen Robustheit‚Äú. Basierend auf Praxiseinsch√§tzungen:
* **Gr√∂√üenvergleich**:
    * **25 MB chinesischer Text**ÔºöVorl√§ufige Daten, die das Modell nur f√ºr die grundlegende Terminologieanpassung ausreichen lassen; bei Textdefekten zeigt es deutliche ‚ÄûSprachgef√ºhlsschw√§chen‚Äú.
    * **256 MB chinesischer Text**ÔºöDas Modell zeigt stabile dom√§nenspezifische Feinabstimmungsf√§higkeiten und erreicht die erwarteten Bewertungsergebnisse.ÔºàSiehe DemoÔºâ

* **Chinesische Token-Umrechnungshinweise**Ôºàbasierend auf UTF-8-Kodierung und mT5-TokenizerÔºâ:

| Textgr√∂√üe | Gesch√§tzte Anzahl chinesischer Zeichen | Gesch√§tzte Token-Gesamtzahl | 
| :--- | :--- | :--- | 
| **25 MB** | Ca. 8 Millionen Zeichen | Ca. 10 Millionen | 
| **256 MB** | Ca. 85 Millionen Zeichen | Ca. 100 Millionen | 

> **Tipps zur Datenqualit√§t**ÔºöEmpfehlung, moderate Rauschen einzuf√ºgen, um echte medizinische Textumgebungen zu simulieren und das Modell zu zwingen, Kontext zur ‚ÄûKorrektur‚Äú zu nutzen.

[Demo](#Demo) 

---

<a name="francais"></a>
# Fran√ßais


## üìñ Contexte et vision
Ce d√©p√¥t fournit un framework d'entra√Ænement de **fine-tuning** personnalis√© pour les architectures **T5** ou **mT5**.

Le projet vise, par une optimisation approfondie des **strat√©gies d'entra√Ænement**, √† doter le mod√®le d'une **¬´ r√©silience s√©mantique ¬ª** intrins√®que, lui permettant de g√©rer plus robustement les d√©ficits textuels et d'injecter pr√©cis√©ment les connaissances sp√©cialis√©es du domaine lorsqu'il fait face √† des textes √† haute densit√© d'information tels que les rapports m√©dicaux ou la litt√©rature professionnelle.

En raison des diff√©rences de distribution significatives entre les textes professionnels comme la **m√©decine** et les corpus g√©n√©raux, le mod√®le est **tr√®s susceptible de tomber dans un optimum local ou de s'arr√™ter pr√©matur√©ment en raison de fluctuations de Loss** au d√©but du fine-tuning. Ce projet introduit des m√©canismes d'optimisation pour r√©soudre ce probl√®me.

**¬´ Mieux vaut une sur-adaptation mod√©r√©e qu'une convergence incompl√®te ¬ª**. Pour les domaines professionnels ne tol√©rant aucune ambigu√Øt√©, un nombre accru d'√©tapes d'entra√Ænement est la garantie fondamentale de la ¬´ fiabilit√© s√©mantique ¬ª du mod√®le.

>En raison des performances limit√©es des √©quipements d√©ploy√©s localement, il y a de nombreux compromis dans les param√®tres. Voir Requirements pour la liste de configuration sp√©cifique.

---

## ‚úÖ Fonctionnalit√©s principales

* **M√©canisme de pr√©chauffage** : En d√©finissant un seuil `start_step`, √©vite forc√©ment les fluctuations al√©atoires locales instables initiales. (D√©marrage √† froid)
* **√âvaluation de tendance Loss par fen√™tre** : Via le param√®tre `patience`, permet des fluctuations ou stagnations de Loss sur une p√©riode donn√©e, et n'arr√™te que si Loss ne rafra√Æchit pas son record optimal sur plusieurs phases cons√©cutives, √©vitant l'arr√™t pr√©matur√© d√ª √† une fausse ¬´ p√©riode de plateau ¬ª caus√©e par des fluctuations temporaires.
* **Suivi d'√©tat** : `SafeDetailedProgressCallback` fournit l'√©volution en temps r√©el du taux d'apprentissage et une pr√©diction dynamique de l'ETA (fr√©quence ajustable), supportant une surveillance transparente des t√¢ches d'entra√Ænement longues.
* **Sauvegarde en temps r√©el et reprise aux points de rupture** : Pour les sc√©narios d'entra√Ænement √† haute consommation de temps en recherche m√©dicale, int√©gration de l'interruption manuelle (Gestion de KeyboardInterrupt) *Ctrl+C* et sauvegarde en temps r√©el, assurant la sauvegarde aussi compl√®te que possible des poids optimaux du mod√®le (Best Weights) et de plusieurs poids de processus en cas d'incident soudain.

---

## üõ†Ô∏è D√©tails de mise en ≈ìuvre technique (Technical Deep-Dive)

### 1. M√©canisme de discrimination de convergence multi-√©tapes (Multi-stage Convergence Analysis)
Contrairement aux t√¢ches g√©n√©rales, les courbes de Loss pour les t√¢ches de fine-tuning m√©dical pr√©sentent souvent une caract√©ristique de ¬´ descente en escalier ¬ª. Ce projet remplace le jugement instantan√© par une **√©valuation de tendance Loss par fen√™tre** :
* **√âviter les intervalles ¬´ pseudo-plateau ¬ª** : T5 pr√©sente souvent une p√©riode de plateau avec une descente de Loss faible au d√©but du transfert de domaine. Si l'arr√™t pr√©coce est d√©clench√© √† ce moment, le mod√®le n'a que des sensibilit√©s de base, manquant d'un ajustement profond √† la logique m√©dicale.
* **Logique de d√©clenchement diff√©r√©** : Via `DelayedEarlyStopping`, retarde forc√©ment le jugement pour capturer la **convergence secondaire (Secondary Convergence)** apr√®s la premi√®re p√©riode de plateau.

**Seul apr√®s plusieurs analyses de fen√™tres Loss, confirmant que le mod√®le est entr√© dans un √©tat de ¬´ saturation s√©mantique ¬ª, le syst√®me √©mettra le signal d'arr√™t.**


### 2. Contr√¥le de stabilit√© des gradients de haut ordre (Gradient Dynamics Control)
Pour le probl√®me d'instabilit√© des gradients caus√© par la distribution rare des vocabulaires professionnels m√©dicaux, le framework a √©t√© optimis√© au niveau bas :
* **Accumulation de gradients (Gradient Accumulation)** : Via `gradient_accumulation_steps=8` **√©conomise la m√©moire vid√©o tout en lissant les chocs de gradients instantan√©s** apport√©s par les longues phrases difficiles, simulant un environnement de mise √† jour avec un grand Batch Size stable.
* **Fr√©quence d'√©valuation asym√©trique** : Avec `eval_steps=1000`, effectue une sauvegarde de s√©lection optimale de haute pr√©cision √† faible fr√©quence pendant l'entra√Ænement long, assurant que les poids verrouill√©s par `load_best_model_at_end` poss√®dent une robustesse inter-√©chantillons r√©elle.
* **Fr√©quence de monitoring asym√©trique** : Configuration `logging_steps=100` et `eval_steps=1000`. Tout en garantissant un monitoring haute fr√©quence (surveillance si les gradients sont normaux), r√©duit la fr√©quence d'√©valuation du jeu de validation co√ªteuse, assurant que la puissance de calcul se concentre sur les mises √† jour de param√®tres.

---
## üî¨ Insights sur l'entra√Ænement : Pourquoi avoir besoin d'une ¬´ analyse multiple des Loss ¬ª ?

Dans cette t√¢che de raffinage m√©dical, la matrice de jugement de convergence est la suivante :

| Phase d'entra√Ænement | Manifestation des caract√©ristiques Loss | √âtat s√©mantique principal | R√©ponse strat√©gique |
| :--- | :--- | :--- | :--- |
| **Phase initiale (0-6000 √©tapes)** | Oscillation violente ou descente lente | √âtablissement du sens linguistique du domaine, alignement initial des param√®tres | **Continuation forc√©e** (interdiction d'un arr√™t pr√©matur√© ant√©rieur) |
| **Phase interm√©diaire (6000-12000 √©tapes)** | Apparition d'une longue p√©riode de plateau (pseudo-convergence) | Injection de connaissances professionnelles, gestion des manques de texte | **Observation continue** (analyse de tendance fen√™tr√©e) |
| **Phase finale (12000+ √©tapes)** | Descente secondaire en escalier suivie d'une stabilisation | Saturation de la profondeur s√©mantique, capacit√© de restauration r√©siliente | **√âvaluation dynamique** (arr√™t si seuil atteint) |

---

## üìä Pr√©paration du dataset et estimation de l'√©chelle des Tokens 

Dans les t√¢ches du domaine m√©dical, l'√©chelle du corpus d√©termine directement la limite sup√©rieure de la ¬´ r√©silience s√©mantique ¬ª. Selon les √©valuations pratiques :
* **Comparaison d'√©chelle** :
    * **25MB de texte chinois** : Donn√©es pr√©liminaires, ne permettant au mod√®le que l'alignement de termes de base, avec une ¬´ sensibilit√© linguistique insuffisante ¬ª √©vidente lors de la gestion des manques de texte.
    * **256MB de texte chinois** : Le mod√®le d√©montre une capacit√© stable de raffinage du domaine, atteignant les attentes d'√©valuation finales. (voir d√©mo)

* **R√©f√©rence de conversion Token chinois** (bas√©e sur l'encodage UTF-8 et le tokenizer mT5) :

| Taille du texte | Nombre estim√© de caract√®res chinois | Total estim√© de Tokens | 
| :--- | :--- | :--- | 
| **25 MB** | Environ 8 millions de caract√®res | Environ 10 millions | 
| **256 MB** | Environ 85 millions de caract√®res | Environ 100 millions | 

> **Conseils sur la qualit√© des donn√©es** : Il est recommand√© d'injecter un bruit mod√©r√© pour simuler un environnement de texte m√©dical r√©el, for√ßant le mod√®le √† apprendre √† utiliser le contexte pour ¬´ corriger ¬ª.

[D√©mo](#Demo) 

---

<a name="espanol"></a>
# Espa√±ol


## üìñ Antecedentes y Visi√≥n
Este repositorio proporciona un marco de entrenamiento de refinamiento (fine-tuning) personalizado para la arquitectura **T5** o **mT5**.

El proyecto busca, mediante una optimizaci√≥n profunda de las **estrategias de entrenamiento**, dotar al modelo de una **‚Äúresiliencia sem√°ntica‚Äù** intr√≠nseca, permiti√©ndole manejar de manera m√°s robusta las deficiencias textuales e inyectar con precisi√≥n el conocimiento especializado del dominio cuando se enfrenta a textos de alta densidad informativa como informes m√©dicos y literatura profesional.

Debido a las significativas diferencias de distribuci√≥n entre textos profesionales como la **medicina** y los corpus generales, el modelo es propenso en las **etapas iniciales de fine-tuning** a caer en √≥ptimos locales o a detenerse prematuramente debido a fluctuaciones en la Loss; este proyecto introduce optimizaciones de mecanismos para abordar este problema.

**‚ÄúMejor un sobreajuste moderado que una convergencia incompleta‚Äù**. Para dominios profesionales que no toleran ambig√ºedades, el aumento en los pasos de entrenamiento es la garant√≠a subyacente de la ‚Äúfiabilidad sem√°ntica‚Äù del modelo.

>Debido a las limitaciones de rendimiento del equipo de despliegue local, hay muchas concesiones en la configuraci√≥n. La lista espec√≠fica de configuraciones se encuentra en Requirements.

---

## ‚úÖ Funciones Principales

* **Mecanismo de precalentamiento**: Mediante el umbral `start_step`, fuerza la evasi√≥n de fluctuaciones aleatorias locales inestables iniciales. (Arranque en fr√≠o)
* **Evaluaci√≥n de tendencias de Loss por ventana**: Mediante la configuraci√≥n `patience`, permite fluctuaciones o estancamientos en la Loss durante un cierto per√≠odo, deteni√©ndose solo cuando la Loss no actualiza el r√©cord √≥ptimo en m√∫ltiples etapas consecutivas, previniendo paradas prematuras por ‚Äúper√≠odos de meseta‚Äù falsos causados por fluctuaciones temporales.
* **Seguimiento de estado**: `SafeDetailedProgressCallback` proporciona evoluci√≥n en tiempo real de la tasa de aprendizaje y predicci√≥n din√°mica de ETA (frecuencia ajustable), soportando monitoreo transparente de trabajos de entrenamiento a largo plazo.
* **Respaldo en tiempo real y continuaci√≥n desde punto de interrupci√≥n**: Para escenarios de entrenamiento de alta duraci√≥n en investigaci√≥n m√©dica, incorpora manejo de interrupciones manuales (KeyboardInterrupt Handling) *Ctrl+C* y respaldos en tiempo real, asegurando que en situaciones inesperadas, los pesos √≥ptimos del modelo (Best Weights) y m√∫ltiples pesos de proceso se guarden lo m√°s completos posible.

---

## üõ†Ô∏è Detalles de Implementaci√≥n T√©cnica (Technical Deep-Dive)

### 1. Mecanismo de Discriminaci√≥n de Convergencia Multi-etapa (Multi-stage Convergence Analysis)
A diferencia de tareas generales, las curvas de Loss en tareas de refinamiento m√©dico a menudo muestran una caracter√≠stica de ‚Äúdescenso escalonado‚Äù. Este proyecto reemplaza el juicio instant√°neo con **evaluaci√≥n de tendencias de Loss por ventana**:
* **Evitar intervalos de ‚Äúpseudo-estabilidad‚Äù**: T5 a menudo muestra per√≠odos de meseta con descenso d√©bil de Loss en las etapas iniciales de transferencia de dominio. Si se activa la parada temprana en este momento, el modelo solo tiene sensibilidad ling√º√≠stica b√°sica, faltando un ajuste profundo a la l√≥gica m√©dica.
* **L√≥gica de activaci√≥n retardada**: A trav√©s de `DelayedEarlyStopping`, fuerza un retraso en el juicio para capturar la **convergencia secundaria (Secondary Convergence)** despu√©s del primer per√≠odo de meseta.

**Solo despu√©s de m√∫ltiples an√°lisis de ventanas de Loss, confirmando que el modelo ha entrado en estado de ‚Äúsaturaci√≥n sem√°ntica‚Äù, el sistema emitir√° la se√±al de parada.**


### 2. Control de Estabilidad de Gradientes de Alto Orden (Gradient Dynamics Control)
Para el problema de inestabilidad de gradientes causado por la distribuci√≥n dispersa de vocabulario profesional m√©dico, el framework realiza optimizaciones a nivel bajo:
* **Acumulaci√≥n de gradientes (Gradient Accumulation)**: Mediante `gradient_accumulation_steps=8` **ahorra memoria de GPU al mismo tiempo que suaviza** los impactos de gradientes instant√°neos de oraciones largas y dif√≠ciles, simulando un entorno de actualizaci√≥n de Batch Size grande estable.
* **Frecuencia de evaluaci√≥n asim√©trica**: Combinado con `eval_steps=1000`, realiza selecciones y guardados de alta precisi√≥n con baja frecuencia en entrenamientos largos, asegurando que los pesos bloqueados por `load_best_model_at_end` tengan verdadera robustez entre muestras.
* **Frecuencia de monitoreo asim√©trica**: Configuraci√≥n `logging_steps=100` y `eval_steps=1000`. Garantiza telemetr√≠a de alta frecuencia (monitoreo de si los gradientes son normales) mientras reduce la frecuencia de evaluaciones costosas en el conjunto de validaci√≥n, asegurando que la potencia computacional se concentre en las actualizaciones de par√°metros.

---
## üî¨ Perspectivas de entrenamiento: ¬øPor qu√© se necesita el ‚Äúan√°lisis de Loss m√∫ltiple‚Äù?

En esta tarea de refinamiento m√©dico, la matriz de determinaci√≥n de convergencia es la siguiente:

| Fase de entrenamiento | Caracter√≠sticas de Loss | Estado sem√°ntico principal | Respuesta estrat√©gica |
| :--- | :--- | :--- | :--- |
| **Inicial (0-6000 pasos)** | Oscilaci√≥n violenta o descenso lento | Establecimiento de sensibilidad de dominio, alineaci√≥n inicial de par√°metros | **Continuar forzosamente** (prohibir early stopping previo) |
| **Media (6000-12000 pasos)** | Aparici√≥n de un largo per√≠odo de meseta (pseudo-convergencia) | Inyecci√≥n de conocimiento profesional, manejo de defectos en el texto | **Observaci√≥n continua** (an√°lisis de tendencias por ventana) |
| **Final (12000+ pasos)** | Descenso secundario en escalera seguido de estabilizaci√≥n | Saturaci√≥n de profundidad sem√°ntica, con capacidad de restauraci√≥n resiliente | **Evaluaci√≥n din√°mica** (detener al cumplir el umbral) |

---

## üìä Preparaci√≥n del conjunto de datos y estimaci√≥n de escala de Tokens 

En tareas del dominio m√©dico, la escala del corpus determina directamente el l√≠mite de la ‚Äúresiliencia sem√°ntica‚Äù. Seg√∫n evaluaciones pr√°cticas:
* **Comparaci√≥n de escala**:
    * **Texto chino de 25MB**: Datos preliminares, solo soporta alineaci√≥n b√°sica de t√©rminos, muestra una clara ‚Äúfalta de sensibilidad‚Äù al manejar defectos en el texto.
    * **Texto chino de 256MB**: El modelo muestra una capacidad estable de refinamiento de dominio, alcanzando las expectativas de evaluaci√≥n final. (ver demo)

* **Referencia de conversi√≥n de Tokens chinos** (basado en codificaci√≥n UTF-8 y tokenizador de mT5):

| Tama√±o del texto | N√∫mero estimado de caracteres chinos | Cantidad total estimada de Tokens | 
| :--- | :--- | :--- | 
| **25 MB** | Aprox. 8 millones de caracteres | Aprox. 10 millones | 
| **256 MB** | Aprox. 85 millones de caracteres | Aprox. 100 millones | 

> **Consejos de calidad de datos**: Se recomienda inyectar ruido moderado para simular entornos de texto m√©dico reales, obligando al modelo a aprender a ‚Äúcorregir‚Äù utilizando el contexto.

[Demostraci√≥n](#Demo) 

---

<a name="hindi"></a>
# ‡§π‡§ø‡§®‡•ç‡§¶‡•Ä


## üìñ ‡§™‡•É‡§∑‡•ç‡§†‡§≠‡•Ç‡§Æ‡§ø ‡§î‡§∞ ‡§¶‡•É‡§∑‡•ç‡§ü‡§ø‡§ï‡•ã‡§£
‡§Ø‡§π ‡§∞‡§ø‡§™‡•â‡§ú‡§ø‡§ü‡§∞‡•Ä **T5** ‡§Ø‡§æ **mT5** ‡§µ‡§æ‡§∏‡•ç‡§§‡•Å‡§ï‡§≤‡§æ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ö‡§®‡•Å‡§ï‡•Ç‡§≤‡§ø‡§§ ‡§è‡§ï ‡§´‡§æ‡§á‡§®-‡§ü‡•ç‡§Ø‡•Ç‡§®‡§ø‡§Ç‡§ó ‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§£ ‡§´‡•ç‡§∞‡•á‡§Æ‡§µ‡§∞‡•ç‡§ï ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§

‡§™‡§∞‡§ø‡§Ø‡•ã‡§ú‡§®‡§æ **‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§£ ‡§∞‡§£‡§®‡•Ä‡§§‡§ø‡§Ø‡•ã‡§Ç** ‡§ï‡•á ‡§ó‡§π‡§® ‡§Ö‡§®‡•Å‡§ï‡•Ç‡§≤‡§® ‡§ï‡•á ‡§Æ‡§æ‡§ß‡•ç‡§Ø‡§Æ ‡§∏‡•á ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•ã ‡§è‡§ï ‡§Ö‡§Ç‡§§‡§∞‡•ç‡§®‡§ø‡§π‡§ø‡§§ **‚Äú‡§Ö‡§∞‡•ç‡§•‡§ó‡§§ ‡§≤‡§ö‡•Ä‡§≤‡§æ‡§™‡§®‚Äù** ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§ï‡§∞‡§®‡•á ‡§ï‡§æ ‡§â‡§¶‡•ç‡§¶‡•á‡§∂‡•ç‡§Ø ‡§∞‡§ñ‡§§‡•Ä ‡§π‡•à, ‡§ú‡§ø‡§∏‡§∏‡•á ‡§ö‡§ø‡§ï‡§ø‡§§‡•ç‡§∏‡§æ ‡§∞‡§ø‡§™‡•ã‡§∞‡•ç‡§ü, ‡§™‡•á‡§∂‡•á‡§µ‡§∞ ‡§∏‡§æ‡§π‡§ø‡§§‡•ç‡§Ø ‡§Ü‡§¶‡§ø ‡§â‡§ö‡•ç‡§ö ‡§∏‡•Ç‡§ö‡§®‡§æ ‡§ò‡§®‡§§‡•ç‡§µ ‡§µ‡§æ‡§≤‡•á ‡§™‡§æ‡§†‡•ã‡§Ç ‡§ï‡§æ ‡§∏‡§æ‡§Æ‡§®‡§æ ‡§ï‡§∞‡§§‡•á ‡§∏‡§Æ‡§Ø ‡§Ø‡§π ‡§™‡§æ‡§† ‡§ï‡•Ä ‡§ï‡§Æ‡•Ä ‡§ï‡•ã ‡§Ö‡§ß‡§ø‡§ï ‡§∏‡•ç‡§•‡§ø‡§∞ ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§∏‡§Ç‡§≠‡§æ‡§≤ ‡§∏‡§ï‡•á ‡§î‡§∞ ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞‡•Ä‡§Ø ‡§µ‡§ø‡§∂‡•á‡§∑‡§ú‡•ç‡§û ‡§ú‡•ç‡§û‡§æ‡§® ‡§ï‡•ã ‡§∏‡§ü‡•Ä‡§ï ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§á‡§Ç‡§ú‡•á‡§ï‡•ç‡§ü ‡§ï‡§∞ ‡§∏‡§ï‡•á‡•§

**‡§ö‡§ø‡§ï‡§ø‡§§‡•ç‡§∏‡§æ** ‡§Ü‡§¶‡§ø ‡§™‡•á‡§∂‡•á‡§µ‡§∞ ‡§™‡§æ‡§†‡•ã‡§Ç ‡§î‡§∞ ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§ï‡•ã‡§∞‡•ç‡§™‡§∏ ‡§Æ‡•á‡§Ç ‡§Æ‡§π‡§§‡•ç‡§µ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§µ‡§ø‡§§‡§∞‡§£ ‡§Ö‡§Ç‡§§‡§∞ ‡§π‡•ã‡§®‡•á ‡§ï‡•á ‡§ï‡§æ‡§∞‡§£, ‡§Æ‡•â‡§°‡§≤ **‡§´‡§æ‡§á‡§®-‡§ü‡•ç‡§Ø‡•Ç‡§®‡§ø‡§Ç‡§ó ‡§ï‡•Ä ‡§∂‡•Å‡§∞‡•Å‡§Ü‡§§‡•Ä ‡§Ö‡§µ‡§∏‡•ç‡§•‡§æ ‡§Æ‡•á‡§Ç ‡§∏‡•ç‡§•‡§æ‡§®‡•Ä‡§Ø ‡§®‡•ç‡§Ø‡•Ç‡§®‡§§‡§Æ ‡§Æ‡•á‡§Ç ‡§´‡§Ç‡§∏‡§®‡§æ ‡§Ü‡§∏‡§æ‡§® ‡§π‡•à ‡§Ø‡§æ Loss ‡§â‡§§‡§æ‡§∞-‡§ö‡§¢‡§º‡§æ‡§µ ‡§ï‡•á ‡§ï‡§æ‡§∞‡§£ ‡§ú‡§≤‡•ç‡§¶‡•Ä ‡§∞‡•Å‡§ï‡§®‡§æ**, ‡§á‡§∏ ‡§™‡§∞‡§ø‡§Ø‡•ã‡§ú‡§®‡§æ ‡§®‡•á ‡§§‡§Ç‡§§‡•ç‡§∞ ‡§Ö‡§®‡•Å‡§ï‡•Ç‡§≤‡§® ‡§á‡§∏ ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ ‡§ï‡•ã ‡§™‡•á‡§∂ ‡§ï‡§ø‡§Ø‡§æ ‡§π‡•à‡•§

**‚Äú‡§â‡§ö‡§ø‡§§ ‡§Ö‡§ß‡§ø‡§ï-‡§Ö‡§®‡•Å‡§ï‡•Ç‡§≤‡§® ‡§¨‡•á‡§π‡§§‡§∞ ‡§π‡•à, ‡§Ö‡§™‡•Ç‡§∞‡•ç‡§£ ‡§Ö‡§≠‡§ø‡§∏‡§∞‡§£ ‡§∏‡•á‚Äù**‡•§ ‡§Ö‡§∏‡•ç‡§™‡§∑‡•ç‡§ü‡§§‡§æ ‡§ï‡•Ä ‡§Ö‡§®‡•Å‡§Æ‡§§‡§ø ‡§® ‡§¶‡•á‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§™‡•á‡§∂‡•á‡§µ‡§∞ ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è, ‡§¨‡§¢‡§º‡•á ‡§π‡•Å‡§è ‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§£ ‡§ö‡§∞‡§£ ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•Ä ‚Äú‡§Ö‡§∞‡•ç‡§•‡§ó‡§§ ‡§µ‡§ø‡§∂‡•ç‡§µ‡§∏‡§®‡•Ä‡§Ø‡§§‡§æ‚Äù ‡§ï‡•Ä ‡§Ü‡§ß‡§æ‡§∞‡§≠‡•Ç‡§§ ‡§ó‡§æ‡§∞‡§Ç‡§ü‡•Ä ‡§π‡•à‡§Ç‡•§

>‡§∏‡•ç‡§•‡§æ‡§®‡•Ä‡§Ø ‡§§‡•à‡§®‡§æ‡§§‡•Ä ‡§ï‡•á ‡§â‡§™‡§ï‡§∞‡§£ ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§® ‡§∏‡•Ä‡§Æ‡§ø‡§§ ‡§π‡•ã‡§®‡•á ‡§ï‡•á ‡§ï‡§æ‡§∞‡§£, ‡§∏‡•á‡§ü‡§ø‡§Ç‡§ó‡•ç‡§∏ ‡§Æ‡•á‡§Ç ‡§ï‡§à ‡§∏‡§Æ‡§ù‡•å‡§§‡•á ‡§π‡•à‡§Ç‡•§ ‡§µ‡§ø‡§∂‡§ø‡§∑‡•ç‡§ü ‡§ï‡•â‡§®‡•ç‡§´‡§º‡§ø‡§ó‡§∞‡•á‡§∂‡§® ‡§∏‡•Ç‡§ö‡•Ä Requirements ‡§¶‡•á‡§ñ‡•á‡§Ç‡•§

---

## ‚úÖ ‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡•ç‡§∑‡§Æ‡§§‡§æ

* **‡§™‡•Ç‡§∞‡•ç‡§µ-‡§ó‡§∞‡•ç‡§Æ ‡§ï‡§∞‡§®‡•á ‡§ï‡•Ä ‡§µ‡•ç‡§Ø‡§µ‡§∏‡•ç‡§•‡§æ**Ôºö`start_step` ‡§•‡•ç‡§∞‡•á‡§∂‡•ã‡§≤‡•ç‡§° ‡§∏‡•á‡§ü ‡§ï‡§∞‡§ï‡•á, ‡§™‡•ç‡§∞‡§æ‡§∞‡§Ç‡§≠‡§ø‡§ï ‡§Ö‡§∏‡•ç‡§•‡§ø‡§∞ ‡§∏‡•ç‡§•‡§æ‡§®‡•Ä‡§Ø ‡§Ø‡§æ‡§¶‡•É‡§ö‡•ç‡§õ‡§ø‡§ï ‡§â‡§§‡§æ‡§∞-‡§ö‡§¢‡§º‡§æ‡§µ ‡§∏‡•á ‡§¨‡§ö‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Æ‡§ú‡§¨‡•Ç‡§∞ ‡§ï‡§∞‡•á‡§Ç‡•§(‡§ï‡•ã‡§≤‡•ç‡§° ‡§∏‡•ç‡§ü‡§æ‡§∞‡•ç‡§ü)
* **‡§ñ‡§ø‡§°‡§º‡§ï‡•Ä‡§ï‡•É‡§§ Loss ‡§™‡•ç‡§∞‡§µ‡•É‡§§‡•ç‡§§‡§ø ‡§Æ‡•Ç‡§≤‡•ç‡§Ø‡§æ‡§Ç‡§ï‡§®**Ôºö`patience` ‡§∏‡•á‡§ü‡§ø‡§Ç‡§ó ‡§ï‡•á ‡§Æ‡§æ‡§ß‡•ç‡§Ø‡§Æ ‡§∏‡•á, Loss ‡§ï‡•ã ‡§è‡§ï ‡§®‡§ø‡§∂‡•ç‡§ö‡§ø‡§§ ‡§ö‡§ï‡•ç‡§∞ ‡§Æ‡•á‡§Ç ‡§â‡§§‡§æ‡§∞-‡§ö‡§¢‡§º‡§æ‡§µ ‡§Ø‡§æ ‡§†‡§π‡§∞‡§æ‡§µ ‡§ï‡•Ä ‡§Ö‡§®‡•Å‡§Æ‡§§‡§ø ‡§¶‡•á‡§Ç, ‡§ï‡•á‡§µ‡§≤ ‡§ú‡§¨ Loss ‡§≤‡§ó‡§æ‡§§‡§æ‡§∞ ‡§ï‡§à ‡§ö‡§∞‡§£‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§á‡§∑‡•ç‡§ü‡§§‡§Æ ‡§∞‡§ø‡§ï‡•â‡§∞‡•ç‡§° ‡§ï‡•ã ‡§§‡§æ‡§ú‡§º‡§æ ‡§® ‡§ï‡§∞ ‡§∏‡§ï‡•á ‡§§‡§≠‡•Ä ‡§∞‡•ã‡§ï‡•á‡§Ç, ‡§Ö‡§∏‡•ç‡§•‡§æ‡§Ø‡•Ä ‡§â‡§§‡§æ‡§∞-‡§ö‡§¢‡§º‡§æ‡§µ ‡§ï‡•á ‡§ï‡§æ‡§∞‡§£ ‡§ù‡•Ç‡§†‡•á ‚Äú‡§™‡•ç‡§≤‡•á‡§ü‡•Ç‚Äù ‡§ï‡•á ‡§ï‡§æ‡§∞‡§£ ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•ã ‡§¨‡§π‡•Å‡§§ ‡§ú‡§≤‡•ç‡§¶‡•Ä ‡§∞‡•ã‡§ï‡§®‡•á ‡§∏‡•á ‡§∞‡•ã‡§ï‡•á‡§Ç‡•§
* **‡§∏‡•ç‡§•‡§ø‡§§‡§ø ‡§ü‡•ç‡§∞‡•à‡§ï‡§ø‡§Ç‡§ó**Ôºö`SafeDetailedProgressCallback` ‡§µ‡§æ‡§∏‡•ç‡§§‡§µ‡§ø‡§ï ‡§∏‡§Æ‡§Ø ‡§≤‡§∞‡•ç‡§®‡§ø‡§Ç‡§ó ‡§∞‡•á‡§ü ‡§µ‡§ø‡§ï‡§æ‡§∏ ‡§î‡§∞ ‡§ó‡§§‡§ø‡§∂‡•Ä‡§≤ ETA ‡§™‡•Ç‡§∞‡•ç‡§µ‡§æ‡§®‡•Å‡§Æ‡§æ‡§® ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§ï‡§∞‡§§‡§æ ‡§π‡•à (‡§∏‡§Æ‡§æ‡§Ø‡•ã‡§ú‡•ç‡§Ø ‡§Ü‡§µ‡•É‡§§‡•ç‡§§‡§ø), ‡§≤‡§Ç‡§¨‡•Ä ‡§Ö‡§µ‡§ß‡§ø ‡§ï‡•á ‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§£ ‡§ï‡§æ‡§∞‡•ç‡§Ø‡•ã‡§Ç ‡§ï‡•Ä ‡§™‡§æ‡§∞‡§¶‡§∞‡•ç‡§∂‡•Ä ‡§®‡§ø‡§ó‡§∞‡§æ‡§®‡•Ä ‡§ï‡§æ ‡§∏‡§Æ‡§∞‡•ç‡§•‡§® ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
* **‡§µ‡§æ‡§∏‡•ç‡§§‡§µ‡§ø‡§ï ‡§∏‡§Æ‡§Ø ‡§¨‡•à‡§ï‡§Ö‡§™ ‡§î‡§∞ ‡§¨‡•ç‡§∞‡•á‡§ï‡§™‡•â‡§á‡§Ç‡§ü ‡§®‡§ø‡§∞‡§Ç‡§§‡§∞‡§§‡§æ**Ôºö‡§ö‡§ø‡§ï‡§ø‡§§‡•ç‡§∏‡§æ ‡§Ö‡§®‡•Å‡§∏‡§Ç‡§ß‡§æ‡§® ‡§â‡§ö‡•ç‡§ö-‡§∏‡§Æ‡§Ø ‡§≤‡•á‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§£ ‡§™‡§∞‡§ø‡§¶‡•É‡§∂‡•ç‡§Ø‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è, ‡§Ö‡§Ç‡§§‡§∞‡•ç‡§®‡§ø‡§π‡§ø‡§§ ‡§Æ‡•à‡§®‡•Å‡§Ö‡§≤ ‡§∞‡•Å‡§ï‡§æ‡§µ‡§ü (KeyboardInterrupt Handling) *Ctrl+C* ‡§î‡§∞ ‡§µ‡§æ‡§∏‡•ç‡§§‡§µ‡§ø‡§ï ‡§∏‡§Æ‡§Ø ‡§¨‡•à‡§ï‡§Ö‡§™, ‡§∏‡•Å‡§®‡§ø‡§∂‡•ç‡§ö‡§ø‡§§ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§ï‡§ø ‡§Ü‡§ï‡§∏‡•ç‡§Æ‡§ø‡§ï ‡§∏‡•ç‡§•‡§ø‡§§‡§ø ‡§Æ‡•á‡§Ç, ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•á ‡§á‡§∑‡•ç‡§ü‡§§‡§Æ ‡§µ‡§ú‡§® (Best Weights) ‡§î‡§∞ ‡§ï‡§à ‡§™‡•ç‡§∞‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ ‡§µ‡§ú‡§® ‡§ï‡•ã ‡§ú‡§ø‡§§‡§®‡§æ ‡§∏‡§Ç‡§≠‡§µ ‡§π‡•ã ‡§∏‡§ï‡•á ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§∏‡§π‡•á‡§ú‡§æ ‡§ú‡§æ‡§è‡•§

---

## üõ†Ô∏è ‡§§‡§ï‡§®‡•Ä‡§ï‡•Ä ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§æ‡§®‡•ç‡§µ‡§Ø‡§® ‡§µ‡§ø‡§µ‡§∞‡§£ (Technical Deep-Dive)

### 1. ‡§¨‡§π‡•Å-‡§ö‡§∞‡§£ ‡§Ö‡§≠‡§ø‡§∏‡§∞‡§£ ‡§®‡§ø‡§∞‡•ç‡§£‡§Ø ‡§§‡§Ç‡§§‡•ç‡§∞ (Multi-stage Convergence Analysis)
‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§ï‡§æ‡§∞‡•ç‡§Ø‡•ã‡§Ç ‡§∏‡•á ‡§≠‡§ø‡§®‡•ç‡§®, ‡§ö‡§ø‡§ï‡§ø‡§§‡•ç‡§∏‡§æ ‡§´‡§æ‡§á‡§®-‡§ü‡•ç‡§Ø‡•Ç‡§®‡§ø‡§Ç‡§ó ‡§ï‡§æ‡§∞‡•ç‡§Ø‡•ã‡§Ç ‡§ï‡§æ Loss ‡§µ‡§ï‡•ç‡§∞ ‡§Ö‡§ï‡•ç‡§∏‡§∞ ‚Äú‡§∏‡•Ä‡§¢‡§º‡•Ä ‡§ú‡•à‡§∏‡•Ä ‡§ï‡§Æ‡•Ä‚Äù ‡§µ‡§ø‡§∂‡•á‡§∑‡§§‡§æ ‡§™‡•ç‡§∞‡§∏‡•ç‡§§‡•Å‡§§ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§ ‡§Ø‡§π ‡§™‡§∞‡§ø‡§Ø‡•ã‡§ú‡§®‡§æ **‡§ñ‡§ø‡§°‡§º‡§ï‡•Ä‡§ï‡•É‡§§ Loss ‡§™‡•ç‡§∞‡§µ‡•É‡§§‡•ç‡§§‡§ø ‡§Æ‡•Ç‡§≤‡•ç‡§Ø‡§æ‡§Ç‡§ï‡§®** ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§§‡§æ‡§§‡•ç‡§ï‡§æ‡§≤‡§ø‡§ï ‡§®‡§ø‡§∞‡•ç‡§£‡§Ø ‡§ï‡•Ä ‡§ú‡§ó‡§π ‡§≤‡•á‡§§‡•Ä ‡§π‡•à:
* **‚Äú‡§ù‡•Ç‡§†‡•á ‡§∏‡§Æ‡§§‡§≤‚Äù ‡§Ö‡§Ç‡§§‡§∞‡§æ‡§≤ ‡§∏‡•á ‡§¨‡§ö‡§æ‡§µ**ÔºöT5 ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞‡•Ä‡§Ø ‡§∏‡•ç‡§•‡§æ‡§®‡§æ‡§Ç‡§§‡§∞‡§£ ‡§ï‡•Ä ‡§∂‡•Å‡§∞‡•Å‡§Ü‡§§ ‡§Æ‡•á‡§Ç Loss ‡§ï‡§Æ‡•Ä ‡§ï‡§Æ‡§ú‡•ã‡§∞ ‡§™‡•ç‡§≤‡•á‡§ü‡•Ç ‡§Ö‡§µ‡§ß‡§ø ‡§Ö‡§ï‡•ç‡§∏‡§∞ ‡§™‡•ç‡§∞‡§ï‡§ü ‡§π‡•ã‡§§‡•Ä ‡§π‡•à‡•§ ‡§Ø‡§¶‡§ø ‡§á‡§∏ ‡§∏‡§Æ‡§Ø ‡§ú‡§≤‡•ç‡§¶‡•Ä ‡§∞‡•Å‡§ï‡§æ‡§µ‡§ü ‡§ü‡•ç‡§∞‡§ø‡§ó‡§∞ ‡§π‡•ã, ‡§§‡•ã ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•á‡§µ‡§≤ ‡§Ü‡§ß‡§æ‡§∞‡§≠‡•Ç‡§§ ‡§≠‡§æ‡§∑‡§æ ‡§¨‡•ã‡§ß ‡§∞‡§ñ‡•á‡§ó‡§æ, ‡§ú‡§¨‡§ï‡§ø ‡§ö‡§ø‡§ï‡§ø‡§§‡•ç‡§∏‡§æ ‡§§‡§∞‡•ç‡§ï ‡§ï‡•á ‡§ó‡§π‡§® ‡§´‡§ø‡§ü‡§ø‡§Ç‡§ó ‡§ï‡•Ä ‡§ï‡§Æ‡•Ä ‡§π‡•ã‡§ó‡•Ä‡•§
* **‡§µ‡§ø‡§≤‡§Ç‡§¨‡§ø‡§§ ‡§ü‡•ç‡§∞‡§ø‡§ó‡§∞ ‡§§‡§∞‡•ç‡§ï**Ôºö`DelayedEarlyStopping` ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§®‡§ø‡§∞‡•ç‡§£‡§Ø ‡§ï‡•ã ‡§Æ‡§ú‡§¨‡•Ç‡§∞‡§® ‡§∏‡•ç‡§•‡§ó‡§ø‡§§ ‡§ï‡§∞‡§®‡§æ, ‡§™‡§π‡§≤‡•á ‡§™‡•ç‡§≤‡•á‡§ü‡•Ç ‡§Ö‡§µ‡§ß‡§ø ‡§ï‡•á ‡§¨‡§æ‡§¶ **‡§¶‡•ç‡§µ‡§ø‡§§‡•Ä‡§Ø‡§ï ‡§Ö‡§≠‡§ø‡§∏‡§∞‡§£ (Secondary Convergence)** ‡§ï‡•ã ‡§ï‡•à‡§™‡•ç‡§ö‡§∞ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§π‡•à‡•§

**‡§ï‡•á‡§µ‡§≤ ‡§ï‡§à Loss ‡§ñ‡§ø‡§°‡§º‡§ï‡•Ä ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£‡•ã‡§Ç ‡§ï‡•á ‡§¨‡§æ‡§¶, ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•á ‚Äú‡§Ö‡§∞‡•ç‡§•‡§ó‡§§ ‡§∏‡§Ç‡§§‡•É‡§™‡•ç‡§§‡§ø‚Äù ‡§∏‡•ç‡§•‡§ø‡§§‡§ø ‡§Æ‡•á‡§Ç ‡§™‡•ç‡§∞‡§µ‡•á‡§∂ ‡§ï‡•Ä ‡§™‡•Å‡§∑‡•ç‡§ü‡§ø ‡§π‡•ã‡§®‡•á ‡§™‡§∞ ‡§π‡•Ä, ‡§∏‡§ø‡§∏‡•ç‡§ü‡§Æ ‡§∞‡•ã‡§ï ‡§∏‡§ø‡§ó‡•ç‡§®‡§≤ ‡§ú‡§æ‡§∞‡•Ä ‡§ï‡§∞‡•á‡§ó‡§æ‡•§**


### 2. ‡§â‡§ö‡•ç‡§ö-‡§ï‡•ç‡§∞‡§Æ ‡§ó‡•ç‡§∞‡•á‡§°‡§ø‡§è‡§Ç‡§ü ‡§∏‡•ç‡§•‡§ø‡§∞‡§§‡§æ ‡§®‡§ø‡§Ø‡§Ç‡§§‡•ç‡§∞‡§£ (Gradient Dynamics Control)
‡§ö‡§ø‡§ï‡§ø‡§§‡•ç‡§∏‡§æ ‡§™‡•á‡§∂‡•á‡§µ‡§∞ ‡§∂‡§¨‡•ç‡§¶‡§æ‡§µ‡§≤‡•Ä ‡§µ‡§ø‡§§‡§∞‡§£ ‡§µ‡§ø‡§∞‡§≤ ‡§π‡•ã‡§®‡•á ‡§∏‡•á ‡§ó‡•ç‡§∞‡•á‡§°‡§ø‡§è‡§Ç‡§ü ‡§Ö‡§∏‡•ç‡§•‡§ø‡§∞‡§§‡§æ ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ ‡§ï‡•á ‡§≤‡§ø‡§è, ‡§´‡•ç‡§∞‡•á‡§Æ‡§µ‡§∞‡•ç‡§ï ‡§®‡•á ‡§§‡§≤ ‡§™‡§∞ ‡§Ö‡§®‡•Å‡§ï‡•Ç‡§≤‡§® ‡§ï‡§ø‡§Ø‡§æ ‡§π‡•à:
* **‡§ó‡•ç‡§∞‡•á‡§°‡§ø‡§è‡§Ç‡§ü ‡§∏‡§Ç‡§ö‡§Ø (Gradient Accumulation)**Ôºö`gradient_accumulation_steps=8` ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ **‡§Æ‡•á‡§Æ‡•ã‡§∞‡•Ä ‡§¨‡§ö‡§§, ‡§∏‡§æ‡§• ‡§π‡•Ä ‡§≤‡§Ç‡§¨‡•á ‡§ï‡§†‡§ø‡§® ‡§µ‡§æ‡§ï‡•ç‡§Ø‡•ã‡§Ç** ‡§∏‡•á ‡§§‡§æ‡§§‡•ç‡§ï‡§æ‡§≤‡§ø‡§ï ‡§ó‡•ç‡§∞‡•á‡§°‡§ø‡§è‡§Ç‡§ü ‡§™‡•ç‡§∞‡§≠‡§æ‡§µ ‡§ï‡•ã ‡§∏‡•Å‡§ö‡§æ‡§∞‡•Ç ‡§¨‡§®‡§æ‡§®‡§æ, ‡§∏‡•ç‡§•‡§ø‡§∞ ‡§¨‡§°‡§º‡•á ‡§¨‡•à‡§ö ‡§Ü‡§ï‡§æ‡§∞ ‡§Ö‡§™‡§°‡•á‡§ü ‡§µ‡§æ‡§§‡§æ‡§µ‡§∞‡§£ ‡§ï‡§æ ‡§Ö‡§®‡•Å‡§ï‡§∞‡§£ ‡§ï‡§∞‡§®‡§æ‡•§
* **‡§Ö‡§∏‡§Æ‡§æ‡§® ‡§Æ‡•Ç‡§≤‡•ç‡§Ø‡§æ‡§Ç‡§ï‡§® ‡§Ü‡§µ‡•É‡§§‡•ç‡§§‡§ø**Ôºö`eval_steps=1000` ‡§ï‡•á ‡§∏‡§æ‡§• ‡§Æ‡•á‡§≤ ‡§ñ‡§æ‡§®‡§æ, ‡§≤‡§Ç‡§¨‡•Ä ‡§Ö‡§µ‡§ß‡§ø ‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§£ ‡§Æ‡•á‡§Ç ‡§ï‡§Æ ‡§Ü‡§µ‡•É‡§§‡•ç‡§§‡§ø ‡§∏‡•á ‡§â‡§ö‡•ç‡§ö ‡§∏‡§ü‡•Ä‡§ï‡§§‡§æ ‡§ö‡§Ø‡§®‡§æ‡§§‡•ç‡§Æ‡§ï ‡§∏‡§Ç‡§∞‡§ï‡•ç‡§∑‡§£ ‡§ï‡§∞‡§®‡§æ, ‡§∏‡•Å‡§®‡§ø‡§∂‡•ç‡§ö‡§ø‡§§ ‡§ï‡§∞‡§®‡§æ ‡§ï‡§ø `load_best_model_at_end` ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§≤‡•â‡§ï ‡§ï‡§ø‡§è ‡§ó‡§è ‡§µ‡§ú‡§® ‡§µ‡§æ‡§∏‡•ç‡§§‡§µ ‡§Æ‡•á‡§Ç ‡§ï‡•ç‡§∞‡•â‡§∏-‡§∏‡•à‡§Ç‡§™‡§≤ ‡§Æ‡§ú‡§¨‡•Ç‡§§‡•Ä ‡§∞‡§ñ‡§§‡•á ‡§π‡•à‡§Ç‡•§
* **‡§Ö‡§∏‡§Æ‡§æ‡§® ‡§®‡§ø‡§ó‡§∞‡§æ‡§®‡•Ä ‡§Ü‡§µ‡•É‡§§‡•ç‡§§‡§ø**Ôºö`logging_steps=100` ‡§î‡§∞ `eval_steps=1000` ‡§ï‡•â‡§®‡•ç‡§´‡§º‡§ø‡§ó‡§∞ ‡§ï‡§∞‡•á‡§Ç‡•§ ‡§â‡§ö‡•ç‡§ö-‡§Ü‡§µ‡•É‡§§‡•ç‡§§‡§ø ‡§ü‡•á‡§≤‡•Ä‡§Æ‡•á‡§ü‡•ç‡§∞‡•Ä (‡§ó‡•ç‡§∞‡•á‡§°‡§ø‡§è‡§Ç‡§ü ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§π‡•à ‡§Ø‡§æ ‡§®‡§π‡•Ä‡§Ç ‡§ï‡•Ä ‡§®‡§ø‡§ó‡§∞‡§æ‡§®‡•Ä) ‡§ï‡•Ä ‡§ó‡§æ‡§∞‡§Ç‡§ü‡•Ä ‡§¶‡•á‡§§‡•á ‡§π‡•Å‡§è, ‡§â‡§ö‡•ç‡§ö-‡§∏‡§Æ‡§Ø ‡§≤‡•á‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§∏‡§§‡•ç‡§Ø‡§æ‡§™‡§® ‡§∏‡•á‡§ü ‡§Æ‡•Ç‡§≤‡•ç‡§Ø‡§æ‡§Ç‡§ï‡§® ‡§Ü‡§µ‡•É‡§§‡•ç‡§§‡§ø ‡§ï‡•ã ‡§ï‡§Æ ‡§ï‡§∞‡§®‡§æ, ‡§∏‡•Å‡§®‡§ø‡§∂‡•ç‡§ö‡§ø‡§§ ‡§ï‡§∞‡§®‡§æ ‡§ï‡§ø ‡§ï‡§Æ‡•ç‡§™‡•ç‡§Ø‡•Ç‡§ü‡§ø‡§Ç‡§ó ‡§™‡§æ‡§µ‡§∞ ‡§™‡•à‡§∞‡§æ‡§Æ‡•Ä‡§ü‡§∞ ‡§Ö‡§™‡§°‡•á‡§ü ‡§™‡§∞ ‡§ï‡•á‡§Ç‡§¶‡•ç‡§∞‡§ø‡§§ ‡§∞‡§π‡•á‡•§

---
## üî¨ ‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§£ ‡§Ö‡§Ç‡§§‡§∞‡•ç‡§¶‡•É‡§∑‡•ç‡§ü‡§ø: ‚Äú‡§ï‡§à ‡§¨‡§æ‡§∞ Loss ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£‚Äù ‡§ï‡•Ä ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï‡§§‡§æ ‡§ï‡•ç‡§Ø‡•ã‡§Ç ‡§π‡•à?

‡§á‡§∏ ‡§ö‡§ø‡§ï‡§ø‡§§‡•ç‡§∏‡§æ ‡§™‡§∞‡§ø‡§∑‡•ç‡§ï‡§∞‡§£ ‡§ï‡§æ‡§∞‡•ç‡§Ø ‡§Æ‡•á‡§Ç, ‡§Ö‡§≠‡§ø‡§∏‡§∞‡§£ ‡§®‡§ø‡§∞‡•ç‡§ß‡§æ‡§∞‡§£ ‡§Æ‡•à‡§ü‡•ç‡§∞‡§ø‡§ï‡•ç‡§∏ ‡§®‡§ø‡§Æ‡•ç‡§®‡§≤‡§ø‡§ñ‡§ø‡§§ ‡§π‡•à:

| ‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§£ ‡§ö‡§∞‡§£ | Loss ‡§µ‡§ø‡§∂‡•á‡§∑‡§§‡§æ ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§® | ‡§ï‡•ã‡§∞ ‡§Ö‡§∞‡•ç‡§•‡§Æ‡•Ç‡§≤‡§ï ‡§∏‡•ç‡§•‡§ø‡§§‡§ø | ‡§∞‡§£‡§®‡•Ä‡§§‡§ø ‡§™‡•ç‡§∞‡§§‡§ø‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ |
| :--- | :--- | :--- | :--- |
| **‡§™‡•ç‡§∞‡§æ‡§∞‡§Ç‡§≠‡§ø‡§ï (0-6000 ‡§ö‡§∞‡§£)** | ‡§§‡•Ä‡§µ‡•ç‡§∞ ‡§¶‡•ã‡§≤‡§® ‡§Ø‡§æ ‡§ß‡•Ä‡§Æ‡•Ä ‡§ï‡§Æ‡•Ä | ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞‡•Ä‡§Ø ‡§≠‡§æ‡§∑‡§æ ‡§∏‡§Ç‡§µ‡•á‡§¶‡§®‡§æ ‡§∏‡•ç‡§•‡§æ‡§™‡§®‡§æ, ‡§™‡•à‡§∞‡§æ‡§Æ‡•Ä‡§ü‡§∞ ‡§™‡•ç‡§∞‡§æ‡§∞‡§Ç‡§≠‡§ø‡§ï ‡§∏‡§Ç‡§∞‡•á‡§ñ‡§£ | **‡§Ö‡§®‡§ø‡§µ‡§æ‡§∞‡•ç‡§Ø ‡§®‡§ø‡§∞‡§Ç‡§§‡§∞** (‡§™‡§π‡§≤‡•á ‡§π‡•ã‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§Ö‡§∞‡•ç‡§≤‡•Ä ‡§∏‡•ç‡§ü‡•â‡§™ ‡§ï‡•ã ‡§™‡•ç‡§∞‡§§‡§ø‡§¨‡§Ç‡§ß‡§ø‡§§) |
| **‡§Æ‡§ß‡•ç‡§Ø (6000-12000 ‡§ö‡§∞‡§£)** | ‡§≤‡§Ç‡§¨‡•Ä ‡§™‡•ç‡§≤‡•á‡§ü‡§´‡•â‡§∞‡•ç‡§Æ ‡§Ö‡§µ‡§ß‡§ø ‡§ï‡§æ ‡§â‡§¶‡§Ø (‡§ù‡•Ç‡§†‡§æ ‡§Ö‡§≠‡§ø‡§∏‡§∞‡§£) | ‡§™‡•á‡§∂‡•á‡§µ‡§∞ ‡§ú‡•ç‡§û‡§æ‡§® ‡§á‡§Ç‡§ú‡•á‡§ï‡•ç‡§∂‡§®, ‡§™‡§æ‡§† ‡§¶‡•ã‡§∑‡•ã‡§Ç ‡§ï‡§æ ‡§™‡•ç‡§∞‡§¨‡§Ç‡§ß‡§® | **‡§®‡§ø‡§∞‡§Ç‡§§‡§∞ ‡§Ö‡§µ‡§≤‡•ã‡§ï‡§®** (‡§µ‡§ø‡§Ç‡§°‡•ã‡§Ø‡•Å‡§ï‡•ç‡§§ ‡§™‡•ç‡§∞‡§µ‡•É‡§§‡•ç‡§§‡§ø ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£) |
| **‡§â‡§§‡•ç‡§§‡§∞‡§æ‡§∞‡•ç‡§ß (12000+ ‡§ö‡§∞‡§£)** | ‡§∏‡•Ä‡§¢‡§º‡•Ä‡§¶‡§æ‡§∞ ‡§¶‡•ç‡§µ‡§ø‡§§‡•Ä‡§Ø‡§ï ‡§ï‡§Æ‡•Ä ‡§ï‡•á ‡§¨‡§æ‡§¶ ‡§∏‡•ç‡§•‡§ø‡§∞ | ‡§Ö‡§∞‡•ç‡§•‡§Æ‡•Ç‡§≤‡§ï ‡§ó‡§π‡§∞‡§æ‡§à ‡§∏‡§Ç‡§§‡•É‡§™‡•ç‡§§‡§ø, ‡§≤‡§ö‡•Ä‡§≤‡§æ‡§™‡§® ‡§™‡•Å‡§®‡§∞‡•ç‡§∏‡•ç‡§•‡§æ‡§™‡§®‡§æ ‡§ï‡•ç‡§∑‡§Æ‡§§‡§æ | **‡§ó‡§§‡§ø‡§∂‡•Ä‡§≤ ‡§Æ‡•Ç‡§≤‡•ç‡§Ø‡§æ‡§Ç‡§ï‡§®** (‡§•‡•ç‡§∞‡•á‡§∂‡•ã‡§≤‡•ç‡§° ‡§∏‡§Ç‡§§‡•Å‡§∑‡•ç‡§ü‡§ø ‡§™‡§∞ ‡§∞‡•ã‡§ï) |

---

## üìä ‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü ‡§§‡•à‡§Ø‡§æ‡§∞‡•Ä ‡§î‡§∞ Token ‡§∏‡•ç‡§ï‡•á‡§≤ ‡§Ö‡§®‡•Å‡§Æ‡§æ‡§® 

‡§ö‡§ø‡§ï‡§ø‡§§‡•ç‡§∏‡§æ ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞ ‡§ï‡§æ‡§∞‡•ç‡§Ø ‡§Æ‡•á‡§Ç, ‡§ï‡•â‡§∞‡•ç‡§™‡§∏ ‡§ï‡§æ ‡§Ü‡§ï‡§æ‡§∞ "‡§Ö‡§∞‡•ç‡§•‡§Æ‡•Ç‡§≤‡§ï ‡§≤‡§ö‡•Ä‡§≤‡§æ‡§™‡§®" ‡§ï‡•Ä ‡§ä‡§™‡§∞‡•Ä ‡§∏‡•Ä‡§Æ‡§æ ‡§∏‡•Ä‡§ß‡•á ‡§®‡§ø‡§∞‡•ç‡§ß‡§æ‡§∞‡§ø‡§§ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§ ‡§µ‡•ç‡§Ø‡§æ‡§µ‡§π‡§æ‡§∞‡§ø‡§ï ‡§Æ‡•Ç‡§≤‡•ç‡§Ø‡§æ‡§Ç‡§ï‡§® ‡§ï‡•á ‡§Ö‡§®‡•Å‡§∏‡§æ‡§∞:
* **‡§Ü‡§ï‡§æ‡§∞ ‡§§‡•Å‡§≤‡§®‡§æ**:
    * **25MB ‡§ö‡•Ä‡§®‡•Ä ‡§™‡§æ‡§†**: ‡§™‡•ç‡§∞‡§æ‡§∞‡§Ç‡§≠‡§ø‡§ï ‡§°‡•á‡§ü‡§æ, ‡§ï‡•á‡§µ‡§≤ ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•ã ‡§Ü‡§ß‡§æ‡§∞‡§≠‡•Ç‡§§ ‡§∂‡§¨‡•ç‡§¶‡§æ‡§µ‡§≤‡•Ä ‡§∏‡§Ç‡§∞‡•á‡§ñ‡§£ ‡§™‡•Ç‡§∞‡§æ ‡§ï‡§∞‡§®‡•á ‡§Æ‡•á‡§Ç ‡§∏‡§Æ‡§∞‡•ç‡§•‡§® ‡§ï‡§∞ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à, ‡§™‡§æ‡§† ‡§¶‡•ã‡§∑‡•ã‡§Ç ‡§ï‡•ã ‡§∏‡§Ç‡§≠‡§æ‡§≤‡§§‡•á ‡§∏‡§Æ‡§Ø ‡§∏‡•ç‡§™‡§∑‡•ç‡§ü "‡§≠‡§æ‡§∑‡§æ ‡§∏‡§Ç‡§µ‡•á‡§¶‡§®‡§æ ‡§ï‡•Ä ‡§ï‡§Æ‡•Ä" ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§ø‡§§ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
    * **256MB ‡§ö‡•Ä‡§®‡•Ä ‡§™‡§æ‡§†**: ‡§Æ‡•â‡§°‡§≤ ‡§∏‡•ç‡§•‡§ø‡§∞ ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞‡•Ä‡§Ø ‡§™‡§∞‡§ø‡§∑‡•ç‡§ï‡§∞‡§£ ‡§ï‡•ç‡§∑‡§Æ‡§§‡§æ ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§ø‡§§ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à, ‡§Ö‡§Ç‡§§‡§ø‡§Æ ‡§Æ‡•Ç‡§≤‡•ç‡§Ø‡§æ‡§Ç‡§ï‡§® ‡§Ö‡§™‡•á‡§ï‡•ç‡§∑‡§æ‡§ì‡§Ç ‡§ï‡•ã ‡§™‡•ç‡§∞‡§æ‡§™‡•ç‡§§ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§(‡§¶‡•á‡§ñ‡•á‡§Ç demo)

* **‡§ö‡•Ä‡§®‡•Ä Token ‡§∞‡•Ç‡§™‡§æ‡§Ç‡§§‡§∞‡§£ ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠** (UTF-8 ‡§è‡§®‡•ç‡§ï‡•ã‡§°‡§ø‡§Ç‡§ó ‡§î‡§∞ mT5 ‡§ü‡•ã‡§ï‡§®‡§æ‡§á‡§ú‡§∞ ‡§™‡§∞ ‡§Ü‡§ß‡§æ‡§∞‡§ø‡§§):

| ‡§™‡§æ‡§† ‡§Ü‡§ï‡§æ‡§∞ | ‡§Ö‡§®‡•Å‡§Æ‡§æ‡§®‡§ø‡§§ ‡§ö‡•Ä‡§®‡•Ä ‡§µ‡§∞‡•ç‡§£ ‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ | ‡§Ö‡§®‡•Å‡§Æ‡§æ‡§®‡§ø‡§§ Token ‡§ï‡•Å‡§≤ |
| :--- | :--- | :--- | 
| **25 MB** | ‡§≤‡§ó‡§≠‡§ó 80 ‡§≤‡§æ‡§ñ ‡§∂‡§¨‡•ç‡§¶ | ‡§≤‡§ó‡§≠‡§ó 1000 ‡§≤‡§æ‡§ñ | 
| **256 MB** | ‡§≤‡§ó‡§≠‡§ó 8500 ‡§≤‡§æ‡§ñ ‡§∂‡§¨‡•ç‡§¶ | ‡§≤‡§ó‡§≠‡§ó 1 ‡§Ö‡§∞‡§¨ | 

> **‡§°‡•á‡§ü‡§æ ‡§ó‡•Å‡§£‡§µ‡§§‡•ç‡§§‡§æ ‡§ü‡§ø‡§™‡•ç‡§∏**: ‡§â‡§ö‡§ø‡§§ ‡§∂‡•ã‡§∞ ‡§á‡§Ç‡§ú‡•á‡§ï‡•ç‡§ü ‡§ï‡§∞‡§®‡•á ‡§ï‡§æ ‡§∏‡•Å‡§ù‡§æ‡§µ, ‡§µ‡§æ‡§∏‡•ç‡§§‡§µ‡§ø‡§ï ‡§ö‡§ø‡§ï‡§ø‡§§‡•ç‡§∏‡§æ ‡§™‡§æ‡§† ‡§µ‡§æ‡§§‡§æ‡§µ‡§∞‡§£ ‡§ï‡§æ ‡§Ö‡§®‡•Å‡§ï‡§∞‡§£ ‡§ï‡§∞‡•á‡§Ç, ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•ã ‡§Æ‡§ú‡§¨‡•Ç‡§∞ ‡§ï‡§∞‡•á‡§Ç ‡§ï‡§ø ‡§µ‡§π ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§ï‡•á "‡§∏‡•Å‡§ß‡§æ‡§∞" ‡§ï‡•à‡§∏‡•á ‡§∏‡•Ä‡§ñ‡•á‡•§

[‡§°‡•á‡§Æ‡•ã](#Demo) 

---

<a name="korean"></a>
# ÌïúÍµ≠Ïñ¥


## üìñ Î∞∞Í≤Ω Î∞è ÎπÑÏ†Ñ
Ïù¥ Ï†ÄÏû•ÏÜåÎäî **T5** ÎòêÎäî **mT5** ÏïÑÌÇ§ÌÖçÏ≤òÏóê ÎßûÏ∂§ÌòïÏúºÎ°ú Ï†úÏûëÎêú Ï†ïÎ∞Ä ÌõàÎ†® ÌîÑÎ†àÏûÑÏõåÌÅ¨(ÎØ∏ÏÑ∏ Ï°∞Ï†ï)Î•º Ï†úÍ≥µÌï©ÎãàÎã§.

ÌîÑÎ°úÏ†ùÌä∏Îäî **ÌõàÎ†® Ï†ÑÎûµ**Ïùò Ïã¨Ï∏µ ÏµúÏ†ÅÌôîÎ•º ÌÜµÌï¥ Î™®Îç∏Ïóê ÎÇ¥Ïû¨Ï†ÅÏù∏ **‚ÄúÏùòÎØ∏ ÌÉÑÎ†•ÏÑ±‚Äù**ÏùÑ Î∂ÄÏó¨ÌïòÏó¨, ÏùòÌïô Î≥¥Í≥†ÏÑú, Ï†ÑÎ¨∏ Î¨∏Ìóå Îì± Í≥†Ï†ïÎ≥¥ Î∞ÄÎèÑ ÌÖçÏä§Ìä∏Î•º ÎßàÏ£ºÌï† Îïå ÌÖçÏä§Ìä∏ Í≤∞ÏÜêÏùÑ Îçî ÏïàÏ†ïÏ†ÅÏúºÎ°ú Ï≤òÎ¶¨ÌïòÍ≥† Î∂ÑÏïº Ï†ÑÎ¨∏ ÏßÄÏãùÏùÑ Ï†ïÎ∞ÄÌïòÍ≤å Ï£ºÏûÖÌï† Ïàò ÏûàÎèÑÎ°ù Ìï©ÎãàÎã§.

**ÏùòÌïô** Îì± Ï†ÑÎ¨∏ ÌÖçÏä§Ìä∏Í∞Ä ÏùºÎ∞ò ÏΩîÌçºÏä§ÏôÄ ÌòÑÏ†ÄÌïú Î∂ÑÌè¨ Ï∞®Ïù¥Î•º Î≥¥Ïù¥Í∏∞ ÎïåÎ¨∏Ïóê, Î™®Îç∏ÏùÄ **ÎØ∏ÏÑ∏ Ï°∞Ï†ï Ï¥àÍ∏∞ Îã®Í≥ÑÏóêÏÑú Íµ≠Î∂Ä ÏµúÏ†ÅÏóê Îπ†ÏßÄÍ±∞ÎÇò Loss Î≥ÄÎèôÏúºÎ°ú Ïù∏Ìï¥ Ï°∞Í∏∞ Ï§ëÎã®**Îê† Í∞ÄÎä•ÏÑ±Ïù¥ ÎÜíÏïÑ, Î≥∏ ÌîÑÎ°úÏ†ùÌä∏Îäî Ïù¥Î•º ÏúÑÌïú Î©îÏª§ÎãàÏ¶ò ÏµúÏ†ÅÌôîÎ•º ÎèÑÏûÖÌïòÏòÄÏäµÎãàÎã§.

**‚ÄúÏ†ÅÎãπÌïú Í≥ºÏ†ÅÌï©Ïù¥ÎùºÎèÑ, ÏàòÎ†¥Ïù¥ Î∂àÏôÑÏ†ÑÌïú Í≤ÉÏùÄ Ïïà ÎêúÎã§‚Äù**. Î™®Ìò∏Ìï®ÏùÑ ÌóàÏö©ÌïòÏßÄ ÏïäÎäî Ï†ÑÎ¨∏ Î∂ÑÏïºÏóêÏÑú Ï¶ùÍ∞ÄÎêú ÌõàÎ†® Ïä§ÌÖù ÏàòÎäî Î™®Îç∏Ïùò ‚ÄúÏùòÎØ∏ Ïã†Î¢∞ÏÑ±‚ÄùÏùò Í∏∞Î∞ò Î≥¥Ïû•ÏûÖÎãàÎã§.

>Î°úÏª¨ Î∞∞Ìè¨ Ïû•Ïπò ÏÑ±Îä•Ïù¥ Ï†úÌïúÏ†ÅÏù¥ÎØÄÎ°ú ÏÑ§Ï†ïÏóê ÎßéÏùÄ ÌÉÄÌòëÏù¥ ÏûàÏäµÎãàÎã§. Íµ¨Ï≤¥Ï†ÅÏù∏ Íµ¨ÏÑ± Î™©Î°ùÏùÄ RequirementsÎ•º Ï∞∏Ï°∞ÌïòÏÑ∏Ïöî.

---

## ‚úÖ ÌïµÏã¨ Í∏∞Îä•

* **ÏòàÏó¥ Î©îÏª§ÎãàÏ¶ò**: `start_step` ÏûÑÍ≥ÑÍ∞íÏùÑ ÏÑ§Ï†ïÌïòÏó¨ Ï¥àÍ∏∞ Î∂àÏïàÏ†ïÌïú Íµ≠Î∂ÄÏ†Å Î¨¥ÏûëÏúÑ Î≥ÄÎèôÏùÑ Í∞ïÏ†úÏ†ÅÏúºÎ°ú ÌîºÌï©ÎãàÎã§. (ÏΩúÎìú Ïä§ÌÉÄÌä∏)
* **Ï∞Ω(window) Í∏∞Î∞ò Loss Ï∂îÏÑ∏ ÌèâÍ∞Ä**: `patience` ÏÑ§Ï†ïÏùÑ ÌÜµÌï¥ LossÍ∞Ä ÏùºÏ†ï Ï£ºÍ∏∞ ÎÇ¥ÏóêÏÑú Î≥ÄÎèôÏù¥ÎÇò Ï†ïÏ≤¥Í∞Ä ÏûàÏñ¥ÎèÑ ÌóàÏö©ÌïòÎ©∞, LossÍ∞Ä Ïó∞ÏÜçÏ†ÅÏù∏ Ïó¨Îü¨ Îã®Í≥ÑÏóêÏÑú ÏµúÏ†Å Í∏∞Î°ùÏùÑ Í∞±Ïã†ÌïòÏßÄ Î™ªÌï† ÎïåÎßå Ï§ëÎã®ÌïòÏó¨, ÏùºÏãúÏ†Å Î≥ÄÎèôÏúºÎ°ú Ïù∏Ìïú Í∞ÄÏßú ‚ÄúÏ†ïÏ≤¥Í∏∞‚ÄùÎ°ú Ïù∏Ìïú Ï°∞Í∏∞ Ï§ëÎã®ÏùÑ Î∞©ÏßÄÌï©ÎãàÎã§.
* **ÏÉÅÌÉú Ï∂îÏ†Å**: `SafeDetailedProgressCallback`ÏùÄ Ïã§ÏãúÍ∞Ñ ÌïôÏäµÎ•† Î≥ÄÌôîÏôÄ ÎèôÏ†Å ETA ÏòàÏ∏°(Ï°∞Ï†ï Í∞ÄÎä• Ï£ºÍ∏∞)ÏùÑ Ï†úÍ≥µÌïòÏó¨, Ïû•Í∏∞ ÌõàÎ†® ÏûëÏóÖÏùò Ìà¨Î™ÖÌïú Î™®ÎãàÌÑ∞ÎßÅÏùÑ ÏßÄÏõêÌï©ÎãàÎã§.
* **Ïã§ÏãúÍ∞Ñ Î∞±ÏóÖ Î∞è Ï§ëÎã®Ï†ê Ïû¨Í∞ú**: ÏùòÌïô Ïó∞Íµ¨Ïùò Í≥†ÏÜåÏöî ÏãúÍ∞Ñ ÌõàÎ†® ÏãúÎÇòÎ¶¨Ïò§Î•º ÏúÑÌï¥, ÏàòÎèô Ï§ëÎã®(KeyboardInterrupt Handling) *Ctrl+C*ÏôÄ Ïã§ÏãúÍ∞Ñ Î∞±ÏóÖÏùÑ ÎÇ¥Ïû•ÌïòÏó¨,Á™ÅÂèë ÏÉÅÌô©ÏóêÏÑú Î™®Îç∏Ïùò ÏµúÏ†Å Í∞ÄÏ§ëÏπò(Best Weights)ÏôÄ Ïó¨Îü¨ Í≥ºÏ†ï Í∞ÄÏ§ëÏπòÎ•º ÏµúÎåÄÌïú ÏôÑÏ†ÑÌïòÍ≤å Î≥¥Ï°¥Ìï©ÎãàÎã§.

---

## üõ†Ô∏è Í∏∞Ïà† Íµ¨ÌòÑ ÏÑ∏Î∂Ä ÏÇ¨Ìï≠ (Technical Deep-Dive)

### 1. Îã§Îã®Í≥Ñ ÏàòÎ†¥ ÌåêÎ≥Ñ Î©îÏª§ÎãàÏ¶ò (Multi-stage Convergence Analysis)
ÏùºÎ∞ò ÏûëÏóÖÍ≥º Îã¨Î¶¨, ÏùòÌïô Ï†ïÎ∞Ä Ï°∞Ï†ï ÏûëÏóÖÏùò Loss Í≥°ÏÑ†ÏùÄ Ï¢ÖÏ¢Ö ‚ÄúÍ≥ÑÎã®Ïãù ÌïòÍ∞ï‚Äù ÌäπÏßïÏùÑ Î≥¥ÏûÖÎãàÎã§. Î≥∏ ÌîÑÎ°úÏ†ùÌä∏Îäî **Ï∞Ω(window) Í∏∞Î∞ò Loss Ï∂îÏÑ∏ ÌèâÍ∞Ä**Î•º ÌÜµÌï¥ ÏàúÍ∞Ñ ÌåêÏ†ï ÎåÄÏã† Ïù¥Î•º ÎåÄÏ≤¥Ìï©ÎãàÎã§:
* **‚ÄúÍ∞ÄÏßú ÌèâÌôú‚Äù Íµ¨Í∞Ñ ÌöåÌîº**: T5Îäî Î∂ÑÏïº Ïù¥Ï†Ñ Ï¥àÍ∏∞ Îã®Í≥ÑÏóêÏÑú Loss ÌïòÍ∞ïÏù¥ ÏïΩÌïú Ï†ïÏ≤¥Í∏∞Î•º ÏûêÏ£º Î≥¥ÏûÖÎãàÎã§. Ïù¥ Îïå Ï°∞Í∏∞ Ï§ëÎã®Ïù¥ Î∞úÏÉùÌïòÎ©¥ Î™®Îç∏ÏùÄ Í∏∞Î≥∏ Ïñ∏Ïñ¥ Í∞êÍ∞ÅÎßå Í∞ñÏ∂îÍ≥† ÏùòÌïô ÎÖºÎ¶¨Ïóê ÎåÄÌïú ÍπäÏùÄ Ï†ÅÌï©Ïù¥ Î∂ÄÏ°±Ìï©ÎãàÎã§.
* **ÏßÄÏó∞ Ìä∏Î¶¨Í±∞ Î°úÏßÅ**: `DelayedEarlyStopping`ÏùÑ ÌÜµÌï¥ ÌåêÏ†ïÏùÑ Í∞ïÏ†ú ÏßÄÏó∞ÏãúÏºú Ï≤´ Î≤àÏß∏ Ï†ïÏ≤¥Í∏∞ Ïù¥ÌõÑÏùò **2Ï∞® ÏàòÎ†¥(Secondary Convergence)**ÏùÑ Ìè¨Ï∞©Ìï©ÎãàÎã§.

**Ïó¨Îü¨ Loss Ï∞Ω Î∂ÑÏÑùÏùÑ Í±∞Ï≥ê Î™®Îç∏Ïù¥ ‚ÄúÏùòÎØ∏ Ìè¨Ìôî‚Äù ÏÉÅÌÉúÏóê ÏßÑÏûÖÌïú Í≤ÉÏùÑ ÌôïÏù∏Ìïú ÌõÑÏóêÎßå ÏãúÏä§ÌÖúÏù¥ Ï§ëÎã® Ïã†Ìò∏Î•º Î∞úÌï©ÎãàÎã§.**


### 2. Í≥†Ï∞®Ïõê Í∏∞Ïö∏Í∏∞ ÏïàÏ†ïÏÑ± Ï†úÏñ¥ (Gradient Dynamics Control)
ÏùòÌïô Ï†ÑÎ¨∏ Ïö©Ïñ¥ Î∂ÑÌè¨ Ìù¨ÏÜåÎ°ú Ïù∏Ìïú Í∏∞Ïö∏Í∏∞ Î∂àÏïàÏ†ï Î¨∏Ï†úÎ•º ÏúÑÌï¥, ÌîÑÎ†àÏûÑÏõåÌÅ¨ÎäîÂ∫ïÂ±ÇÏóêÏÑú ÏµúÏ†ÅÌôîÎ•º ÏàòÌñâÌñàÏäµÎãàÎã§:
* **Í∏∞Ïö∏Í∏∞ ÎàÑÏ†Å (Gradient Accumulation)**: `gradient_accumulation_steps=8`ÏùÑ ÌÜµÌï¥ **Î©îÎ™®Î¶¨ Ï†àÏïΩÍ≥º ÎèôÏãúÏóê Ïû•ÎÇúÎ¨∏Ïû•**ÏúºÎ°ú Ïù∏Ìïú ÏàúÍ∞Ñ Í∏∞Ïö∏Í∏∞ Ï∂©Í≤©ÏùÑ ÌèâÌôúÌôîÌïòÏó¨, ÏïàÏ†ïÏ†ÅÏù∏ ÎåÄÌòï Batch Size ÏóÖÎç∞Ïù¥Ìä∏ ÌôòÍ≤ΩÏùÑ ÏãúÎÆ¨Î†àÏù¥ÏÖòÌï©ÎãàÎã§.
* **ÎπÑÎåÄÏπ≠ ÌèâÍ∞Ä Ï£ºÍ∏∞**: `eval_steps=1000`Í≥º ÌòëÎ†•ÌïòÏó¨ Ïû•Í∏∞ ÌõàÎ†®ÏóêÏÑú ÎÇÆÏùÄ Ï£ºÍ∏∞Î°ú Í≥†Ï†ïÎ∞Ä ÏµúÏ†Å Î≥¥Ï°¥ÏùÑ ÏàòÌñâÌïòÎ©∞, `load_best_model_at_end`Í∞Ä Ïû†Í∏¥ Í∞ÄÏ§ëÏπòÍ∞Ä ÏÉòÌîå Í∞Ñ Í≤¨Í≥†ÏÑ±ÏùÑÁúüÊ≠£ Í∞ñÏ∂îÎèÑÎ°ù Ìï©ÎãàÎã§.
* **ÎπÑÎåÄÏπ≠ Î™®ÎãàÌÑ∞ÎßÅ Ï£ºÍ∏∞**: `logging_steps=100`Í≥º `eval_steps=1000`ÏùÑ Íµ¨ÏÑ±Ìï©ÎãàÎã§. Í≥†Ï£ºÍ∏∞ ÏõêÍ≤© Ï∏°Ï†ï(Í∏∞Ïö∏Í∏∞ Ï†ïÏÉÅ Ïó¨Î∂Ä Î™®ÎãàÌÑ∞ÎßÅ)ÏùÑ Î≥¥Ïû•ÌïòÎ©¥ÏÑú Í≥†ÏÜåÏöî ÏãúÍ∞Ñ Í≤ÄÏ¶ù ÏßëÌï© ÌèâÍ∞Ä Ï£ºÍ∏∞Î•º ÎÇÆÏ∂∞, Í≥ÑÏÇ∞Î†•ÏùÑ Îß§Í∞úÎ≥ÄÏàò ÏóÖÎç∞Ïù¥Ìä∏Ïóê ÏßëÏ§ëÌï©ÎãàÎã§.

---
## üî¨ ÌõàÎ†® ÌÜµÏ∞∞: Ïôú ‚ÄúÎã§Ï§ë Loss Î∂ÑÏÑù‚ÄùÏù¥ ÌïÑÏöîÌïúÍ∞Ä?

Ïù¥Î≤à ÏùòÌïô Ï†ïÎ∞Ä ÌõàÎ†® ÏûëÏóÖÏóêÏÑú ÏàòÎ†¥ ÌåêÏ†ï Îß§Ìä∏Î¶≠Ïä§Îäî Îã§ÏùåÍ≥º Í∞ôÏäµÎãàÎã§:

| ÌõàÎ†® Îã®Í≥Ñ | Loss ÌäπÏßï ÌëúÌòÑ | ÌïµÏã¨ ÏùòÎØ∏ ÏÉÅÌÉú | Ï†ÑÎûµ ÏùëÎãµ |
| :--- | :--- | :--- | :--- |
| **Ï¥àÍ∏∞ (0-6000 Îã®Í≥Ñ)** | Í∏âÍ≤©Ìïú ÏßÑÎèô ÎòêÎäî ÎäêÎ¶∞ ÏôÑÎßåÌïú ÌïòÍ∞ï | Î∂ÑÏïº Ïñ∏Í∞ê Íµ¨Ï∂ï, Îß§Í∞úÎ≥ÄÏàò Ï¥àÍ∏∞ Ï†ïÎ†¨ | **Í∞ïÏ†ú ÏßÄÏÜç** (Ïù¥Ï†ÑÏóê Î∞úÏÉùÌïú Ï°∞Í∏∞ Ï§ëÏßÄ Í∏àÏßÄ) |
| **Ï§ëÍ∏∞ (6000-12000 Îã®Í≥Ñ)** | Ïû•Í∏∞ ÌîåÎû´ÌèºÍ∏∞ Ï∂úÌòÑ (Í∞ÄÏàòÎ†¥) | Ï†ÑÎ¨∏ ÏßÄÏãù Ï£ºÏûÖ, ÌÖçÏä§Ìä∏ Í≤∞ÏÜê Ï≤òÎ¶¨ | **ÏßÄÏÜç Í¥ÄÏ∞∞** (ÏúàÎèÑÏö∞Ìôî Ï∂îÏÑ∏ Î∂ÑÏÑù) |
| **ÌõÑÍ∏∞ (12000+ Îã®Í≥Ñ)** | Í≥ÑÎã®Ïãù 2Ï∞® ÌïòÍ∞ï ÌõÑ ÏïàÏ†ï | ÏùòÎØ∏ ÍπäÏù¥ Ìè¨Ìôî, ÌÉÑÏÑ± Î≥µÏõê Î≥¥Ïú† | **ÎèôÏ†Å ÌèâÍ∞Ä** (ÏûÑÍ≥ÑÍ∞í Ï∂©Ï°± Ïãú Ï§ëÏßÄ) |

---

## üìä Îç∞Ïù¥ÌÑ∞ÏÖã Ï§ÄÎπÑÏôÄ Token Í∑úÎ™® Ï∂îÏ†ï 

ÏùòÌïô Î∂ÑÏïº ÏûëÏóÖÏóêÏÑú ÎßêÎ≠âÏπò Í∑úÎ™®Îäî ‚ÄúÏùòÎØ∏ ÌÉÑÏÑ±‚ÄùÏùò ÏÉÅÌïúÏùÑ ÏßÅÏ†ë Í≤∞Ï†ïÌï©ÎãàÎã§. Ïã§Ï†Ñ ÌèâÍ∞ÄÏóê Îî∞Î•¥Î©¥:
* **Í∑úÎ™® ÎπÑÍµê**:
    * **25MB Ï§ëÍµ≠Ïñ¥ ÌÖçÏä§Ìä∏**: Ï¥àÍ∏∞ Îç∞Ïù¥ÌÑ∞Î°ú, Î™®Îç∏Ïù¥ Í∏∞Î≥∏ Ïö©Ïñ¥ Ï†ïÎ†¨ÏùÑ ÏôÑÎ£åÌï† Ïàò ÏûàÏùÑ Îøê ÌÖçÏä§Ìä∏ Í≤∞ÏÜê Ï≤òÎ¶¨ Ïãú Î™ÖÎ∞±Ìïú ‚ÄúÏñ∏Í∞ê Î∂ÄÏ°±‚ÄùÏùÑ Î≥¥ÏûÖÎãàÎã§.
    * **256MB Ï§ëÍµ≠Ïñ¥ ÌÖçÏä§Ìä∏**: Î™®Îç∏Ïù¥ ÏïàÏ†ïÏ†ÅÏù∏ Î∂ÑÏïº Ï†ïÎ∞Ä ÌõàÎ†® Îä•Î†•ÏùÑ Î≥¥Ïù¥Î©∞ ÏµúÏ¢Ö ÌèâÍ∞Ä Í∏∞ÎåÄÎ•º Îã¨ÏÑ±Ìï©ÎãàÎã§. (Îç∞Î™® Ï∞∏Ï°∞)

* **Ï§ëÍµ≠Ïñ¥ Token ÌôòÏÇ∞ Í∏∞Ï§Ä** (UTF-8 Ïù∏ÏΩîÎî©Í≥º mT5 ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Í∏∞Î∞ò):

| ÌÖçÏä§Ìä∏ ÌÅ¨Í∏∞ | ÏòàÏÉÅ Ï§ëÍµ≠Ïñ¥ Î¨∏Ïûê Ïàò | ÏòàÏÉÅ Token Ï¥ùÎüâ | 
| :--- | :--- | :--- | 
| **25 MB** | ÏïΩ 800 Îßå Ïûê | ÏïΩ 1000 Îßå | 
| **256 MB** | ÏïΩ 8500 Îßå Ïûê | ÏïΩ 1 Ïñµ | 

> **Îç∞Ïù¥ÌÑ∞ ÌíàÏßà ÌåÅ**: Ï†ÅÎãπÌïú ÎÖ∏Ïù¥Ï¶à Ï£ºÏûÖÏùÑ Ï†úÏïàÌïòÎ©∞, Ïã§Ï†ú ÏùòÌïô ÌÖçÏä§Ìä∏ ÌôòÍ≤ΩÏùÑ ÏãúÎÆ¨Î†àÏù¥ÏÖòÌïòÏó¨ Î™®Îç∏Ïù¥ Ïª®ÌÖçÏä§Ìä∏Î•º Ïù¥Ïö©Ìïú ‚ÄúÍµêÏ†ï‚Äù ÌïôÏäµÏùÑ Í∞ïÏ†úÌï©ÎãàÎã§.

[Îç∞Î™®](#Demo) 

---

<a name="portuguese"></a>
# Portugu√™s


## üìñ Contexto e Vis√£o
Este reposit√≥rio fornece um framework de treinamento de refinamento (fine-tuning) personalizado para as arquiteturas **T5** ou **mT5**.

O projeto visa, atrav√©s da otimiza√ß√£o profunda de **estrat√©gias de treinamento**, dotar o modelo de uma **‚Äúresili√™ncia sem√¢ntica‚Äù** inerente, permitindo que ele lide de forma mais robusta com defici√™ncias de texto e injete precisamente conhecimento especializado do dom√≠nio ao enfrentar textos de alta densidade informacional, como relat√≥rios m√©dicos e literatura profissional.

Devido √†s diferen√ßas significativas de distribui√ß√£o entre textos profissionais como **medicina** e corpora gerais, o modelo √© **extremamente propenso a cair em √≥timos locais ou parar precocemente devido a flutua√ß√µes de Loss no in√≠cio do fine-tuning**, este projeto introduz otimiza√ß√µes de mecanismo para resolver esse problema.

**‚ÄúMelhor um leve overfitting do que uma converg√™ncia incompleta‚Äù**. Para dom√≠nios profissionais que n√£o toleram ambiguidades, o aumento no n√∫mero de passos de treinamento √© a garantia fundamental da ‚Äúconfiabilidade sem√¢ntica‚Äù do modelo.

>Devido √†s limita√ß√µes de desempenho do equipamento de implanta√ß√£o local, h√° muitas concess√µes nas configura√ß√µes. A lista espec√≠fica de configura√ß√µes est√° em Requirements.

---

## ‚úÖ Funcionalidades Principais

* **Mecanismo de Aquecimento**: Atrav√©s da defini√ß√£o do limiar `start_step`, for√ßa a evas√£o de flutua√ß√µes aleat√≥rias locais inst√°veis no in√≠cio. (Cold Start)
* **Avalia√ß√£o de Tend√™ncia de Loss em Janela**: Atrav√©s da configura√ß√£o `patience`, permite que o Loss apresente flutua√ß√µes ou estagna√ß√£o em um ciclo certo, parando apenas quando o Loss falha em atualizar o recorde √≥timo em m√∫ltiplos est√°gios consecutivos, prevenindo a parada prematura do modelo devido a um falso ‚Äúplat√¥‚Äù causado por flutua√ß√µes tempor√°rias.
* **Rastreamento de Estado**: `SafeDetailedProgressCallback` fornece evolu√ß√£o em tempo real da taxa de aprendizado e previs√£o din√¢mica de ETA (frequ√™ncia ajust√°vel), suportando monitoramento transparente de tarefas de treinamento de longa dura√ß√£o.
* **Backup em Tempo Real e Continua√ß√£o de Ponto de Verifica√ß√£o**: Para cen√°rios de treinamento de alta dura√ß√£o em pesquisa m√©dica, incorpora interrup√ß√£o manual (KeyboardInterrupt Handling) *Ctrl+C* e backup em tempo real, garantindo que, em situa√ß√µes inesperadas, os pesos √≥timos (Best Weights) do modelo e m√∫ltiplos pesos de processo sejam salvos da forma mais completa poss√≠vel.

---

## üõ†Ô∏è Detalhes de Implementa√ß√£o T√©cnica (Technical Deep-Dive)

### 1. Mecanismo de Discrimina√ß√£o de Converg√™ncia Multiest√°gio (Multi-stage Convergence Analysis)
Diferente de tarefas gerais, a curva de Loss em tarefas de refinamento m√©dico frequentemente apresenta caracter√≠sticas de ‚Äúdescida em escada‚Äù. Este projeto substitui a determina√ß√£o instant√¢nea pela **avalia√ß√£o de tend√™ncia de Loss em janela**:
* **Evitar Intervalos de ‚ÄúFalso Suaviza√ß√£o‚Äù**: O T5 frequentemente apresenta um plat√¥ com descida fraca de Loss no in√≠cio da transfer√™ncia de dom√≠nio. Se o early stopping for acionado nesse momento, o modelo possui apenas sensibilidade lingu√≠stica b√°sica, faltando ajuste profundo √† l√≥gica m√©dica.
* **L√≥gica de Acionamento Atrasado**: Atrav√©s de `DelayedEarlyStopping`, for√ßa o adiamento da determina√ß√£o para capturar a **converg√™ncia secund√°ria (Secondary Convergence)** ap√≥s o primeiro plat√¥.

**Somente ap√≥s m√∫ltiplas an√°lises de janelas de Loss, confirmando que o modelo entrou no estado de ‚Äúsatura√ß√£o sem√¢ntica‚Äù, o sistema emite o sinal de parada.**


### 2. Controle de Estabilidade de Gradiente de Alta Ordem (Gradient Dynamics Control)
Para o problema de instabilidade de gradiente causado pela distribui√ß√£o esparsa de vocabul√°rio profissional m√©dico, o framework realizou otimiza√ß√µes na camada inferior:
* **Acumula√ß√£o de Gradiente (Gradient Accumulation)**: Atrav√©s de `gradient_accumulation_steps=8`, **economiza mem√≥ria de v√≠deo e suaviza o impacto instant√¢neo de gradiente trazido por longas frases dif√≠ceis**, simulando um ambiente de atualiza√ß√£o de grande Batch Size est√°vel.
* **Frequ√™ncia de Avalia√ß√£o Assim√©trica**: Combinado com `eval_steps=1000`, em treinamentos de longa dura√ß√£o, realiza salvamento de sele√ß√£o √≥tima de alta precis√£o em frequ√™ncia mais baixa, garantindo que os pesos travados por `load_best_model_at_end` possuam verdadeira robustez entre amostras.
* **Frequ√™ncia de Monitoramento Assim√©trica**: Configura√ß√£o de `logging_steps=100` e `eval_steps=1000`. Enquanto garante telemetria de alta frequ√™ncia (monitorando se o gradiente est√° normal), reduz a frequ√™ncia de avalia√ß√£o custosa do conjunto de valida√ß√£o, garantindo que a capacidade computacional se concentre na atualiza√ß√£o de par√¢metros.

---
## üî¨ Insights de Treinamento: Por que precisamos de ‚ÄúAn√°lise de Loss M√∫ltipla‚Äù?

Nesta tarefa de refinamento m√©dico, a matriz de determina√ß√£o de converg√™ncia √© a seguinte:

| Fase de Treinamento | Manifesta√ß√£o das Caracter√≠sticas de Loss | Estado Sem√¢ntico Principal | Resposta Estrat√©gica |
| :--- | :--- | :--- | :--- |
| **Fase Inicial (0-6000 passos)** | Oscila√ß√£o violenta ou descida lenta gradual | Estabelecimento do senso de linguagem do dom√≠nio, alinhamento inicial de par√¢metros | **Continuar For√ßadamente** (proibi√ß√£o de early stopping ocorrido anteriormente) |
| **Fase Intermedi√°ria (6000-12000 passos)** | Aparecimento de longo per√≠odo de plat√¥ (pseudo-converg√™ncia) | Inje√ß√£o de conhecimento profissional, tratamento de defeitos de texto | **Observa√ß√£o Cont√≠nua** (an√°lise de tend√™ncias em janela) |
| **Fase Final (12000+ passos)** | Descida secund√°ria em degraus seguida de estabiliza√ß√£o | Satura√ß√£o de profundidade sem√¢ntica, com capacidade de restaura√ß√£o resiliente | **Avalia√ß√£o Din√¢mica** (parar ao satisfazer o limiar) |

---

## üìä Prepara√ß√£o do Conjunto de Dados e Estimativa de Escala de Tokens 

Em tarefas do dom√≠nio m√©dico, a escala do corpus determina diretamente o limite da ‚Äúresili√™ncia sem√¢ntica‚Äù. De acordo com avalia√ß√µes pr√°ticas:
* **Compara√ß√£o de Escala**:
    * **25MB de texto chin√™s**ÔºöDados iniciais, apenas suficientes para suportar o alinhamento de termos b√°sicos do modelo, exibindo ‚Äúsenso de linguagem deficiente‚Äù √≥bvio ao lidar com defeitos de texto.
    * **256MB de texto chin√™s**ÔºöO modelo exibe capacidade est√°vel de refinamento de dom√≠nio, atingindo as expectativas de avalia√ß√£o final.Ôºàver demoÔºâ

* **Refer√™ncia de Convers√£o de Tokens em Chin√™s**Ôºàbaseado em codifica√ß√£o UTF-8 e tokenizador mT5ÔºâÔºö

| Tamanho do Texto | N√∫mero Estimado de Caracteres Chineses | Total Estimado de Tokens | 
| :--- | :--- | :--- | 
| **25 MB** | Aprox. 8 milh√µes de caracteres | Aprox. 10 milh√µes | 
| **256 MB** | Aprox. 85 milh√µes de caracteres | Aprox. 100 milh√µes | 

> **Dicas de Qualidade de Dados**ÔºöSugere-se injetar ru√≠do moderado, simulando o ambiente real de texto m√©dico, for√ßando o modelo a aprender como usar o contexto para ‚Äúcorrigir‚Äù.

[Demonstra√ß√£o](#Demo) 



---
<a name="Demo"></a>
## üì° Demo
**Due to copyright and privacy constraints associated with real clinical documents and academic literature used in testing, data is not directly displayed in this project.**

**Áî±‰∫éÊµãËØïÊâÄ‰ΩøÁî®ÁöÑÁúüÂÆû‰∏¥Â∫äÊñáÊ°£‰∏éÂ≠¶ÊúØÊñáÁåÆÊ∂âÂèäÁâàÊùÉ‰∏éÈöêÁßÅÈóÆÈ¢òÔºåÊú¨È°πÁõÆÊú™Áõ¥Êé•Â±ïÁ§∫Ê†∑‰æãÊï∞ÊçÆ„ÄÇ**

**ÂÆüÈöõ„ÅÆËá®Â∫äÊñáÊõ∏„Åä„Çà„Å≥Â≠¶Ë°ìÊñáÁåÆ„ÅØ„ÄÅËëó‰ΩúÊ®©„Åä„Çà„Å≥„Éó„É©„Ç§„Éê„Ç∑„Éº„ÅÆÂïèÈ°å„ÇíÂê´„ÇÄ„Åü„ÇÅ„ÄÅÊú¨„Éó„É≠„Ç∏„Çß„ÇØ„Éà„Åß„ÅØ„Çµ„É≥„Éó„É´„Éá„Éº„Çø„ÇíÁõ¥Êé•ÂÖ¨Èñã„Åó„Å¶„ÅÑ„Åæ„Åõ„Çì„ÄÇ**


### üìä Evaluation
> Without adjusting the training strategy, the model may stop training prematurely, achieving only around 60% restoration accuracy.
> 
> More than half of the remaining 40% fails to reach semantically coherent results.

Based on preliminary testing with the mT5-base standard model:
* **Standard Model Performance**: The restoration rate for specialized terminology is estimated to be below 60%. The remaining 40% of results are often logically incoherent and unacceptable for professional use.
* **With DomainFocus Improvement**: The estimated restoration rate reaches 85%. Of the remaining 15% error margin, most are semantic synonyms, which greatly improves the overall readability and logical consistency of the text.
  
[Prerequisites](#Prerequisites) 
[Introduction](#Introduction)

### üìäÊïàÊûúËØÑ‰º∞
> Â¶ÇÊûú‰∏çÂØπËÆ≠ÁªÉÁ≠ñÁï•ËøõË°åË∞ÉÊï¥ÔºåÊ®°ÂûãÂèØËÉΩ‰ºöÂú®Êó©ÊúüÈò∂ÊÆµÊèêÂâçÂÅúÊ≠¢ËÆ≠ÁªÉÔºåÊúÄÁªàÂè™ËÉΩËææÂà∞Á∫¶ 60% ÁöÑËøòÂéüÁéá„ÄÇ
> 
> Âú®Ââ©‰ΩôÁöÑ 40% ÁªìÊûú‰∏≠ÔºåË∂ÖËøá‰∏ÄÂçäÊó†Ê≥ïËææÂà∞ËØ≠‰πâÈÄöÈ°∫ÁöÑÊïàÊûú„ÄÇ

Ê†πÊçÆÂàùÊ≠•ÊµãËØïÂØπÊØîÔºåÂú® mT5-base Ê†áÂáÜÊ®°Âûã‰∏≠Ôºö
* **Ê†áÂáÜÊ®°ÂûãË°®Áé∞**ÔºöÂú®‰∏ì‰∏öÈ¢ÜÂüüÁöÑËØçÊ±áËøòÂéüÁéá‰º∞ÁÆóÂú® 60% ‰ª•‰∏ãÔºåÂâ©‰Ωô 40% ÁöÑËøòÂéüÁªìÊûúÈÄªËæëÊ∑∑‰π±ÔºåÂá†‰πéÊó†Ê≥ïË¢´‰∏öÂä°Êé•Âèó„ÄÇ
* **Êú¨È°πÁõÆÊîπËøõÂêé**Ôºö‰∏ì‰∏öËØçÊ±áËøòÂéüÁéá‰º∞ÁÆóËææÂà∞‰∫Ü 85%„ÄÇÂâ©‰∏ãÁöÑ 15% ËØØÂ∑Æ‰∏≠ÔºåÂ§ßÈÉ®ÂàÜÊòØËØ≠‰πâÁõ∏ËøëÁöÑËØçÊ±áÊõø‰ª£ÔºåÊûÅÂ§ßÂú∞ÊèêÈ´ò‰∫ÜÊñáÊú¨ÁöÑÊï¥‰ΩìÂèØËØªÊÄßÂíåÈÄªËæëËøûË¥ØÊÄß„ÄÇ

[Prerequisites](#Prerequisites) 
[Introduction](#Introduction)

### üìäÂäπÊûúË©ï‰æ°ÔºàÊ©üÊ¢∞ÁøªË®≥Ôºâ
> Â≠¶ÁøíÊà¶Áï•„ÇíË™øÊï¥„Åó„Å™„ÅÑÂ†¥Âêà„ÄÅ„É¢„Éá„É´„ÅåÊó©Êúü„Å´Â≠¶Áøí„ÇíÂÅúÊ≠¢„Åó„Å¶„Åó„Åæ„ÅÑ„ÄÅÂæ©ÂÖÉÁéá„ÅØÁ¥Ñ 60% „Å´„Å®„Å©„Åæ„ÇãÂèØËÉΩÊÄß„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ
> 
> ÊÆã„Çä„ÅÆ 40% „ÅÆ„ÅÜ„Å°„ÄÅÂçäÊï∞‰ª•‰∏ä„ÅØÊÑèÂë≥ÁöÑ„Å´Ëá™ÁÑ∂„Å™ÁµêÊûú„Å´ÈÅî„Åó„Åæ„Åõ„Çì„ÄÇ

mT5-baseÊ®ôÊ∫ñ„É¢„Éá„É´„ÇíÁî®„ÅÑ„ÅüÂàùÊúü„ÉÜ„Çπ„Éà„ÅÆÊØîËºÉÔºö
* **Ê®ôÊ∫ñ„É¢„Éá„É´„ÅÆ„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ**ÔºöÂ∞ÇÈñÄÂàÜÈáé„ÅÆË™ûÂΩôÂæ©ÂÖÉÁéá„ÅØÊé®ÂÆö60%‰ª•‰∏ã„ÄÇÊÆã„Çä„ÅÆ40%„ÅØË´ñÁêÜ„ÅåÊ∑∑‰π±„Åó„Å¶„Åä„Çä„ÄÅÊ•≠ÂãôÂà©Áî®„ÅØ„Åª„Åº‰∏çÂèØËÉΩ„Åß„Åô„ÄÇ
* **Êú¨„Éó„É≠„Ç∏„Çß„ÇØ„Éà„Å´„Çà„ÇãÊîπÂñÑÂæå**ÔºöÂ∞ÇÈñÄË™ûÂΩô„ÅÆÂæ©ÂÖÉÁéá„ÅØÊé®ÂÆö85%„Å´ÈÅî„Åó„Åæ„Åó„Åü„ÄÇÊÆã„Çä„ÅÆ15%„ÅÆË™§Â∑Æ„ÅÆÂ§ßÈÉ®ÂàÜ„ÅØÊÑèÂë≥„ÅÆËøë„ÅÑË™ûÂΩô„Å∏„ÅÆÁΩÆÊèõ„Åß„ÅÇ„Çä„ÄÅ„ÉÜ„Ç≠„Çπ„ÉàÂÖ®‰Ωì„ÅÆÂèØË™≠ÊÄß„Å®Ë´ñÁêÜÁöÑ„Å™‰∏ÄË≤´ÊÄß„ÅåÂ§ßÂπÖ„Å´Âêë‰∏ä„Åó„Åæ„Åó„Åü„ÄÇ
* 
[Prerequisites](#Prerequisites) 
[Introduction](#Introduction)

---
<a name='Prerequisites'></a>
## Prerequisites - without experience using T5 or mT5

**If you have experience in T5 or mT5**: [Requirements](#Requirements)

[google-research/multilingual-t5](https://github.com/google-research/multilingual-t5)

> **English:**  
> This project provides basic training code for T5/mT5 models.  
> Training focuses on fine-tuning a pretrained model to recover masked or corrupted text.  
> If you are new to T5 training, it is helpful to understand the T5 model, masking, and tokenization.
>
> **‰∏≠ÊñáÔºö**  
> Êú¨È°πÁõÆÊèê‰æõÁî®‰∫éËÆ≠ÁªÉ T5/mT5 Ê®°ÂûãÁöÑÂü∫Á°Ä‰ª£Á†Å„ÄÇ  
> ËÆ≠ÁªÉÂÜÖÂÆπ‰∏ªË¶ÅÊòØÂØπÂ∑≤ÊúâÁöÑÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãËøõË°åÂæÆË∞ÉÔºå‰ΩøÂÖ∂Â≠¶‰ºö‰ªéË¢´ÈÅÆËîΩÊàñÁ†¥ÊçüÁöÑÊñáÊú¨‰∏≠ËøòÂéüÂÆåÊï¥ÂÜÖÂÆπ„ÄÇ  
> Â¶ÇÊûú‰Ω†‰∏çÁÜüÊÇâ T5 ËÆ≠ÁªÉÊµÅÁ®ãÔºå‰∫ÜËß£ T5 Ê®°Âûã„ÄÅMasking Âíå Tokenization Âç≥ÂèØ„ÄÇ
>
> **Êó•Êú¨Ë™ûÔºàÊ©üÊ¢∞ÁøªË®≥Ôºâ:**  
> Êú¨„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅØ„ÄÅT5/mT5 „É¢„Éá„É´„ÇíÂ≠¶Áøí„Åï„Åõ„Çã„Åü„ÇÅ„ÅÆÂü∫Êú¨ÁöÑ„Å™„Éà„É¨„Éº„Éã„É≥„Ç∞„Ç≥„Éº„Éâ„Åß„Åô„ÄÇ  
> ‰∫ãÂâçÂ≠¶ÁøíÊ∏à„Åø„É¢„Éá„É´„Çí„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åó„ÄÅ„Éû„Çπ„ÇØ„Åï„Çå„Åü„ÉÜ„Ç≠„Çπ„Éà„ÅÆÂæ©ÂÖÉ„ÇíÂ≠¶Áøí„Åó„Åæ„Åô„ÄÇ  
> T5„É¢„Éá„É´„ÄÅMasking„ÄÅTokenization „ÅÆÂü∫Á§é„ÇíÁêÜËß£„Åó„Å¶„ÅÑ„Çå„Å∞ÂçÅÂàÜ„Åß„Åô„ÄÇ
>
[Introduction](#Introduction)

---
<a name="Requirements"></a>
## üõ†Ô∏è Requirements


```text
datasets
transformers
torch
accelerate          
```

> **Equipment List:**
> 
> GPU: NVIDIA RTX 3060 Laptop GPU (6GB)
> 
> Memory: 64GB DDR4 (upgraded prior to the price increaseüòÑüòÜ)
> 
>Notice:
>
>All essential instructions are included as comments within the code.
>
>No separate Quickstart guide is provided.
>
>I hate Quickstart!

[Introduction](#Introduction)

---
<a name="References"></a>
## üí™References / Citation
```markdown
This project builds upon the T5 or mT5. If you use mT5, please cite:

@inproceedings{xue-etal-2021-mt5,
    title = "m{T}5: A Massively Multilingual Pre-trained Text-to-Text Transformer",
    author = "Xue, Linting  and
      Constant, Noah  and
      Roberts, Adam  and
      Kale, Mihir  and
      Al-Rfou, Rami  and
      Siddhant, Aditya  and
      Barua, Aditya  and
      Raffel, Colin",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.41",
    doi = "10.18653/v1/2021.naacl-main.41",
    pages = "483--498"
}

If you use this project, please cite it as:

@misc{llap4585,
    title={{T5-Refiner-DomainFocus-TrainOnly}: fine-tuning T5/mT5 models on data preprocessed by T5-Refiner-DomainFocus.},
    author={llap4585},
    howpublished = {\url{https://github.com/llap4585/T5-Refiner-DomainFocus-TrainOnly}},
    year={2026}
}

```

[Introduction](#Introduction)

---

<a name="Privacy"></a>
## üõ°Ô∏è Privacy & Security

**Local Processing Only:** This tool performs all operations locally on your machine. No medical reports, patient data, or sensitive information are uploaded to any external servers or cloud services. Your data remains under your control at all times.

**Third-party Disclaimer:** All third-party libraries required for operation are provided by the user's environment. These dependencies and their components are not under the management or control of this project.

**‰ªÖÈôêÊú¨Âú∞Â§ÑÁêÜÔºö** Êú¨Â∑•ÂÖ∑ÁöÑÊâÄÊúâÊìç‰ΩúÂùáÂú®ÊÇ®ÁöÑÊú¨Âú∞ËÆ°ÁÆóÊú∫‰∏äÊâßË°å„ÄÇ‰∏ç‰ºöÂ∞Ü‰ªª‰ΩïÂåªÁñóÊä•Âëä„ÄÅÊÇ£ËÄÖÊï∞ÊçÆÊàñÊïèÊÑü‰ø°ÊÅØ‰∏ä‰º†Âà∞‰ªª‰ΩïÂ§ñÈÉ®ÊúçÂä°Âô®Êàñ‰∫ëÊúçÂä°„ÄÇÊÇ®ÁöÑÊï∞ÊçÆÂßãÁªàÁî±ÊÇ®ÊéåÊéß„ÄÇ

**Á¨¨‰∏âÊñπÂ∫ìÂ£∞ÊòéÔºö** Êú¨Â∑•ÂÖ∑ËøêË°åÊâÄ‰æùËµñÁöÑÊâÄÊúâÁ¨¨‰∏âÊñπÂ∫ìÂùáÁî±Áî®Êà∑ÁéØÂ¢ÉÊèê‰æõÔºåËøô‰∫õÁ¨¨‰∏âÊñπÂ∫ìÂèäÂÖ∂Áõ∏ÂÖ≥ÁªÑ‰ª∂‰∏çÂú®Êú¨È°πÁõÆÁöÑÁÆ°ÁêÜ‰∏éÊéßÂà∂ËåÉÂõ¥ÂÜÖ„ÄÇ

[Introduction](#Introduction)

---
> **‚ö†Ô∏èDisclaimer:** The non-English and non-Chinese versions of this documentation are provided for convenience only and were generated using machine translation. README may have been revised multiple times, and non-Chinese content may be missing. In case of any discrepancy, the Chinese version shall prevail.

> ÂøΩÁÑ∂ÊÉ≥Ëµ∑Êù•ÊàëËÉΩË∞ÉGrok APIÁõ¥Êé•ÁøªËØëÊù•ÁùÄÔºå‰πãÂâçÊâãÂä®ÁªôaiÁøªËØëÂ∑ÆÁÇπÊ≤°ÊäòËÖæÊ≠ª„ÄÇËøôÁé©ÊÑèËÆ≠ÁªÉÊòØÁúüÁéÑÂ≠¶ÔºåÁõ¥Êé•ÊµãËØïÂèëÁé∞‰∏çÂØπÂä≤Â∞±Á´ãÂàªÊîπÁ≠ñÁï•‰∫Ü„ÄÇËøòÂ•Ω‰ª•ÂâçËÆ≠ÁªÉyoloÁöÑÊó∂ÂÄôËøòÂ•ΩÈÅáÂà∞ËøáÁ±ª‰ººÁöÑÊÉÖÂÜµÔºå

