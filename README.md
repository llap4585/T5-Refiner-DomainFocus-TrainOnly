# T5-Refiner-DomainFocus-TrainOnly
# ç»è¿‡T5-Refiner-DomainFocusé¢„å¤„ç†åçš„æ•°æ®å¾®è°ƒè®­ç»ƒä»£ç 

![Views](https://komarev.com/ghpvc/?username=llap4585&repo=T5-Refiner-DomainFocus-TrainOnly&label=Project%20Views&color=blue&style=flat-square)

If you like this project, give it a â­ï¸ on GitHub!  
Your support keeps the project going and motivates me to improve it. ğŸ˜„

> It is recommended that the data be preprocessed using the following project:  
> [T5-Refiner-DomainFocus](https://github.com/llap4585/T5-Refiner-DomainFocus)
>
> æ•°æ®å»ºè®®ç»è¿‡ä»¥ä¸‹é¡¹ç›®é¢„å¤„ç†ï¼š
> [T5-Refiner-DomainFocus](https://github.com/llap4585/T5-Refiner-DomainFocus)
>
> ãƒ‡ãƒ¼ã‚¿ã¯ã€ä»¥ä¸‹ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½¿ç”¨ã—ã¦å‰å‡¦ç†ã™ã‚‹ã“ã¨ã‚’ãŠã™ã™ã‚ã—ã¾ã™ï¼ˆæ©Ÿæ¢°ç¿»è¨³ï¼‰ï¼š  
> [T5-Refiner-DomainFocus](https://github.com/llap4585/T5-Refiner-DomainFocus)

---
<a name="Introduction"></a>
## Introduction
[â­ï¸English](#english) | [â­ï¸ä¸­æ–‡](#chinese)

*Machine translation/æ©Ÿæ¢°ç¿»è¨³:*

[æ—¥æœ¬èª](#japanese) | [Deutsch](#deutsch) | [FranÃ§ais](#francais) | [EspaÃ±ol](#espanol) | [à¤¹à¤¿à¤¨à¥à¤¦à¥€](#hindi) | [í•œêµ­ì–´](#korean) | [PortuguÃªs](#portuguese)

### Introduction to Other Languages 

â€” **one-time *quick* machine translation only**, provided according to the version as of February 1, 2026:

Arabic Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©, Bengali à¦¬à¦¾à¦‚à¦²à¦¾, Russian Ñ€ÑƒÑÑĞºĞ¸Ğ¹, Italian italiano, Dutch Nederlands, Swedish svenska

[Introduction to Other Languages](./Introduction-to-Other-Languages.md)

---

[Demo](#Demo) 

[Prerequisite - without experience using T5 or mT5](#Prerequisites)

[Requirements](#Requirements)

[References](#References)

[Privacy](#Privacy)

---
<a name="Demo"></a>
## ğŸ“¡ Demo
**Due to copyright and privacy constraints associated with real clinical documents and academic literature used in testing, data is not directly displayed in this project.**

**ç”±äºæµ‹è¯•æ‰€ä½¿ç”¨çš„çœŸå®ä¸´åºŠæ–‡æ¡£ä¸å­¦æœ¯æ–‡çŒ®æ¶‰åŠç‰ˆæƒä¸éšç§é—®é¢˜ï¼Œæœ¬é¡¹ç›®æœªç›´æ¥å±•ç¤ºæ ·ä¾‹æ•°æ®ã€‚**

**å®Ÿéš›ã®è‡¨åºŠæ–‡æ›¸ãŠã‚ˆã³å­¦è¡“æ–‡çŒ®ã¯ã€è‘—ä½œæ¨©ãŠã‚ˆã³ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ã®å•é¡Œã‚’å«ã‚€ãŸã‚ã€æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç›´æ¥å…¬é–‹ã—ã¦ã„ã¾ã›ã‚“ã€‚**


### ğŸ“Š Evaluation
> Without adjusting the training strategy, the model may stop training prematurely, achieving only around 60% restoration accuracy.
> 
> More than half of the remaining 40% fails to reach semantically coherent results.

Based on preliminary testing with the mT5-base standard model:
* **Standard Model Performance**: The restoration rate for specialized terminology is estimated to be below 60%. The remaining 40% of results are often logically incoherent and unacceptable for professional use.
* **With DomainFocus Improvement**: The estimated restoration rate reaches 85%. Of the remaining 15% error margin, most are semantic synonyms, which greatly improves the overall readability and logical consistency of the text.

### ğŸ“Šæ•ˆæœè¯„ä¼°
> å¦‚æœä¸å¯¹è®­ç»ƒç­–ç•¥è¿›è¡Œè°ƒæ•´ï¼Œæ¨¡å‹å¯èƒ½ä¼šåœ¨æ—©æœŸé˜¶æ®µæå‰åœæ­¢è®­ç»ƒï¼Œæœ€ç»ˆåªèƒ½è¾¾åˆ°çº¦ 60% çš„è¿˜åŸç‡ã€‚
> 
> åœ¨å‰©ä½™çš„ 40% ç»“æœä¸­ï¼Œè¶…è¿‡ä¸€åŠæ— æ³•è¾¾åˆ°è¯­ä¹‰é€šé¡ºçš„æ•ˆæœã€‚

æ ¹æ®åˆæ­¥æµ‹è¯•å¯¹æ¯”ï¼Œåœ¨ mT5-base æ ‡å‡†æ¨¡å‹ä¸­ï¼š
* **æ ‡å‡†æ¨¡å‹è¡¨ç°**ï¼šåœ¨ä¸“ä¸šé¢†åŸŸçš„è¯æ±‡è¿˜åŸç‡ä¼°ç®—åœ¨ 60% ä»¥ä¸‹ï¼Œå‰©ä½™ 40% çš„è¿˜åŸç»“æœé€»è¾‘æ··ä¹±ï¼Œå‡ ä¹æ— æ³•è¢«ä¸šåŠ¡æ¥å—ã€‚
* **æœ¬é¡¹ç›®æ”¹è¿›å**ï¼šä¸“ä¸šè¯æ±‡è¿˜åŸç‡ä¼°ç®—è¾¾åˆ°äº† 85%ã€‚å‰©ä¸‹çš„ 15% è¯¯å·®ä¸­ï¼Œå¤§éƒ¨åˆ†æ˜¯è¯­ä¹‰ç›¸è¿‘çš„è¯æ±‡æ›¿ä»£ï¼Œæå¤§åœ°æé«˜äº†æ–‡æœ¬çš„æ•´ä½“å¯è¯»æ€§å’Œé€»è¾‘è¿è´¯æ€§ã€‚

### ğŸ“ŠåŠ¹æœè©•ä¾¡ï¼ˆæ©Ÿæ¢°ç¿»è¨³ï¼‰
> å­¦ç¿’æˆ¦ç•¥ã‚’èª¿æ•´ã—ãªã„å ´åˆã€ãƒ¢ãƒ‡ãƒ«ãŒæ—©æœŸã«å­¦ç¿’ã‚’åœæ­¢ã—ã¦ã—ã¾ã„ã€å¾©å…ƒç‡ã¯ç´„ 60% ã«ã¨ã©ã¾ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
> 
> æ®‹ã‚Šã® 40% ã®ã†ã¡ã€åŠæ•°ä»¥ä¸Šã¯æ„å‘³çš„ã«è‡ªç„¶ãªçµæœã«é”ã—ã¾ã›ã‚“ã€‚

mT5-baseæ¨™æº–ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ãŸåˆæœŸãƒ†ã‚¹ãƒˆã®æ¯”è¼ƒï¼š
* **æ¨™æº–ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹**ï¼šå°‚é–€åˆ†é‡ã®èªå½™å¾©å…ƒç‡ã¯æ¨å®š60%ä»¥ä¸‹ã€‚æ®‹ã‚Šã®40%ã¯è«–ç†ãŒæ··ä¹±ã—ã¦ãŠã‚Šã€æ¥­å‹™åˆ©ç”¨ã¯ã»ã¼ä¸å¯èƒ½ã§ã™ã€‚
* **æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ã‚ˆã‚‹æ”¹å–„å¾Œ**ï¼šå°‚é–€èªå½™ã®å¾©å…ƒç‡ã¯æ¨å®š85%ã«é”ã—ã¾ã—ãŸã€‚æ®‹ã‚Šã®15%ã®èª¤å·®ã®å¤§éƒ¨åˆ†ã¯æ„å‘³ã®è¿‘ã„èªå½™ã¸ã®ç½®æ›ã§ã‚ã‚Šã€ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã®å¯èª­æ€§ã¨è«–ç†çš„ãªä¸€è²«æ€§ãŒå¤§å¹…ã«å‘ä¸Šã—ã¾ã—ãŸã€‚
  
---

[Introduction](#Introduction)

---
<a name='Prerequisites'></a>
## Prerequisites - without experience using T5 or mT5

**If you have experience in T5 or mT5**: [Requirements](#Requirements)

[google-research/multilingual-t5](https://github.com/google-research/multilingual-t5)

> **English:**  
> This project provides basic training code for T5/mT5 models.  
> Training focuses on fine-tuning a pretrained model to recover masked or corrupted text.  
> If you are new to T5 training, it is helpful to understand the T5 model, masking, and tokenization.
>
> **ä¸­æ–‡ï¼š**  
> æœ¬é¡¹ç›®æä¾›ç”¨äºè®­ç»ƒ T5/mT5 æ¨¡å‹çš„åŸºç¡€ä»£ç ã€‚  
> è®­ç»ƒå†…å®¹ä¸»è¦æ˜¯å¯¹å·²æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶å­¦ä¼šä»è¢«é®è”½æˆ–ç ´æŸçš„æ–‡æœ¬ä¸­è¿˜åŸå®Œæ•´å†…å®¹ã€‚  
> å¦‚æœä½ ä¸ç†Ÿæ‚‰ T5 è®­ç»ƒæµç¨‹ï¼Œäº†è§£ T5 æ¨¡å‹ã€Masking å’Œ Tokenization å³å¯ã€‚
>
> **æ—¥æœ¬èªï¼ˆæ©Ÿæ¢°ç¿»è¨³ï¼‰:**  
> æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€T5/mT5 ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã•ã›ã‚‹ãŸã‚ã®åŸºæœ¬çš„ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚³ãƒ¼ãƒ‰ã§ã™ã€‚  
> äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€ãƒã‚¹ã‚¯ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®å¾©å…ƒã‚’å­¦ç¿’ã—ã¾ã™ã€‚  
> T5ãƒ¢ãƒ‡ãƒ«ã€Maskingã€Tokenization ã®åŸºç¤ã‚’ç†è§£ã—ã¦ã„ã‚Œã°ååˆ†ã§ã™ã€‚
>
[Introduction](#Introduction)

---
<a name="Requirements"></a>
## ğŸ› ï¸ Requirements


```text
            
                
```

> **Equipment List:**
> 
> GPU: NVIDIA RTX 3060 Laptop GPU (6GB)
> 
> Memory: 64GB DDR4 (upgraded prior to the price increaseğŸ˜„ğŸ˜†)
> 
>Notice:
>
>All essential instructions are included as comments within the code.
>
>No separate Quickstart guide is provided.
>
>I hate Quickstart!

[Introduction](#Introduction)

---
<a name="References"></a>
## ğŸ’ªReferences / Citation
```markdown
This project builds upon the T5 or mT5. If you use mT5, please cite:

@inproceedings{xue-etal-2021-mt5,
    title = "m{T}5: A Massively Multilingual Pre-trained Text-to-Text Transformer",
    author = "Xue, Linting  and
      Constant, Noah  and
      Roberts, Adam  and
      Kale, Mihir  and
      Al-Rfou, Rami  and
      Siddhant, Aditya  and
      Barua, Aditya  and
      Raffel, Colin",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.41",
    doi = "10.18653/v1/2021.naacl-main.41",
    pages = "483--498"
}

If you use this project, please cite it as:

@misc{llap4585,
    title={{T5-Refiner-DomainFocus-TrainOnly}: fine-tuning T5/mT5 models on data preprocessed by T5-Refiner-DomainFocus.},
    author={llap4585},
    howpublished = {\url{https://github.com/llap4585/T5-Refiner-DomainFocus-TrainOnly}},
    year={2026}
}

```

[Introduction](#Introduction)

---

<a name="Privacy"></a>
## ğŸ›¡ï¸ Privacy & Security

**Local Processing Only:** This tool performs all operations locally on your machine. No medical reports, patient data, or sensitive information are uploaded to any external servers or cloud services. Your data remains under your control at all times.

**Third-party Disclaimer:** All third-party libraries required for operation are provided by the user's environment. These dependencies and their components are not under the management or control of this project.

**ä»…é™æœ¬åœ°å¤„ç†ï¼š** æœ¬å·¥å…·çš„æ‰€æœ‰æ“ä½œå‡åœ¨æ‚¨çš„æœ¬åœ°è®¡ç®—æœºä¸Šæ‰§è¡Œã€‚ä¸ä¼šå°†ä»»ä½•åŒ»ç–—æŠ¥å‘Šã€æ‚£è€…æ•°æ®æˆ–æ•æ„Ÿä¿¡æ¯ä¸Šä¼ åˆ°ä»»ä½•å¤–éƒ¨æœåŠ¡å™¨æˆ–äº‘æœåŠ¡ã€‚æ‚¨çš„æ•°æ®å§‹ç»ˆç”±æ‚¨æŒæ§ã€‚

**ç¬¬ä¸‰æ–¹åº“å£°æ˜ï¼š** æœ¬å·¥å…·è¿è¡Œæ‰€ä¾èµ–çš„æ‰€æœ‰ç¬¬ä¸‰æ–¹åº“å‡ç”±ç”¨æˆ·ç¯å¢ƒæä¾›ï¼Œè¿™äº›ç¬¬ä¸‰æ–¹åº“åŠå…¶ç›¸å…³ç»„ä»¶ä¸åœ¨æœ¬é¡¹ç›®çš„ç®¡ç†ä¸æ§åˆ¶èŒƒå›´å†…ã€‚

[Introduction](#Introduction)

---
> **âš ï¸Disclaimer:** The non-English and non-Chinese versions of this documentation are provided for convenience only and were generated using machine translation. README may have been revised multiple times, and non-Chinese content may be missing. In case of any discrepancy, the Chinese version shall prevail. 


